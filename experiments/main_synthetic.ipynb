{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3ae89778",
      "metadata": {
        "id": "3ae89778"
      },
      "source": [
        "# **Experiments on synthetic data**\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lealagonotte/Geometric_data_analysis_project/blob/raph-colab/experiments/main_synthetic.ipynb)  \n",
        "See on Hugging Face: https://huggingface.co/datasets/xingjiepan/PerturbMulti"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00db3d82",
      "metadata": {
        "id": "00db3d82"
      },
      "source": [
        "## **Colab setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "75700374",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75700374",
        "outputId": "e7937595-46e2-4dfd-e843-7f442663a852"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Geometric_data_analysis_project'...\n",
            "remote: Enumerating objects: 116, done.\u001b[K\n",
            "remote: Counting objects: 100% (116/116), done.\u001b[K\n",
            "remote: Compressing objects: 100% (90/90), done.\u001b[K\n",
            "remote: Total 116 (delta 55), reused 68 (delta 23), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (116/116), 4.47 MiB | 10.58 MiB/s, done.\n",
            "Resolving deltas: 100% (55/55), done.\n",
            "/content/Geometric_data_analysis_project\n",
            "Cloning into 'Perturb-OT'...\n",
            "remote: Enumerating objects: 906, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 906 (delta 8), reused 44 (delta 2), pack-reused 851 (from 2)\u001b[K\n",
            "Receiving objects: 100% (906/906), 38.95 MiB | 14.54 MiB/s, done.\n",
            "Resolving deltas: 100% (118/118), done.\n",
            "M\tPerturb-OT\n",
            "Branch 'raph-colab' set up to track remote branch 'raph-colab' from 'origin'.\n",
            "Switched to a new branch 'raph-colab'\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/lealagonotte/Geometric_data_analysis_project.git\n",
        "%cd Geometric_data_analysis_project/\n",
        "!git clone https://github.com/raphaelrubrice/Perturb-OT.git\n",
        "!git checkout raph-colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "91761e60",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "91761e60",
        "outputId": "40e45694-50dd-4fa2-9c0f-c3ed7e0a9466"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing ./Perturb-OT/ott (from -r requirements-env.txt (line 4))\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Processing ./Perturb-OT/perturbot (from -r requirements-env.txt (line 5))\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Processing ./Perturb-OT/scvi-tools (from -r requirements-env.txt (line 6))\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting absl-py==2.3.1 (from -r requirements-env.txt (line 8))\n",
            "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 9)) (24.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs==2.6.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 10)) (2.6.1)\n",
            "Requirement already satisfied: aiohttp==3.13.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 11)) (3.13.2)\n",
            "Requirement already satisfied: aiosignal==1.4.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 12)) (1.4.0)\n",
            "Collecting anndata==0.10.9 (from -r requirements-env.txt (line 13))\n",
            "  Downloading anndata-0.10.9-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: anyio==4.11.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 14)) (4.11.0)\n",
            "Collecting array-api-compat==1.12.0 (from -r requirements-env.txt (line 15))\n",
            "  Downloading array_api_compat-1.12.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: attrs==25.4.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 16)) (25.4.0)\n",
            "Requirement already satisfied: charset-normalizer==3.4.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 17)) (3.4.4)\n",
            "Requirement already satisfied: chex in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 18)) (0.1.90)\n",
            "Collecting click==8.3.0 (from -r requirements-env.txt (line 19))\n",
            "  Downloading click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: contourpy==1.3.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 20)) (1.3.3)\n",
            "Collecting crc32c==2.8 (from -r requirements-env.txt (line 21))\n",
            "  Downloading crc32c-2.8-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 22)) (0.12.1)\n",
            "Collecting datasets==4.4.1 (from -r requirements-env.txt (line 23))\n",
            "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting decorator==5.2.1 (from -r requirements-env.txt (line 24))\n",
            "  Downloading decorator-5.2.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting dill==0.4.0 (from -r requirements-env.txt (line 25))\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting docrep==0.3.2 (from -r requirements-env.txt (line 26))\n",
            "  Downloading docrep-0.3.2.tar.gz (33 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting donfig==0.8.1.post1 (from -r requirements-env.txt (line 27))\n",
            "  Downloading donfig-0.8.1.post1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting equinox (from -r requirements-env.txt (line 28))\n",
            "  Downloading equinox-0.13.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: etils==1.13.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 29)) (1.13.0)\n",
            "Requirement already satisfied: filelock==3.20.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 30)) (3.20.0)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 31)) (0.10.7)\n",
            "Requirement already satisfied: fonttools==4.60.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 32)) (4.60.1)\n",
            "Requirement already satisfied: frozenlist==1.8.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 33)) (1.8.0)\n",
            "Requirement already satisfied: h11==0.16.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 34)) (0.16.0)\n",
            "Requirement already satisfied: h5py==3.15.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 35)) (3.15.1)\n",
            "Collecting harmonypy (from -r requirements-env.txt (line 36))\n",
            "  Downloading harmonypy-0.0.10-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: hf-xet==1.2.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 37)) (1.2.0)\n",
            "Requirement already satisfied: httpcore==1.0.9 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 38)) (1.0.9)\n",
            "Requirement already satisfied: httpx==0.28.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 39)) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 40)) (0.36.0)\n",
            "Requirement already satisfied: humanize==4.14.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 41)) (4.14.0)\n",
            "Requirement already satisfied: idna==3.11 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 42)) (3.11)\n",
            "Collecting jax==0.4.36 (from jax[cpu]==0.4.36->-r requirements-env.txt (line 43))\n",
            "  Downloading jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib==0.4.36 (from -r requirements-env.txt (line 45))\n",
            "  Downloading jaxlib-0.4.36-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: jinja2==3.1.6 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 46)) (3.1.6)\n",
            "Requirement already satisfied: joblib==1.5.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 47)) (1.5.2)\n",
            "Requirement already satisfied: kiwisolver==1.4.9 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 48)) (1.4.9)\n",
            "Collecting legacy-api-wrap==1.5 (from -r requirements-env.txt (line 49))\n",
            "  Downloading legacy_api_wrap-1.5-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting lightning==2.1.4 (from -r requirements-env.txt (line 50))\n",
            "  Downloading lightning-2.1.4-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightning-utilities==0.15.2 (from -r requirements-env.txt (line 51))\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting lineax (from -r requirements-env.txt (line 52))\n",
            "  Downloading lineax-0.0.8-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: markdown-it-py==4.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 53)) (4.0.0)\n",
            "Requirement already satisfied: markupsafe==3.0.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 54)) (3.0.3)\n",
            "Collecting matplotlib==3.10.7 (from -r requirements-env.txt (line 55))\n",
            "  Downloading matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: mdurl==0.1.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 56)) (0.1.2)\n",
            "Collecting ml-collections==1.1.0 (from -r requirements-env.txt (line 57))\n",
            "  Downloading ml_collections-1.1.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting ml-dtypes==0.5.3 (from -r requirements-env.txt (line 58))\n",
            "  Downloading ml_dtypes-0.5.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 59)) (1.3.0)\n",
            "Requirement already satisfied: msgpack==1.1.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 60)) (1.1.2)\n",
            "Collecting mudata==0.3.2 (from -r requirements-env.txt (line 61))\n",
            "  Downloading mudata-0.3.2-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: multidict==6.7.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 62)) (6.7.0)\n",
            "Requirement already satisfied: multipledispatch==1.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 63)) (1.0.0)\n",
            "Collecting multiprocess==0.70.18 (from -r requirements-env.txt (line 64))\n",
            "  Downloading multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: natsort==8.4.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 65)) (8.4.0)\n",
            "Requirement already satisfied: nest-asyncio==1.6.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 66)) (1.6.0)\n",
            "Collecting networkx==3.5 (from -r requirements-env.txt (line 67))\n",
            "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting numcodecs==0.16.3 (from -r requirements-env.txt (line 68))\n",
            "  Downloading numcodecs-0.16.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting numpyro (from -r requirements-env.txt (line 69))\n",
            "  Downloading numpyro-0.19.0-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: opt-einsum==3.4.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 70)) (3.4.0)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 71)) (0.2.6)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 72)) (0.11.28)\n",
            "Requirement already satisfied: parso==0.8.5 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 73)) (0.8.5)\n",
            "Requirement already satisfied: patsy==1.0.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 74)) (1.0.2)\n",
            "Requirement already satisfied: pexpect==4.9.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 75)) (4.9.0)\n",
            "Collecting pot==0.9.6.post1 (from -r requirements-env.txt (line 76))\n",
            "  Downloading pot-0.9.6.post1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: propcache==0.4.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 77)) (0.4.1)\n",
            "Requirement already satisfied: pynndescent==0.5.13 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 78)) (0.5.13)\n",
            "Requirement already satisfied: pyparsing==3.2.5 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 79)) (3.2.5)\n",
            "Collecting pyro-api==0.1.2 (from -r requirements-env.txt (line 80))\n",
            "  Downloading pyro_api-0.1.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting pyro-ppl==1.9.1 (from -r requirements-env.txt (line 81))\n",
            "  Downloading pyro_ppl-1.9.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 82)) (2.9.0.post0)\n",
            "Collecting pytorch-lightning==2.5.6 (from -r requirements-env.txt (line 83))\n",
            "  Downloading pytorch_lightning-2.5.6-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: pyyaml==6.0.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 84)) (6.0.3)\n",
            "Collecting scanpy==1.11.5 (from -r requirements-env.txt (line 85))\n",
            "  Downloading scanpy-1.11.5-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting scikit-learn==1.7.2 (from -r requirements-env.txt (line 86))\n",
            "  Downloading scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: scipy==1.16.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 87)) (1.16.3)\n",
            "Requirement already satisfied: seaborn==0.13.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 88)) (0.13.2)\n",
            "Collecting session-info2==0.2.3 (from -r requirements-env.txt (line 89))\n",
            "  Downloading session_info2-0.2.3-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: shellingham==1.5.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 90)) (1.5.4)\n",
            "Requirement already satisfied: simplejson==3.20.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 91)) (3.20.2)\n",
            "Requirement already satisfied: six==1.17.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 92)) (1.17.0)\n",
            "Requirement already satisfied: sniffio==1.3.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 93)) (1.3.1)\n",
            "Collecting sparse==0.17.0 (from -r requirements-env.txt (line 94))\n",
            "  Downloading sparse-0.17.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: statsmodels==0.14.5 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 95)) (0.14.5)\n",
            "Requirement already satisfied: treescope==0.1.10 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 96)) (0.1.10)\n",
            "Requirement already satisfied: typer-slim==0.20.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 97)) (0.20.0)\n",
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 98)) (4.67.1)\n",
            "Requirement already satisfied: umap-learn==0.5.9.post2 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 99)) (0.5.9.post2)\n",
            "Requirement already satisfied: xxhash==3.6.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 100)) (3.6.0)\n",
            "Requirement already satisfied: yarl==1.22.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements-env.txt (line 101)) (1.22.0)\n",
            "Collecting zarr==3.1.3 (from -r requirements-env.txt (line 102))\n",
            "  Downloading zarr-3.1.3-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal==1.4.0->-r requirements-env.txt (line 12)) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from anndata==0.10.9->-r requirements-env.txt (line 13)) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from anndata==0.10.9->-r requirements-env.txt (line 13)) (25.0)\n",
            "Requirement already satisfied: pandas!=2.1.0rc0,!=2.1.2,>=1.4 in /usr/local/lib/python3.12/dist-packages (from anndata==0.10.9->-r requirements-env.txt (line 13)) (2.2.2)\n",
            "Collecting pyarrow>=21.0.0 (from datasets==4.4.1->-r requirements-env.txt (line 23))\n",
            "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets==4.4.1->-r requirements-env.txt (line 23)) (2.32.4)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets==4.4.1->-r requirements-env.txt (line 23)) (2025.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpcore==1.0.9->-r requirements-env.txt (line 38)) (2025.11.12)\n",
            "Collecting fsspec<=2025.10.0,>=2023.1.0 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets==4.4.1->-r requirements-env.txt (line 23))\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting packaging>=20.0 (from anndata==0.10.9->-r requirements-env.txt (line 13))\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: torch<4.0,>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from lightning==2.1.4->-r requirements-env.txt (line 50)) (2.9.0+cu126)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning==2.1.4->-r requirements-env.txt (line 50))\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities==0.15.2->-r requirements-env.txt (line 51)) (75.2.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.7->-r requirements-env.txt (line 55)) (11.3.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect==4.9.0->-r requirements-env.txt (line 75)) (0.7.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.12/dist-packages (from pynndescent==0.5.13->-r requirements-env.txt (line 78)) (0.60.0)\n",
            "Requirement already satisfied: llvmlite>=0.30 in /usr/local/lib/python3.12/dist-packages (from pynndescent==0.5.13->-r requirements-env.txt (line 78)) (0.43.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.7.2->-r requirements-env.txt (line 86)) (3.6.0)\n",
            "Collecting jax-cuda12-plugin<=0.4.36,>=0.4.36 (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda\"->jax[cuda]==0.4.36->-r requirements-env.txt (line 44))\n",
            "  Downloading jax_cuda12_plugin-0.4.36-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jaxopt>=0.8 (from ott-jax@ file:./Perturb-OT/ott->-r requirements-env.txt (line 4))\n",
            "  Downloading jaxopt-0.8.5-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from scvi-tools@ file:./Perturb-OT/scvi-tools->-r requirements-env.txt (line 6)) (13.9.4)\n",
            "Requirement already satisfied: xarray>=2023.2.0 in /usr/local/lib/python3.12/dist-packages (from scvi-tools@ file:./Perturb-OT/scvi-tools->-r requirements-env.txt (line 6)) (2025.11.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chex->-r requirements-env.txt (line 18)) (0.12.1)\n",
            "INFO: pip is looking at multiple versions of equinox to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting equinox (from -r requirements-env.txt (line 28))\n",
            "  Downloading equinox-0.13.1-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading equinox-0.13.0-py3-none-any.whl.metadata (18 kB)\n",
            "  Downloading equinox-0.12.2-py3-none-any.whl.metadata (18 kB)\n",
            "  Downloading equinox-0.12.1-py3-none-any.whl.metadata (18 kB)\n",
            "  Downloading equinox-0.12.0-py3-none-any.whl.metadata (18 kB)\n",
            "  Downloading equinox-0.11.12-py3-none-any.whl.metadata (18 kB)\n",
            "  Downloading equinox-0.11.11-py3-none-any.whl.metadata (18 kB)\n",
            "INFO: pip is still looking at multiple versions of equinox to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading equinox-0.11.10-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting jaxtyping>=0.2.20 (from equinox->-r requirements-env.txt (line 28))\n",
            "  Downloading jaxtyping-0.3.3-py3-none-any.whl.metadata (7.8 kB)\n",
            "INFO: pip is looking at multiple versions of flax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting flax (from -r requirements-env.txt (line 31))\n",
            "  Downloading flax-0.12.1-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading flax-0.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading flax-0.11.2-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading flax-0.11.1-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading flax-0.11.0-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading flax-0.10.6-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading flax-0.10.5-py3-none-any.whl.metadata (11 kB)\n",
            "INFO: pip is still looking at multiple versions of flax to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading flax-0.10.4-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.12/dist-packages (from flax->-r requirements-env.txt (line 31)) (0.1.79)\n",
            "INFO: pip is looking at multiple versions of lineax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting lineax (from -r requirements-env.txt (line 52))\n",
            "  Downloading lineax-0.0.7-py3-none-any.whl.metadata (17 kB)\n",
            "INFO: pip is looking at multiple versions of optax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting optax (from -r requirements-env.txt (line 71))\n",
            "  Downloading optax-0.2.5-py3-none-any.whl.metadata (7.5 kB)\n",
            "INFO: pip is looking at multiple versions of orbax-checkpoint to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting orbax-checkpoint (from -r requirements-env.txt (line 72))\n",
            "  Downloading orbax_checkpoint-0.11.30-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Downloading orbax_checkpoint-0.11.27-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Downloading orbax_checkpoint-0.11.26-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Downloading orbax_checkpoint-0.11.25-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading orbax_checkpoint-0.11.24-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading orbax_checkpoint-0.11.23-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading orbax_checkpoint-0.11.22-py3-none-any.whl.metadata (2.3 kB)\n",
            "INFO: pip is still looking at multiple versions of orbax-checkpoint to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading orbax_checkpoint-0.11.21-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading orbax_checkpoint-0.11.20-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading orbax_checkpoint-0.11.19-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading orbax_checkpoint-0.11.18-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading orbax_checkpoint-0.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading orbax_checkpoint-0.11.16-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.11.15-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.11.14-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.11.13-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.11.12-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.11.11-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.11.10-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.11.9-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.11.8-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.11.7-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.11.6-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading orbax_checkpoint-0.11.5-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->-r requirements-env.txt (line 72)) (5.29.5)\n",
            "Collecting jax-cuda12-pjrt==0.4.36 (from jax-cuda12-plugin<=0.4.36,>=0.4.36->jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda\"->jax[cuda]==0.4.36->-r requirements-env.txt (line 44))\n",
            "  Downloading jax_cuda12_pjrt-0.4.36-py3-none-manylinux2014_x86_64.whl.metadata (349 bytes)\n",
            "Requirement already satisfied: nvidia-cublas-cu12>=12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda\"->jax[cuda]==0.4.36->-r requirements-env.txt (line 44)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12>=12.1.105 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda\"->jax[cuda]==0.4.36->-r requirements-env.txt (line 44)) (12.6.80)\n",
            "Collecting nvidia-cuda-nvcc-cu12>=12.6.85 (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda\"->jax[cuda]==0.4.36->-r requirements-env.txt (line 44))\n",
            "  Downloading nvidia_cuda_nvcc_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12>=12.1.105 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda\"->jax[cuda]==0.4.36->-r requirements-env.txt (line 44)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12<10.0,>=9.1 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda\"->jax[cuda]==0.4.36->-r requirements-env.txt (line 44)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cufft-cu12>=11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda\"->jax[cuda]==0.4.36->-r requirements-env.txt (line 44)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12>=11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda\"->jax[cuda]==0.4.36->-r requirements-env.txt (line 44)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12>=12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda\"->jax[cuda]==0.4.36->-r requirements-env.txt (line 44)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12>=2.18.1 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda\"->jax[cuda]==0.4.36->-r requirements-env.txt (line 44)) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12>=12.1.105 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda\"->jax[cuda]==0.4.36->-r requirements-env.txt (line 44)) (12.6.85)\n",
            "Collecting wadler-lindig>=0.1.3 (from jaxtyping>=0.2.20->equinox->-r requirements-env.txt (line 28))\n",
            "  Downloading wadler_lindig-0.1.7-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata==0.10.9->-r requirements-env.txt (line 13)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata==0.10.9->-r requirements-env.txt (line 13)) (2025.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.4.1->-r requirements-env.txt (line 23)) (2.5.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.0.0->scvi-tools@ file:./Perturb-OT/scvi-tools->-r requirements-env.txt (line 6)) (2.19.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=1.12.0->lightning==2.1.4->-r requirements-env.txt (line 50)) (1.14.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=1.12.0->lightning==2.1.4->-r requirements-env.txt (line 50)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=1.12.0->lightning==2.1.4->-r requirements-env.txt (line 50)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=1.12.0->lightning==2.1.4->-r requirements-env.txt (line 50)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=1.12.0->lightning==2.1.4->-r requirements-env.txt (line 50)) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=1.12.0->lightning==2.1.4->-r requirements-env.txt (line 50)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=1.12.0->lightning==2.1.4->-r requirements-env.txt (line 50)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=1.12.0->lightning==2.1.4->-r requirements-env.txt (line 50)) (3.5.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->-r requirements-env.txt (line 72)) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->-r requirements-env.txt (line 72)) (3.23.0)\n",
            "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anndata-0.10.9-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.0/129.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading array_api_compat-1.12.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.2/58.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.3.0-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading crc32c-2.8-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.0/80.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.6/511.6 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading decorator-5.2.1-py3-none-any.whl (9.2 kB)\n",
            "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading donfig-0.8.1.post1-py3-none-any.whl (21 kB)\n",
            "Downloading jax-0.4.36-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.4.36-cp312-cp312-manylinux2014_x86_64.whl (100.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading legacy_api_wrap-1.5-py3-none-any.whl (10 kB)\n",
            "Downloading lightning-2.1.4-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m112.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_collections-1.1.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.5.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m124.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mudata-0.3.2-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.18-py312-none-any.whl (150 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.3/150.3 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numcodecs-0.16.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pot-0.9.6.post1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyro_api-0.1.2-py3-none-any.whl (11 kB)\n",
            "Downloading pyro_ppl-1.9.1-py3-none-any.whl (755 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.5.6-py3-none-any.whl (831 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m831.6/831.6 kB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scanpy-1.11.5-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m120.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading session_info2-0.2.3-py3-none-any.whl (16 kB)\n",
            "Downloading sparse-0.17.0-py2.py3-none-any.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.4/259.4 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zarr-3.1.3-py3-none-any.whl (276 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.4/276.4 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading equinox-0.11.10-py3-none-any.whl (178 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.9/178.9 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flax-0.10.4-py3-none-any.whl (451 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m451.8/451.8 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading harmonypy-0.0.10-py3-none-any.whl (20 kB)\n",
            "Downloading lineax-0.0.7-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpyro-0.19.0-py3-none-any.whl (370 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m370.9/370.9 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optax-0.2.5-py3-none-any.whl (354 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.3/354.3 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orbax_checkpoint-0.11.5-py3-none-any.whl (342 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.8/342.8 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax_cuda12_plugin-0.4.36-cp312-cp312-manylinux2014_x86_64.whl (16.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.2/16.2 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax_cuda12_pjrt-0.4.36-py3-none-manylinux2014_x86_64.whl (101.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxopt-0.8.5-py3-none-any.whl (172 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.4/172.4 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxtyping-0.3.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvcc_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (40.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wadler_lindig-0.1.7-py3-none-any.whl (20 kB)\n",
            "Building wheels for collected packages: docrep, ott-jax, perturbot, scvi-tools\n",
            "  Building wheel for docrep (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docrep: filename=docrep-0.3.2-py3-none-any.whl size=19876 sha256=9459ddceb7d29e2b0323dbb1b09147f5f5660e1c7037a8d0d1e8414f8ace3ca9\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/19/ee/0a6a1793d91c449563b49ccab57ce52da3e6fab7614836bd8c\n",
            "  Building wheel for ott-jax (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ott-jax: filename=ott_jax-0.4.6a0-py3-none-any.whl size=249819 sha256=218e86f6394b5afe01341e545e35715acf337d8603d8ff445004205ea89dba20\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-eagv2w_9/wheels/71/c1/63/1269694ac552cc69c9a3f6d81ca6459a6d516b62eedef2fa64\n",
            "  Building wheel for perturbot (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for perturbot: filename=perturbot-0.0.1-py3-none-any.whl size=52340 sha256=e2f7f3fedf866d247b0e8af6900191154c0e6400ec0c993bbba54220e207ecd0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-eagv2w_9/wheels/9d/67/b3/ce899a66ce4b5a11cb086b10580d420c075923aa50838a04e8\n",
            "  Building wheel for scvi-tools (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scvi-tools: filename=scvi_tools-1.1.0a0-py3-none-any.whl size=392346 sha256=4c06d0627e5f5d6b1a392e90b56824f0a1aa0fd2da24f4536313efbbde64b2d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/e6/36/db541baea5134e05ffd774bddde54d756b3542455ea6bc89a5\n",
            "Successfully built docrep ott-jax perturbot scvi-tools\n",
            "Installing collected packages: pyro-api, jax-cuda12-pjrt, wadler-lindig, session-info2, pyarrow, packaging, nvidia-cuda-nvcc-cu12, numcodecs, networkx, ml-dtypes, legacy-api-wrap, jax-cuda12-plugin, fsspec, donfig, docrep, dill, decorator, crc32c, click, array-api-compat, absl-py, sparse, scikit-learn, pot, multiprocess, ml-collections, matplotlib, lightning-utilities, jaxtyping, jaxlib, zarr, jax, harmonypy, anndata, torchmetrics, pyro-ppl, orbax-checkpoint, numpyro, mudata, jaxopt, equinox, datasets, scanpy, pytorch-lightning, optax, lineax, ott-jax, lightning, flax, scvi-tools, perturbot\n",
            "  Attempting uninstall: jax-cuda12-pjrt\n",
            "    Found existing installation: jax-cuda12-pjrt 0.7.2\n",
            "    Uninstalling jax-cuda12-pjrt-0.7.2:\n",
            "      Successfully uninstalled jax-cuda12-pjrt-0.7.2\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "  Attempting uninstall: nvidia-cuda-nvcc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvcc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvcc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvcc-cu12-12.5.82\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.6\n",
            "    Uninstalling networkx-3.6:\n",
            "      Successfully uninstalled networkx-3.6\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml_dtypes 0.5.4\n",
            "    Uninstalling ml_dtypes-0.5.4:\n",
            "      Successfully uninstalled ml_dtypes-0.5.4\n",
            "  Attempting uninstall: jax-cuda12-plugin\n",
            "    Found existing installation: jax-cuda12-plugin 0.7.2\n",
            "    Uninstalling jax-cuda12-plugin-0.7.2:\n",
            "      Successfully uninstalled jax-cuda12-plugin-0.7.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.8\n",
            "    Uninstalling dill-0.3.8:\n",
            "      Successfully uninstalled dill-0.3.8\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 4.4.2\n",
            "    Uninstalling decorator-4.4.2:\n",
            "      Successfully uninstalled decorator-4.4.2\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.3.1\n",
            "    Uninstalling click-8.3.1:\n",
            "      Successfully uninstalled click-8.3.1\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.4.0\n",
            "    Uninstalling absl-py-1.4.0:\n",
            "      Successfully uninstalled absl-py-1.4.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: multiprocess\n",
            "    Found existing installation: multiprocess 0.70.16\n",
            "    Uninstalling multiprocess-0.70.16:\n",
            "      Successfully uninstalled multiprocess-0.70.16\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.7.2\n",
            "    Uninstalling jaxlib-0.7.2:\n",
            "      Successfully uninstalled jaxlib-0.7.2\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.7.2\n",
            "    Uninstalling jax-0.7.2:\n",
            "      Successfully uninstalled jax-0.7.2\n",
            "  Attempting uninstall: orbax-checkpoint\n",
            "    Found existing installation: orbax-checkpoint 0.11.28\n",
            "    Uninstalling orbax-checkpoint-0.11.28:\n",
            "      Successfully uninstalled orbax-checkpoint-0.11.28\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "  Attempting uninstall: optax\n",
            "    Found existing installation: optax 0.2.6\n",
            "    Uninstalling optax-0.2.6:\n",
            "      Successfully uninstalled optax-0.2.6\n",
            "  Attempting uninstall: flax\n",
            "    Found existing installation: flax 0.10.7\n",
            "    Uninstalling flax-0.10.7:\n",
            "      Successfully uninstalled flax-0.10.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\n",
            "moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed absl-py-2.3.1 anndata-0.10.9 array-api-compat-1.12.0 click-8.3.0 crc32c-2.8 datasets-4.4.1 decorator-5.2.1 dill-0.4.0 docrep-0.3.2 donfig-0.8.1.post1 equinox-0.11.10 flax-0.10.4 fsspec-2024.12.0 harmonypy-0.0.10 jax-0.4.36 jax-cuda12-pjrt-0.4.36 jax-cuda12-plugin-0.4.36 jaxlib-0.4.36 jaxopt-0.8.5 jaxtyping-0.3.3 legacy-api-wrap-1.5 lightning-2.1.4 lightning-utilities-0.15.2 lineax-0.0.7 matplotlib-3.10.7 ml-collections-1.1.0 ml-dtypes-0.5.3 mudata-0.3.2 multiprocess-0.70.18 networkx-3.5 numcodecs-0.16.3 numpyro-0.19.0 nvidia-cuda-nvcc-cu12-12.9.86 optax-0.2.5 orbax-checkpoint-0.11.5 ott-jax-0.4.6a0 packaging-24.2 perturbot-0.0.1 pot-0.9.6.post1 pyarrow-22.0.0 pyro-api-0.1.2 pyro-ppl-1.9.1 pytorch-lightning-2.5.6 scanpy-1.11.5 scikit-learn-1.7.2 scvi-tools-1.1.0a0 session-info2-0.2.3 sparse-0.17.0 torchmetrics-1.8.2 wadler-lindig-0.1.7 zarr-3.1.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "59e7f399fd224054a0b2ac4efb251157",
              "pip_warning": {
                "packages": [
                  "decorator",
                  "matplotlib",
                  "mpl_toolkits",
                  "packaging"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install -r requirements-env.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77f6ea26",
      "metadata": {
        "id": "77f6ea26"
      },
      "source": [
        "YOU WILL NEED TO RESTART THE SESSION AFTER THE PREVIOUS CELL.\n",
        "After restarting you should have a working env."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37549e8b",
      "metadata": {
        "id": "37549e8b"
      },
      "source": [
        "Let's check that we have succesfully recreated the env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f077840a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f077840a",
        "outputId": "d70b80c6-3c09-45fe-ee8e-445856eeb258"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imports succeeded\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "from perturbot.match import (\n",
        "    get_coupling_cotl,\n",
        "    get_coupling_cotl_sinkhorn,\n",
        "    get_coupling_egw_labels_ott,\n",
        "    get_coupling_egw_all_ott,\n",
        "    get_coupling_eot_ott,\n",
        "    get_coupling_leot_ott,\n",
        "    get_coupling_egw_ott,\n",
        "    get_coupling_cot,\n",
        "    get_coupling_cot_sinkhorn,\n",
        "    get_coupling_gw_labels,\n",
        "    get_coupling_fot,\n",
        ")\n",
        "from perturbot.predict import train_mlp\n",
        "import ot\n",
        "print(\"Imports succeeded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "046155ed",
      "metadata": {
        "id": "046155ed"
      },
      "source": [
        "## **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3edd1b1b",
      "metadata": {
        "id": "3edd1b1b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import perturbot.match\n",
        "import perturbot.predict\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "import seaborn as sns\n",
        "sns.set_theme(\"paper\")\n",
        "import pandas as pd\n",
        "from perturbot.eval.prediction import get_evals, get_evals_preds\n",
        "from perturbot.eval.match import get_FOSCTTM_single, get_FOSCTTM\n",
        "import ot\n",
        "import itertools, time, os\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50e874fa",
      "metadata": {
        "id": "50e874fa"
      },
      "source": [
        "## **Generate data according to Appendix G from [Ryu et al., 2024](https://arxiv.org/abs/2405.00838)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2468b18c",
      "metadata": {
        "id": "2468b18c"
      },
      "outputs": [],
      "source": [
        "## Generate synthetic data as specified in Annex G\n",
        "\n",
        "def generate_synthetic_data(n_cells_per_pert=50, n_perturbations=10, d_latent=5, p_X=50, p_Y=200):\n",
        "    \"\"\"Generate synthetic data as specified in Annex G \"\"\"\n",
        "    print(f\"Generation of {n_cells_per_pert * n_perturbations} synthetic cells\")\n",
        "    n_total_cells = n_cells_per_pert * n_perturbations\n",
        "\n",
        "    # Z ~ N(0, 0.1)\n",
        "    Z_base = np.random.normal(0.0, np.sqrt(0.1), (n_total_cells, d_latent))\n",
        "\n",
        "    # A_X, A_Y, b_X, b_Y, s_X, s_Y\n",
        "    A_X = np.random.normal(0.0, 1.0, (d_latent, p_X))\n",
        "    A_Y = np.random.normal(0.0, 1.0, (d_latent, p_Y))\n",
        "    b_X = np.random.normal(0.0, 1.0, (p_X,))\n",
        "    b_Y = np.random.normal(0.0, 1.0, (p_Y,))\n",
        "    s_X = np.random.gamma(1.0, 1.0, (p_X,))\n",
        "    s_Y = np.random.gamma(1.0, 1.0, (p_Y,))\n",
        "\n",
        "    # zeta_X, zeta_Y (Technical noise)\n",
        "    std_X = np.sqrt(np.exp(np.random.normal(-3, 0)))\n",
        "    zeta_X = np.random.normal(0, std_X, (n_total_cells, d_latent))\n",
        "    std_Y = np.sqrt(np.exp(np.random.normal(-3, 0)))\n",
        "    zeta_Y = np.random.normal(0, std_Y, (n_total_cells, d_latent))\n",
        "\n",
        "    # Labels and perturbation\n",
        "    labels = np.repeat(np.arange(n_perturbations), n_cells_per_pert)\n",
        "    target_dims = np.zeros(n_perturbations, dtype=int)\n",
        "    target_dims[1:] = (np.arange(n_perturbations - 1) % d_latent)\n",
        "    effect_sizes = np.zeros(n_perturbations)\n",
        "    effect_sizes[1:] = np.maximum(3.0, np.random.gamma(1.0, 1.0, n_perturbations - 1))\n",
        "    penetrance = np.random.beta(1.0, 10.0, n_total_cells)\n",
        "\n",
        "    Z_perturbed = Z_base.copy()\n",
        "    for i in range(n_total_cells):\n",
        "        label_idx = labels[i]\n",
        "        if label_idx > 0: # No perturbation for label 0\n",
        "            target_dim = target_dims[label_idx]\n",
        "            effect = effect_sizes[label_idx]\n",
        "            Z_perturbed[i, target_dim] += effect * penetrance[i]\n",
        "\n",
        "    # Final Generation\n",
        "    Z_noisy_X = Z_perturbed + zeta_X\n",
        "    Z_noisy_Y = Z_perturbed + zeta_Y\n",
        "    X = ((Z_noisy_X @ A_X) + b_X) * s_X\n",
        "    Y = ((Z_noisy_Y @ A_Y) + b_Y) * s_Y\n",
        "\n",
        "    # Identifier of the cells\n",
        "    ids = np.arange(n_total_cells).reshape(-1, 1)\n",
        "\n",
        "    return X, Y, labels, ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a3a0e7fe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3a0e7fe",
        "outputId": "0b3640d9-3350-4dec-eb27-f62f0cfdd143"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generation of 500 synthetic cells\n"
          ]
        }
      ],
      "source": [
        "#We generate the data as presented in Annex G\n",
        "X_full, Y_full, labels_full, ids_full = generate_synthetic_data(\n",
        "        n_cells_per_pert=50,\n",
        "        n_perturbations=10,\n",
        "        d_latent=5,\n",
        "        p_X=50,\n",
        "        p_Y=200\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b279b22d",
      "metadata": {
        "id": "b279b22d"
      },
      "outputs": [],
      "source": [
        "def format_data_for_coupling(X, Y, labels):\n",
        "    \"\"\"\n",
        "    Transform the data into dictionaries keyed by integer labels.\n",
        "    \"\"\"\n",
        "    X_dict = {}\n",
        "    Y_dict = {}\n",
        "    unique_labels = np.unique(labels)\n",
        "\n",
        "    for l_numpy in unique_labels:\n",
        "        l_python = int(l_numpy)\n",
        "\n",
        "        indices = np.where(labels == l_numpy)[0]\n",
        "\n",
        "        X_dict[l_python] = X[indices]\n",
        "        Y_dict[l_python] = Y[indices]\n",
        "\n",
        "    return (X_dict, Y_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "48a373d7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48a373d7",
        "outputId": "26c45440-8f8d-434a-b061-ffec28e425a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1 created: (400, 50) (100, 50)\n",
            "Fold 2 created: (400, 50) (100, 50)\n",
            "Fold 3 created: (400, 50) (100, 50)\n",
            "Fold 4 created: (400, 50) (100, 50)\n",
            "Fold 5 created: (400, 50) (100, 50)\n"
          ]
        }
      ],
      "source": [
        "#prepare folds for cross-validation\n",
        "\n",
        "X = np.array(X_full)\n",
        "Y = np.array(Y_full)\n",
        "labels = np.array(labels_full)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "\n",
        "folds = []\n",
        "\n",
        "for fold_idx, (train_index, test_index) in enumerate(skf.split(X, labels), 1):\n",
        "\n",
        "    # --- 1) Split brut ---\n",
        "    X_train_raw, X_test_raw = X[train_index], X[test_index]\n",
        "    Y_train_raw, Y_test_raw = Y[train_index], Y[test_index]\n",
        "    lab_train_raw, lab_test_raw = labels[train_index], labels[test_index]\n",
        "\n",
        "    # Format the data for coupling\n",
        "\n",
        "    data_train = format_data_for_coupling(X_train_raw, Y_train_raw, lab_train_raw)\n",
        "    data_test  = format_data_for_coupling(X_test_raw,  Y_test_raw,  lab_test_raw)\n",
        "    unique_labels_test = sorted(data_test[0].keys())\n",
        "\n",
        "    #Ground truth test set preparation\n",
        "\n",
        "    X_test_concat = np.concatenate([data_test[0][l] for l in unique_labels_test], axis=0)\n",
        "    X_test_t = torch.tensor(X_test_concat, dtype=torch.double)\n",
        "\n",
        "    Y_test_concat = np.concatenate([data_test[1][l] for l in unique_labels_test], axis=0)\n",
        "\n",
        "    unique_labels_train = sorted(data_train[0].keys())\n",
        "\n",
        "    labels_train_concat = np.concatenate([\n",
        "        np.full(len(data_train[0][l]), l) for l in unique_labels_train\n",
        "    ])\n",
        "\n",
        "    # --- 5) Sauvegarde du fold ---\n",
        "    folds.append({\n",
        "        \"fold\": fold_idx,\n",
        "\n",
        "        \"X_train_raw\": X_train_raw,\n",
        "        \"X_test_raw\":  X_test_raw,\n",
        "        \"Y_train_raw\": Y_train_raw,\n",
        "        \"Y_test_raw\":  Y_test_raw,\n",
        "        \"lab_train_raw\": lab_train_raw,\n",
        "        \"lab_test_raw\":  lab_test_raw,\n",
        "\n",
        "        \"data_train\": data_train,\n",
        "        \"data_test\":  data_test,\n",
        "\n",
        "        \"X_test_torch\": X_test_t,\n",
        "        \"Y_test_concat\": Y_test_concat,\n",
        "\n",
        "        \"labels_train_concat\": labels_train_concat,\n",
        "    })\n",
        "\n",
        "    print(f\"Fold {fold_idx} created:\",\n",
        "          X_train_raw.shape, X_test_raw.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65342e72",
      "metadata": {
        "id": "65342e72"
      },
      "source": [
        "## **Compute couplings with diverse methods**\n",
        "(GWOT, COOT, FGWOT, with or without labels, with or without entropic regularization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0559ffc5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0559ffc5",
        "outputId": "4bbcd6af-358c-4013-9f0c-8c54a3e67d43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running EGWL with ott\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "Label considered for Sinkhorn run\n",
            "lse step\n",
            "updating linearization\n",
            "Label considered for Sinkhorn run\n",
            "5 outer iterations were needed.\n",
            "The last Sinkhorn iteration has converged: True\n",
            "The outer loop of Gromov Wasserstein has converged: True\n",
            "The final regularized GW cost is: 0.535\n",
            "Done running LEGWOT with ott\n",
            "running EGWL with ott\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "Label considered for Sinkhorn run\n",
            "lse step\n",
            "updating linearization\n",
            "Label considered for Sinkhorn run\n",
            "5 outer iterations were needed.\n",
            "The last Sinkhorn iteration has converged: True\n",
            "The outer loop of Gromov Wasserstein has converged: True\n",
            "The final regularized GW cost is: 0.492\n",
            "Done running LEGWOT with ott\n",
            "running EGWL with ott\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "Label considered for Sinkhorn run\n",
            "lse step\n",
            "updating linearization\n",
            "Label considered for Sinkhorn run\n",
            "5 outer iterations were needed.\n",
            "The last Sinkhorn iteration has converged: True\n",
            "The outer loop of Gromov Wasserstein has converged: True\n",
            "The final regularized GW cost is: 0.516\n",
            "Done running LEGWOT with ott\n",
            "running EGWL with ott\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "Label considered for Sinkhorn run\n",
            "lse step\n",
            "updating linearization\n",
            "Label considered for Sinkhorn run\n",
            "5 outer iterations were needed.\n",
            "The last Sinkhorn iteration has converged: True\n",
            "The outer loop of Gromov Wasserstein has converged: True\n",
            "The final regularized GW cost is: 0.455\n",
            "Done running LEGWOT with ott\n",
            "running EGWL with ott\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "Label considered for Sinkhorn run\n",
            "lse step\n",
            "updating linearization\n",
            "Label considered for Sinkhorn run\n",
            "5 outer iterations were needed.\n",
            "The last Sinkhorn iteration has converged: True\n",
            "The outer loop of Gromov Wasserstein has converged: True\n",
            "The final regularized GW cost is: 0.514\n",
            "Done running LEGWOT with ott\n"
          ]
        }
      ],
      "source": [
        "#EGWOT labeled\n",
        "legw = [perturbot.match.get_coupling_egw_labels_ott(\n",
        "        folds[i][\"data_train\"],\n",
        "        eps=0.001\n",
        "    ) for i in range(len(folds))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b572dfd1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b572dfd1",
        "outputId": "2a14253a-1c34-48c5-f3dc-4f79e8fd7143"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running EGWOT with ott\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "5 outer iterations were needed.\n",
            "The last Sinkhorn iteration has converged: True\n",
            "The outer loop of Gromov Wasserstein has converged: True\n",
            "The final regularized GW cost is: 0.014\n",
            "Done running EGWOT with ott\n",
            "running EGWOT with ott\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "5 outer iterations were needed.\n",
            "The last Sinkhorn iteration has converged: True\n",
            "The outer loop of Gromov Wasserstein has converged: True\n",
            "The final regularized GW cost is: 0.014\n",
            "Done running EGWOT with ott\n",
            "running EGWOT with ott\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "5 outer iterations were needed.\n",
            "The last Sinkhorn iteration has converged: True\n",
            "The outer loop of Gromov Wasserstein has converged: True\n",
            "The final regularized GW cost is: 0.017\n",
            "Done running EGWOT with ott\n",
            "running EGWOT with ott\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "5 outer iterations were needed.\n",
            "The last Sinkhorn iteration has converged: True\n",
            "The outer loop of Gromov Wasserstein has converged: True\n",
            "The final regularized GW cost is: 0.014\n",
            "Done running EGWOT with ott\n",
            "running EGWOT with ott\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "5 outer iterations were needed.\n",
            "The last Sinkhorn iteration has converged: True\n",
            "The outer loop of Gromov Wasserstein has converged: True\n",
            "The final regularized GW cost is: 0.017\n",
            "Done running EGWOT with ott\n"
          ]
        }
      ],
      "source": [
        "#EGWOT without labels\n",
        "egw = [perturbot.match.get_coupling_egw_all_ott(\n",
        "        folds[i][\"data_train\"],\n",
        "        eps=0.005\n",
        "    ) for i in range(len(folds))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b9facba5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9facba5",
        "outputId": "551ba29c-e0f7-44f7-8a55-f403f8007c8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n",
            "GW called\n",
            "lse step\n",
            "updating linearization\n",
            "lse step\n",
            "updating linearization\n"
          ]
        }
      ],
      "source": [
        "#EGWOT per labels\n",
        "egwper = [perturbot.match.get_coupling_egw_ott(\n",
        "        folds[i][\"data_train\"],\n",
        "        eps=0.050\n",
        "    ) for i in range(len(folds))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "de02cb06",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de02cb06",
        "outputId": "4723e535-d891-4384-cd52-6eeb8ac42cfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "M_0:2.0807757240709446 - 9.979260661462156\n",
            "M_1:1.9576554364051793 - 10.695716332959297\n",
            "M_2:1.9554863818289678 - 15.559588021754632\n",
            "M_3:2.1764447796976265 - 14.311995659199445\n",
            "M_4:2.136620590661646 - 9.661518660147577\n",
            "M_5:2.191343394354142 - 8.484049171487667\n",
            "M_6:2.114891508200784 - 13.851452273958127\n",
            "M_7:2.1945293772877617 - 11.035159084601723\n",
            "M_8:2.1087095675881575 - 13.0066007730346\n",
            "M_9:2.0609096425162714 - 12.281403566229883\n",
            "M:0.012201868744381105 - 823.2843595750833\n",
            "It 0 Delta: 0.06999999999999976  Loss: 25.67649195831666\n",
            "M_0:0.4581908439838984 - 8.110383495445143\n",
            "M_1:0.5504603209463741 - 8.467301514823104\n",
            "M_2:0.5131228929233251 - 11.46995449244077\n",
            "M_3:0.46855901588044757 - 12.274733507813181\n",
            "M_4:0.48363200042993215 - 9.545162977697828\n",
            "M_5:0.5243299476177494 - 8.077377835616801\n",
            "M_6:0.7770172026203339 - 11.938007828074328\n",
            "M_7:0.5382157399111311 - 10.708639275272043\n",
            "M_8:0.34614478425679573 - 9.715440865062266\n",
            "M_9:0.6072757733088914 - 12.458662415708435\n",
            "M:0.01217299043027878 - 721.9708691085183\n",
            "It 1 Delta: 0.07810249675906639  Loss: 19.99712314430541\n",
            "M_0:0.5037997828157246 - 10.088452938348201\n",
            "M_1:0.48619789697981686 - 9.571040723867718\n",
            "M_2:0.5644846466391993 - 11.539906896204997\n",
            "M_3:0.5832284834076682 - 14.526893121360931\n",
            "M_4:0.5416806658415245 - 9.951448821038444\n",
            "M_5:0.47546431069141226 - 8.226766601748079\n",
            "M_6:0.6687752377512726 - 12.52929195900561\n",
            "M_7:0.5596464950186057 - 10.103848326254239\n",
            "M_8:0.4476426024938913 - 10.88716133253922\n",
            "M_9:0.5642776399207334 - 13.295800680510272\n",
            "M:0.012035579505400008 - 724.3713081406183\n",
            "It 2 Delta: 0.058736700622353394  Loss: 18.561830188712424\n",
            "M_0:0.49039708969862517 - 11.205740180887066\n",
            "M_1:0.4526735265743915 - 10.606534307471012\n",
            "M_2:0.5863874772554718 - 11.489575577487951\n",
            "M_3:0.62789237645082 - 14.432353321641449\n",
            "M_4:0.5507637179183758 - 10.201117988286109\n",
            "M_5:0.45121099973851275 - 8.376241742449515\n",
            "M_6:0.5884814119069866 - 12.584320665391186\n",
            "M_7:0.5320362672681271 - 10.404005675049318\n",
            "M_8:0.40822538502263184 - 10.666541941211626\n",
            "M_9:0.5507356308782214 - 13.821377166963487\n",
            "M:0.012093835571698252 - 725.1939243041755\n",
            "It 3 Delta: 0.03807886552931923  Loss: 18.29410548374053\n",
            "M_0:0.5050970604447249 - 11.212221167517228\n",
            "M_1:0.4759592171531737 - 11.387160999735471\n",
            "M_2:0.5951681771357582 - 11.676894355035195\n",
            "M_3:0.6351902238350089 - 14.665086275179288\n",
            "M_4:0.5570024624834722 - 10.148519539007774\n",
            "M_5:0.4659090778083588 - 8.388168007659369\n",
            "M_6:0.5963958743154261 - 12.233691951377152\n",
            "M_7:0.5519877215521236 - 10.53038245681611\n",
            "M_8:0.4426599423709714 - 10.831163557656744\n",
            "M_9:0.522831288392132 - 13.583873130284083\n",
            "M:0.012100556197529175 - 728.4873169046042\n",
            "It 4 Delta: 0.033911649915626084  Loss: 18.187125635031215\n",
            "M_0:0.5020003154200556 - 11.214524631444965\n",
            "M_1:0.4672470859957889 - 11.508373174037187\n",
            "M_2:0.6032393825205462 - 11.573053735923597\n",
            "M_3:0.6140284021007116 - 14.744256205570833\n",
            "M_4:0.5438141115486923 - 10.138963697982605\n",
            "M_5:0.46058047044124084 - 8.055670484537107\n",
            "M_6:0.6041188689357324 - 12.127951908469658\n",
            "M_7:0.5378960004970936 - 10.630467844584992\n",
            "M_8:0.4376586767278181 - 10.805890649714113\n",
            "M_9:0.5297633409519398 - 13.281623390412317\n",
            "M:0.012044454723644495 - 728.2216718349667\n",
            "It 5 Delta: 0.02549509756796372  Loss: 18.128173850885418\n",
            "M_0:0.5119829205726796 - 11.303108636112032\n",
            "M_1:0.47781455659788663 - 11.623185251072252\n",
            "M_2:0.6085448207111048 - 11.629151911344552\n",
            "M_3:0.5979102329988217 - 14.905844514387791\n",
            "M_4:0.5624011726666511 - 10.154929929494132\n",
            "M_5:0.4727534925565149 - 8.044460022027303\n",
            "M_6:0.5966350215620779 - 12.27737402863158\n",
            "M_7:0.5474681378856436 - 10.659430551411049\n",
            "M_8:0.4462352010368704 - 10.866333600913087\n",
            "M_9:0.5695808165517902 - 13.269806192856667\n",
            "M:0.012044640141664177 - 727.8554100959213\n",
            "It 6 Delta: 0.01414213562373089  Loss: 18.127636668254215\n",
            "M_0:0.5110264177656658 - 11.264301032335132\n",
            "M_1:0.47862453364198365 - 11.607488676383506\n",
            "M_2:0.6125599009017666 - 11.677129160251177\n",
            "M_3:0.5891297350946871 - 14.892608764000766\n",
            "M_4:0.5639862828452431 - 10.185893249467846\n",
            "M_5:0.4714831660654857 - 8.03880050836264\n",
            "M_6:0.5963028790581832 - 12.190208628143006\n",
            "M_7:0.5488146394794491 - 10.679584050341917\n",
            "M_8:0.4462194347040622 - 10.871525075962555\n",
            "M_9:0.5696998940660014 - 13.34170332982289\n",
            "M:0.012046972712304226 - 728.0597382471503\n",
            "It 7 Delta: 4.729404707955036e-16  Loss: 18.127386791285524\n",
            "M_0:0.5110264177656663 - 11.26430103233513\n",
            "M_1:0.47862453364198343 - 11.607488676383502\n",
            "M_2:0.6125599009017668 - 11.677129160251177\n",
            "M_3:0.589129735094688 - 14.892608764000764\n",
            "M_4:0.5639862828452435 - 10.185893249467846\n",
            "M_5:0.4714831660654859 - 8.03880050836264\n",
            "M_6:0.5963028790581837 - 12.190208628143004\n",
            "M_7:0.5488146394794493 - 10.679584050341917\n",
            "M_8:0.44621943470406267 - 10.871525075962555\n",
            "M_9:0.5696998940660016 - 13.341703329822888\n",
            "M:0.012046972712304226 - 728.0597382471503\n",
            "It 8 Delta: 0.0  Loss: 18.127386791285524\n",
            "converged at iter  8\n",
            "M_0:2.0343119548619515 - 9.979260661462156\n",
            "M_1:2.0503325289238323 - 10.695716332959297\n",
            "M_2:1.9554863818289678 - 12.995803890626913\n",
            "M_3:2.1347339533359357 - 10.80348636182694\n",
            "M_4:1.939488590857749 - 9.661518660147577\n",
            "M_5:2.191343394354142 - 7.259263566979997\n",
            "M_6:2.114891508200784 - 13.730081311775844\n",
            "M_7:2.3370615013310396 - 11.4409246866428\n",
            "M_8:2.1087095675881575 - 13.025418884030364\n",
            "M_9:2.014817991176674 - 12.55720477288663\n",
            "M:0.012359307654778898 - 902.7415735386749\n",
            "It 0 Delta: 0.06999999999999972  Loss: 26.373504429201724\n",
            "M_0:0.5164191954446546 - 8.861938911632532\n",
            "M_1:0.6866315702851675 - 10.004164029014785\n",
            "M_2:0.6073849600175687 - 10.253500990296995\n",
            "M_3:0.4639340725884642 - 9.086062292245972\n",
            "M_4:0.5109770323514387 - 8.23865421026042\n",
            "M_5:0.488417856444306 - 7.122190760884797\n",
            "M_6:0.772245467886912 - 12.06464549035652\n",
            "M_7:0.5311681797323349 - 11.442154405554115\n",
            "M_8:0.3743770077195383 - 10.431240582688183\n",
            "M_9:0.5728529375681288 - 11.538194172547396\n",
            "M:0.013213215313386003 - 761.5056963443878\n",
            "It 1 Delta: 0.07745966692414813  Loss: 20.443988416019423\n",
            "M_0:0.4962398731637947 - 10.370141162337166\n",
            "M_1:0.7122833735251961 - 11.222446900637399\n",
            "M_2:0.6155728574070758 - 10.592058039078848\n",
            "M_3:0.5162942258592014 - 11.450527167011217\n",
            "M_4:0.5614022369848433 - 8.33333031319985\n",
            "M_5:0.5165157188842988 - 7.192186785394314\n",
            "M_6:0.7279096105678788 - 11.700510970919229\n",
            "M_7:0.6163874087587433 - 10.995422338588096\n",
            "M_8:0.438034229578812 - 11.306478547706327\n",
            "M_9:0.551418879163436 - 12.468879886621316\n",
            "M:0.013024550403538655 - 756.4712319415514\n",
            "It 2 Delta: 0.056124860801608854  Loss: 19.125457970205716\n",
            "M_0:0.5158976758257512 - 9.929193329660254\n",
            "M_1:0.6056378864777803 - 11.663902826390753\n",
            "M_2:0.6092092643396474 - 10.402546601609586\n",
            "M_3:0.5083357881825976 - 11.721764751083708\n",
            "M_4:0.5379040045772998 - 8.848868642260527\n",
            "M_5:0.5379197190731162 - 7.75206427746466\n",
            "M_6:0.7475420995411248 - 11.838721068152829\n",
            "M_7:0.6283521739108449 - 10.422408496531654\n",
            "M_8:0.48415449650178566 - 11.025084581967933\n",
            "M_9:0.5881386264172248 - 12.617720944333366\n",
            "M:0.012909644665261023 - 758.0918031286159\n",
            "It 3 Delta: 0.04358898943540649  Loss: 18.56878114746371\n",
            "M_0:0.4973071845584134 - 10.025604186475007\n",
            "M_1:0.5319766075622145 - 11.842830351463773\n",
            "M_2:0.6192659517561703 - 10.505995406228404\n",
            "M_3:0.4953633071938257 - 11.742495375801617\n",
            "M_4:0.5176599238213662 - 8.95232492855599\n",
            "M_5:0.5375472603758045 - 7.760756700288059\n",
            "M_6:0.7030844488856478 - 11.438262519131621\n",
            "M_7:0.6290227127594088 - 10.403536630277156\n",
            "M_8:0.46696896012505995 - 10.998195164014524\n",
            "M_9:0.5873318181736464 - 12.772874028096993\n",
            "M:0.012814991176702636 - 755.3444793174245\n",
            "It 4 Delta: 0.033911649915626084  Loss: 18.529677892755533\n",
            "M_0:0.4867183859046256 - 10.092044907822393\n",
            "M_1:0.5421897511041847 - 11.9936151196446\n",
            "M_2:0.6137934275059318 - 10.54918951964588\n",
            "M_3:0.503314790827116 - 11.59921492345781\n",
            "M_4:0.5079843781150184 - 9.08648035017916\n",
            "M_5:0.5397957158988373 - 7.722606891022909\n",
            "M_6:0.7319651283991879 - 11.235813841267532\n",
            "M_7:0.6199671726101457 - 10.380537976332256\n",
            "M_8:0.4578362590939853 - 10.906797864808855\n",
            "M_9:0.5894955159227175 - 12.91111077318654\n",
            "M:0.012812617202206396 - 753.8121151869515\n",
            "It 5 Delta: 0.03605551275463957  Loss: 18.51233001037528\n",
            "M_0:0.493126181746411 - 10.196820611862584\n",
            "M_1:0.5523011996000022 - 12.127034639781199\n",
            "M_2:0.6277031671879234 - 10.551692020764506\n",
            "M_3:0.5067455837087587 - 11.521188537497483\n",
            "M_4:0.5138263886056771 - 9.264728616027277\n",
            "M_5:0.5441175772994526 - 7.698039230280134\n",
            "M_6:0.7488402329819124 - 10.88207621824066\n",
            "M_7:0.6262443149673258 - 10.156921771337867\n",
            "M_8:0.47315089617658357 - 11.020942063389924\n",
            "M_9:0.5905460346594504 - 12.81156133541121\n",
            "M:0.01280727654715784 - 751.716805189303\n",
            "It 6 Delta: 0.03741657386773916  Loss: 18.47170921239849\n",
            "M_0:0.494896182699714 - 10.198109587006407\n",
            "M_1:0.557238784889027 - 12.165804404839223\n",
            "M_2:0.6081424886141522 - 10.50515005912143\n",
            "M_3:0.5175410710687238 - 11.535892309900785\n",
            "M_4:0.5144458226051749 - 9.308701692893544\n",
            "M_5:0.5414879476225787 - 7.670512651631832\n",
            "M_6:0.7674818826263365 - 10.81656725228286\n",
            "M_7:0.6240449008727817 - 10.058135309174942\n",
            "M_8:0.47817216516896144 - 10.999505230192813\n",
            "M_9:0.6033666631170218 - 12.729068755178949\n",
            "M:0.01278755560224857 - 752.0573079933539\n",
            "It 7 Delta: 0.030822070014844643  Loss: 18.462339204694022\n",
            "M_0:0.5032503626982148 - 10.502662305163705\n",
            "M_1:0.5531039316461626 - 12.017425179508107\n",
            "M_2:0.5752431637141577 - 10.422233125451063\n",
            "M_3:0.5344738237659239 - 11.468616944857471\n",
            "M_4:0.5438587639133463 - 9.369688685180426\n",
            "M_5:0.5352797021901714 - 7.66041416994256\n",
            "M_6:0.7536648530275261 - 10.840471304812286\n",
            "M_7:0.607711526511804 - 9.97584972573047\n",
            "M_8:0.46809283627509757 - 11.084613412760692\n",
            "M_9:0.6363213409155857 - 12.562932267710268\n",
            "M:0.01270760169684859 - 752.2304806922087\n",
            "It 8 Delta: 0.029154759474226206  Loss: 18.45186138518131\n",
            "M_0:0.5025176772831839 - 10.687183608259108\n",
            "M_1:0.5559713467465341 - 12.09603679402273\n",
            "M_2:0.5793044458110033 - 10.349181854030492\n",
            "M_3:0.5250072687937362 - 11.393841164187437\n",
            "M_4:0.5530159145510576 - 9.371349595064522\n",
            "M_5:0.5413352484189664 - 7.707337640205267\n",
            "M_6:0.7252258950971768 - 10.896595452707958\n",
            "M_7:0.6000026480643732 - 9.992681191626353\n",
            "M_8:0.45621925251509543 - 11.026122457167794\n",
            "M_9:0.6316374705839096 - 12.573728179062037\n",
            "M:0.012689426003193676 - 754.3626431509729\n",
            "It 9 Delta: 0.018708286933869313  Loss: 18.435625639361444\n",
            "M_0:0.4918018320477988 - 10.689673989165962\n",
            "M_1:0.5569320989050994 - 12.046708518598656\n",
            "M_2:0.5515571227952414 - 10.47005044167141\n",
            "M_3:0.5201284832364097 - 11.35930622332208\n",
            "M_4:0.5447142636116624 - 9.221970945007559\n",
            "M_5:0.5242134163508552 - 7.706683190418858\n",
            "M_6:0.7309635754222938 - 10.856594088652159\n",
            "M_7:0.5779893850337197 - 9.7485214858942\n",
            "M_8:0.43786543400905975 - 10.916137324166712\n",
            "M_9:0.6246284982292478 - 12.702610902575207\n",
            "M:0.012599857770040246 - 752.2817305441135\n",
            "It 10 Delta: 0.02121320343559605  Loss: 18.424264942333323\n",
            "M_0:0.48697107234794323 - 10.561391216577489\n",
            "M_1:0.5451736292220288 - 12.103477445178239\n",
            "M_2:0.5474580217267611 - 10.610445758494382\n",
            "M_3:0.5198098434813292 - 11.459533114631888\n",
            "M_4:0.5376416913855249 - 9.222838656415918\n",
            "M_5:0.5239705709064872 - 7.713936465018092\n",
            "M_6:0.7290657572073656 - 10.83618140178406\n",
            "M_7:0.5823835024892006 - 9.714979463455988\n",
            "M_8:0.43272765910894506 - 10.921939762509396\n",
            "M_9:0.6131957650933986 - 12.817753259566812\n",
            "M:0.012527515508563038 - 749.9921835658163\n",
            "It 11 Delta: 0.012247448713915794  Loss: 18.419100944489674\n",
            "M_0:0.48649516332528475 - 10.60214051745738\n",
            "M_1:0.5448262256394023 - 12.122283095942423\n",
            "M_2:0.5493309092680421 - 10.615806499171189\n",
            "M_3:0.5180586911651146 - 11.4270478575489\n",
            "M_4:0.5403223430817596 - 9.232539416030999\n",
            "M_5:0.5099215174492828 - 7.725852736586782\n",
            "M_6:0.7260125781933113 - 10.832555670051024\n",
            "M_7:0.5780369471199518 - 9.726374942153791\n",
            "M_8:0.4325685471736387 - 10.9128119815939\n",
            "M_9:0.620162625693309 - 12.859372237404203\n",
            "M:0.01251202746836724 - 749.9297295164517\n",
            "It 12 Delta: 6.66495117516187e-16  Loss: 18.418439074061205\n",
            "M_0:0.48649516332528475 - 10.60214051745738\n",
            "M_1:0.5448262256394023 - 12.122283095942425\n",
            "M_2:0.549330909268043 - 10.61580649917119\n",
            "M_3:0.5180586911651139 - 11.427047857548898\n",
            "M_4:0.540322343081759 - 9.232539416030999\n",
            "M_5:0.5099215174492833 - 7.725852736586778\n",
            "M_6:0.7260125781933109 - 10.832555670051024\n",
            "M_7:0.5780369471199527 - 9.726374942153793\n",
            "M_8:0.4325685471736387 - 10.912811981593904\n",
            "M_9:0.6201626256933088 - 12.859372237404203\n",
            "M:0.01251202746836724 - 749.9297295164517\n",
            "It 13 Delta: 0.0  Loss: 18.418439074061205\n",
            "converged at iter  13\n",
            "M_0:2.0343119548619515 - 9.979260661462156\n",
            "M_1:2.197622768586986 - 10.695716332959297\n",
            "M_2:1.9554863818289678 - 15.559588021754632\n",
            "M_3:2.0727790036552376 - 14.311995659199445\n",
            "M_4:1.939488590857749 - 9.661518660147577\n",
            "M_5:2.191343394354142 - 8.39000947866813\n",
            "M_6:2.114891508200784 - 13.439554693386917\n",
            "M_7:2.231275859910852 - 10.829313947485936\n",
            "M_8:2.1087095675881575 - 13.025418884030364\n",
            "M_9:2.014817991176674 - 9.48604980878497\n",
            "M:0.011997312776603085 - 829.3527329629344\n",
            "It 0 Delta: 0.06999999999999977  Loss: 25.496164325207204\n",
            "M_0:0.4518077719405955 - 8.049687241471533\n",
            "M_1:0.5374130399705861 - 8.36905148074469\n",
            "M_2:0.5251395416915434 - 12.144632537462835\n",
            "M_3:0.43482407628231745 - 13.307382937425254\n",
            "M_4:0.38030438314211534 - 8.126779726354266\n",
            "M_5:0.4851165531582238 - 7.225885132708748\n",
            "M_6:0.7753447577836194 - 10.54524557821671\n",
            "M_7:0.5247913396598514 - 7.959729207898287\n",
            "M_8:0.3707317397837464 - 10.1075960213681\n",
            "M_9:0.5567572514077581 - 12.138663795269933\n",
            "M:0.011668508006547909 - 762.73611649497\n",
            "It 1 Delta: 0.07713624310270735  Loss: 19.246164897792124\n",
            "M_0:0.5020369174885251 - 10.376744667458121\n",
            "M_1:0.5851921631300969 - 10.19300219980763\n",
            "M_2:0.582250275171424 - 12.550594146607246\n",
            "M_3:0.48440182140489063 - 15.238345999532484\n",
            "M_4:0.4762379119469693 - 9.206548249380033\n",
            "M_5:0.47680862069262364 - 7.941804846396725\n",
            "M_6:0.672500903397748 - 11.373757570435265\n",
            "M_7:0.5831360764626374 - 8.988730152859212\n",
            "M_8:0.5170038506552479 - 11.274416870917051\n",
            "M_9:0.5965834293656669 - 12.704927459852927\n",
            "M:0.011376794219933171 - 761.8394200673504\n",
            "It 2 Delta: 0.05244044240850745  Loss: 18.40541814131056\n",
            "M_0:0.48931036349696644 - 10.696653461900633\n",
            "M_1:0.598248250543882 - 10.533966244994463\n",
            "M_2:0.5799014421340747 - 12.453547869056003\n",
            "M_3:0.5042506439348098 - 15.245400760022346\n",
            "M_4:0.45451036587096216 - 9.175135612101018\n",
            "M_5:0.48303243104533156 - 7.734970100446643\n",
            "M_6:0.6853902807620305 - 11.468804777775489\n",
            "M_7:0.5140726719527153 - 9.09634859776303\n",
            "M_8:0.515223089638553 - 11.03674404532555\n",
            "M_9:0.5915962589113843 - 12.60361479472817\n",
            "M:0.011241042456819957 - 758.3418251765127\n",
            "It 3 Delta: 0.0430116263352128  Loss: 18.26164707948574\n",
            "M_0:0.4731417196338712 - 10.807137468251424\n",
            "M_1:0.5349956142549126 - 10.956953940063046\n",
            "M_2:0.5880199614694721 - 12.342032429764739\n",
            "M_3:0.4589897182880067 - 15.068518530236824\n",
            "M_4:0.4336874138012483 - 9.300557559118182\n",
            "M_5:0.48781531898850217 - 7.7984265764186285\n",
            "M_6:0.6945572326924632 - 11.497967278784477\n",
            "M_7:0.5296961431070162 - 9.120523593754667\n",
            "M_8:0.5080031694640397 - 10.940973692149878\n",
            "M_9:0.5616446466801788 - 12.224035309921703\n",
            "M:0.011257870401453757 - 754.7274805009109\n",
            "It 4 Delta: 0.029999999999999645  Loss: 18.176629775145656\n",
            "M_0:0.47138921026678293 - 10.876508842560813\n",
            "M_1:0.5384169199300097 - 11.086805007372872\n",
            "M_2:0.5887275564973273 - 12.343098509561262\n",
            "M_3:0.46153130680420684 - 15.053139309014501\n",
            "M_4:0.43202168363122917 - 9.362751106905524\n",
            "M_5:0.48914674179269513 - 7.776737055779482\n",
            "M_6:0.6960596038224245 - 11.511268810203397\n",
            "M_7:0.529137162483037 - 9.110013875587722\n",
            "M_8:0.5150594212163806 - 10.944268095250456\n",
            "M_9:0.5638981777541148 - 12.21753965186142\n",
            "M:0.011237269482976117 - 754.4615078260639\n",
            "It 5 Delta: 0.029999999999999635  Loss: 18.17109342823519\n",
            "M_0:0.5188623058091 - 10.875364290091833\n",
            "M_1:0.5596418222367967 - 11.320168091629778\n",
            "M_2:0.6119085376920985 - 12.43180960392143\n",
            "M_3:0.4840864905986437 - 15.0737882139866\n",
            "M_4:0.48031453572872107 - 9.317509750837115\n",
            "M_5:0.5280581365686778 - 7.504999899106185\n",
            "M_6:0.7345970913101305 - 11.396515832950191\n",
            "M_7:0.5844330290622151 - 9.274413940454995\n",
            "M_8:0.5845961339226997 - 10.945247339370637\n",
            "M_9:0.5972621108755154 - 12.323728892367845\n",
            "M:0.011298591480229297 - 752.3199253364086\n",
            "It 6 Delta: 0.038078865529319314  Loss: 18.103087667845717\n",
            "M_0:0.5025188014492818 - 10.55107534411277\n",
            "M_1:0.5395849638173478 - 11.52962206691641\n",
            "M_2:0.6027777784673476 - 12.068815346674594\n",
            "M_3:0.4728458205875705 - 15.090711363649813\n",
            "M_4:0.4603774102380005 - 9.358423282853433\n",
            "M_5:0.4918566586382851 - 7.6194564560075015\n",
            "M_6:0.7034220287097401 - 11.362631126113396\n",
            "M_7:0.5663825472413464 - 9.245541105423944\n",
            "M_8:0.5721007729758942 - 11.045142727478337\n",
            "M_9:0.5876289193206639 - 12.315731197911969\n",
            "M:0.011428349017239955 - 746.550590897572\n",
            "It 7 Delta: 0.040620192023179374  Loss: 18.02505829458252\n",
            "M_0:0.5058591797080263 - 10.855752648927286\n",
            "M_1:0.5502230590221899 - 11.594715541089485\n",
            "M_2:0.5618924624959458 - 11.728436958502773\n",
            "M_3:0.4890106935575922 - 15.11751127286328\n",
            "M_4:0.4779960294917789 - 9.358366671315437\n",
            "M_5:0.5045524161431612 - 7.581257161151597\n",
            "M_6:0.6911847159227276 - 11.343151658270791\n",
            "M_7:0.5541366200591156 - 9.217207014730892\n",
            "M_8:0.5849137186100022 - 10.99016628865579\n",
            "M_9:0.5814955079149013 - 12.501464506405714\n",
            "M:0.011475877540428886 - 746.3664274592207\n",
            "It 8 Delta: 0.03082207001484472  Loss: 17.97781952013404\n",
            "M_0:0.5070475858402219 - 10.897774356824327\n",
            "M_1:0.5460914672876838 - 11.546483460506463\n",
            "M_2:0.5519314841135414 - 11.648738226387671\n",
            "M_3:0.4934380354164405 - 15.081792932336803\n",
            "M_4:0.48202065518111015 - 9.376175646543812\n",
            "M_5:0.505168347124024 - 7.547597020198738\n",
            "M_6:0.6767414650821397 - 11.311440989453999\n",
            "M_7:0.5578239678189043 - 9.219836707706017\n",
            "M_8:0.5823156926490685 - 10.967840776342548\n",
            "M_9:0.6052977957948877 - 12.46427892739548\n",
            "M:0.01157711865685498 - 745.6244323138175\n",
            "It 9 Delta: 0.03082207001484464  Loss: 17.96876423643619\n",
            "M_0:0.5147834845864494 - 10.741555631005937\n",
            "M_1:0.5275274773269734 - 11.679728500567338\n",
            "M_2:0.5832104805278449 - 11.697728896493867\n",
            "M_3:0.4726779969050259 - 14.978597598528282\n",
            "M_4:0.4961056591006505 - 9.365452751398518\n",
            "M_5:0.5262408778653374 - 7.487601847729317\n",
            "M_6:0.6641477357773997 - 11.28981318273098\n",
            "M_7:0.5674556655769756 - 9.271177230869608\n",
            "M_8:0.5954590306399197 - 10.907484361564057\n",
            "M_9:0.6052735002654279 - 12.281452892140791\n",
            "M:0.011600403962256917 - 745.1121708449159\n",
            "It 10 Delta: 0.028284271247461516  Loss: 17.958548986988244\n",
            "M_0:0.512656222223075 - 10.73016013053528\n",
            "M_1:0.5273937585688933 - 11.683478395699211\n",
            "M_2:0.5855184077068509 - 11.609882610638708\n",
            "M_3:0.4756969607150998 - 14.965613691221499\n",
            "M_4:0.4926542741571165 - 9.412904208135107\n",
            "M_5:0.528259292542153 - 7.500179365734505\n",
            "M_6:0.6634728724977821 - 11.269008347515031\n",
            "M_7:0.564342824377857 - 9.276527074746854\n",
            "M_8:0.5994468915564681 - 10.923056577795503\n",
            "M_9:0.597375330222023 - 12.261346869951755\n",
            "M:0.011624578747173776 - 745.981609826778\n",
            "It 11 Delta: 0.021213203435596108  Loss: 17.956479372469094\n",
            "M_0:0.5137479171109676 - 10.740556096287573\n",
            "M_1:0.5292218215367566 - 11.679510370006785\n",
            "M_2:0.5834969775760923 - 11.590008498778507\n",
            "M_3:0.477713692685374 - 14.964298376184363\n",
            "M_4:0.49528155752779934 - 9.424945326580946\n",
            "M_5:0.5272162417776278 - 7.49543050887126\n",
            "M_6:0.6637687832713373 - 11.26111262366006\n",
            "M_7:0.564613294850643 - 9.27923932936605\n",
            "M_8:0.6008692308252968 - 10.920793418737826\n",
            "M_9:0.6000331252977749 - 12.263119734824212\n",
            "M:0.011618393946799813 - 747.2401166898583\n",
            "It 12 Delta: 0.009999999999999976  Loss: 17.954658948321665\n",
            "M_0:0.5137258758889724 - 10.745314364853604\n",
            "M_1:0.5286705779110881 - 11.682651785956946\n",
            "M_2:0.5835630643458596 - 11.594570289941927\n",
            "M_3:0.4776746952098705 - 14.950055272830706\n",
            "M_4:0.4887053767792042 - 9.429619268616975\n",
            "M_5:0.528446914374078 - 7.496383070926392\n",
            "M_6:0.663758013618835 - 11.283929797207541\n",
            "M_7:0.5645858872532368 - 9.281032557677037\n",
            "M_8:0.6009614977437914 - 10.924917720823187\n",
            "M_9:0.5981099226455606 - 12.259479358608548\n",
            "M:0.011622946981964566 - 747.1240700757439\n",
            "It 13 Delta: 0.014142135623730496  Loss: 17.953417950252632\n",
            "M_0:0.5139557113496931 - 10.744128648816474\n",
            "M_1:0.5284834212584519 - 11.687645746676546\n",
            "M_2:0.5851366778865041 - 11.5922278267473\n",
            "M_3:0.4777875604516204 - 14.947653453083156\n",
            "M_4:0.48886291536522686 - 9.43208900591101\n",
            "M_5:0.5284094970707716 - 7.500003595705772\n",
            "M_6:0.6644446132219746 - 11.285177336110719\n",
            "M_7:0.5653770498425956 - 9.281114209778043\n",
            "M_8:0.602085484388396 - 10.922664199815102\n",
            "M_9:0.5980863010300181 - 12.260984276665527\n",
            "M:0.011631928644473216 - 747.7277559368682\n",
            "It 14 Delta: 5.730787064327495e-16  Loss: 17.953409307435976\n",
            "M_0:0.5139557113496931 - 10.744128648816476\n",
            "M_1:0.5284834212584517 - 11.687645746676552\n",
            "M_2:0.5851366778865055 - 11.592227826747298\n",
            "M_3:0.4777875604516204 - 14.947653453083156\n",
            "M_4:0.4888629153652273 - 9.43208900591101\n",
            "M_5:0.5284094970707716 - 7.500003595705774\n",
            "M_6:0.6644446132219755 - 11.285177336110719\n",
            "M_7:0.5653770498425954 - 9.281114209778044\n",
            "M_8:0.6020854843883965 - 10.9226641998151\n",
            "M_9:0.5980863010300179 - 12.260984276665527\n",
            "M:0.011631928644473216 - 747.7277559368682\n",
            "It 15 Delta: 0.0  Loss: 17.953409307435976\n",
            "converged at iter  15\n",
            "M_0:2.059229489699611 - 8.88204221993624\n",
            "M_1:1.9576554364051793 - 10.695716332959297\n",
            "M_2:2.180254700975593 - 13.669234781765015\n",
            "M_3:2.0727790036552376 - 14.311995659199445\n",
            "M_4:1.9991594261090957 - 9.444535587146579\n",
            "M_5:2.2114097624794686 - 8.484049171487667\n",
            "M_6:2.114891508200784 - 13.851452273958127\n",
            "M_7:2.1945293772877617 - 11.4409246866428\n",
            "M_8:2.1087095675881575 - 13.025418884030364\n",
            "M_9:2.014817991176674 - 12.55720477288663\n",
            "M:0.011601827892151806 - 811.651166334183\n",
            "It 0 Delta: 0.0699999999999997  Loss: 25.80475600489607\n",
            "M_0:0.39341073465917464 - 7.4977631589067935\n",
            "M_1:0.5313132103305664 - 9.57257872796779\n",
            "M_2:0.5879916685591402 - 11.529105252550657\n",
            "M_3:0.4352589501687507 - 10.101744580292978\n",
            "M_4:0.3870712662505309 - 8.644051881534352\n",
            "M_5:0.4182421237037204 - 7.449248635086179\n",
            "M_6:0.7130859256656801 - 10.507830201155183\n",
            "M_7:0.5197368658499948 - 9.710835511523555\n",
            "M_8:0.4401000772987613 - 10.30100775750238\n",
            "M_9:0.5668073480039519 - 13.042461122806586\n",
            "M:0.011995053023736749 - 735.4183654330642\n",
            "It 1 Delta: 0.07713624310270736  Loss: 18.984122356180368\n",
            "M_0:0.5508280207905969 - 8.182815171678692\n",
            "M_1:0.6614673871969248 - 11.116675389364234\n",
            "M_2:0.6464995838495695 - 11.148431558403837\n",
            "M_3:0.48391104408692387 - 10.02195545396114\n",
            "M_4:0.4496138797441087 - 9.325851237379448\n",
            "M_5:0.5098471892338605 - 7.681022465494592\n",
            "M_6:0.7303685719040653 - 10.831159941394464\n",
            "M_7:0.5370236694360375 - 9.0230184210497\n",
            "M_8:0.5235955405293766 - 11.546751501442978\n",
            "M_9:0.6309681594946186 - 13.652101240131536\n",
            "M:0.011986264676012725 - 738.8314745242292\n",
            "It 2 Delta: 0.052915026221291545  Loss: 18.073597583132205\n",
            "M_0:0.5359379125915913 - 8.390841186488709\n",
            "M_1:0.6739036921950754 - 12.087081408070949\n",
            "M_2:0.6274984584152592 - 10.818501279117521\n",
            "M_3:0.5126926481927887 - 9.806437670652288\n",
            "M_4:0.4874031913009649 - 9.668183385387845\n",
            "M_5:0.5525540525534973 - 7.663271624239083\n",
            "M_6:0.7099370118490396 - 10.886699323201995\n",
            "M_7:0.5596025327260423 - 9.272246718414976\n",
            "M_8:0.5156369280223729 - 11.108240892615518\n",
            "M_9:0.6301730048722565 - 13.65979385038236\n",
            "M:0.011764411275683374 - 733.7263066332763\n",
            "It 3 Delta: 0.038729833462073995  Loss: 17.816829862650987\n",
            "M_0:0.5117841987063301 - 8.606672010403141\n",
            "M_1:0.6500509399435859 - 11.837216753334529\n",
            "M_2:0.6128461568089634 - 10.612971116951936\n",
            "M_3:0.5075803385320896 - 10.071868059057477\n",
            "M_4:0.4680393452487799 - 9.532154075271972\n",
            "M_5:0.5562513778677673 - 7.692978736132908\n",
            "M_6:0.6585773394579326 - 10.479791112436867\n",
            "M_7:0.524938682208818 - 9.347899374300534\n",
            "M_8:0.49963907888985437 - 11.06078242596124\n",
            "M_9:0.6085467403448386 - 13.39698622518971\n",
            "M:0.011778997442377115 - 733.2776145277597\n",
            "It 4 Delta: 0.04690415759823379  Loss: 17.71713211318913\n",
            "M_0:0.4984987828619678 - 8.590044658107969\n",
            "M_1:0.6462054485846997 - 11.957808219265765\n",
            "M_2:0.6003606847819483 - 10.55716083665326\n",
            "M_3:0.5120173626774323 - 10.029632059352137\n",
            "M_4:0.4800800779594454 - 9.708788986321512\n",
            "M_5:0.5723883873669884 - 7.712360909296598\n",
            "M_6:0.6304760393989275 - 10.124233476573071\n",
            "M_7:0.5092291015314754 - 9.26590400357003\n",
            "M_8:0.4812204879696602 - 10.999353821979476\n",
            "M_9:0.6069648869925264 - 13.428449847799703\n",
            "M:0.011778480412071296 - 731.1992368653009\n",
            "It 5 Delta: 0.02549509756796359  Loss: 17.690628491825226\n",
            "M_0:0.4996957281626413 - 8.64238294253055\n",
            "M_1:0.6466958875951148 - 11.933462073816552\n",
            "M_2:0.5955250215637053 - 10.546615420578233\n",
            "M_3:0.5117713220390838 - 10.032487604805551\n",
            "M_4:0.4774647449095242 - 9.684822047577999\n",
            "M_5:0.574316970935767 - 7.709742388029186\n",
            "M_6:0.6240433268279157 - 10.125521532530385\n",
            "M_7:0.5033492319638899 - 9.267396873639644\n",
            "M_8:0.483221468673416 - 10.998156297639744\n",
            "M_9:0.6101214329722544 - 13.400207742715837\n",
            "M:0.01181100453367369 - 730.3647519119748\n",
            "It 6 Delta: 0.014142135623730701  Loss: 17.68860841335384\n",
            "M_0:0.4996656475833463 - 8.642198980648647\n",
            "M_1:0.6466731365323923 - 11.933530443475817\n",
            "M_2:0.5955836493143547 - 10.55000506439234\n",
            "M_3:0.5118607704158025 - 10.030030474706908\n",
            "M_4:0.47749319924701394 - 9.685456829105828\n",
            "M_5:0.5741782067366437 - 7.709067340193711\n",
            "M_6:0.6237151737968816 - 10.125583027161156\n",
            "M_7:0.503320082495837 - 9.268957615376545\n",
            "M_8:0.4831882495306954 - 10.99657043138711\n",
            "M_9:0.61012048099757 - 13.399950707607884\n",
            "M:0.011814276079406808 - 730.1210194148656\n",
            "It 7 Delta: 2.4345532246906656e-16  Loss: 17.688586474581996\n",
            "M_0:0.4996656475833463 - 8.642198980648653\n",
            "M_1:0.6466731365323934 - 11.933530443475817\n",
            "M_2:0.5955836493143574 - 10.55000506439234\n",
            "M_3:0.5118607704158025 - 10.03003047470691\n",
            "M_4:0.47749319924701417 - 9.685456829105824\n",
            "M_5:0.5741782067366437 - 7.709067340193701\n",
            "M_6:0.6237151737968816 - 10.12558302716116\n",
            "M_7:0.503320082495837 - 9.268957615376545\n",
            "M_8:0.4831882495306967 - 10.996570431387118\n",
            "M_9:0.6101204809975715 - 13.399950707607893\n",
            "M:0.011814276079406808 - 730.1210194148656\n",
            "It 8 Delta: 0.0  Loss: 17.688586474581996\n",
            "converged at iter  8\n",
            "M_0:2.0343119548619515 - 9.979260661462156\n",
            "M_1:1.9576554364051793 - 9.344667519351924\n",
            "M_2:2.0743910017369056 - 15.559588021754632\n",
            "M_3:2.0727790036552376 - 14.311995659199445\n",
            "M_4:1.939488590857749 - 8.866318218303968\n",
            "M_5:2.196600378884352 - 8.484049171487667\n",
            "M_6:2.2312843036858996 - 13.851452273958127\n",
            "M_7:2.1945293772877617 - 11.4409246866428\n",
            "M_8:2.371500515437197 - 12.363459611261229\n",
            "M_9:2.0703188533234127 - 12.55720477288663\n",
            "M:0.012047344463724697 - 894.4007667333075\n",
            "It 0 Delta: 0.06999999999999976  Loss: 25.503288804432568\n",
            "M_0:0.41977332818453506 - 8.126322177071975\n",
            "M_1:0.5744500089798419 - 8.056787915183481\n",
            "M_2:0.5905582278944972 - 11.859114928369836\n",
            "M_3:0.4639220136191742 - 12.270157350792473\n",
            "M_4:0.37604995967423505 - 7.834047371551321\n",
            "M_5:0.4520363736534354 - 7.618888761803354\n",
            "M_6:0.7077686191074739 - 11.41869085818583\n",
            "M_7:0.54299632716641 - 11.002621345324597\n",
            "M_8:0.4904140174610707 - 10.236835118756726\n",
            "M_9:0.7258950157336299 - 13.321957040187643\n",
            "M:0.01250823932998284 - 772.7640271863303\n",
            "It 1 Delta: 0.07810249675906637  Loss: 18.722219480415507\n",
            "M_0:0.5421909066750337 - 9.104258058930991\n",
            "M_1:0.68272160591044 - 8.684150258748248\n",
            "M_2:0.5989934109884676 - 11.449077519288121\n",
            "M_3:0.4944107974034577 - 14.378736545382477\n",
            "M_4:0.48358722836599477 - 8.061994030038264\n",
            "M_5:0.5321877209400434 - 8.38838557871455\n",
            "M_6:0.6929003970715719 - 11.52113278596893\n",
            "M_7:0.5453576859561633 - 10.503998275950197\n",
            "M_8:0.6391772619426366 - 10.209237507160687\n",
            "M_9:0.8439991869517525 - 13.976922806726677\n",
            "M:0.013003668225551698 - 776.0773843969808\n",
            "It 2 Delta: 0.052915026221291614  Loss: 17.879912410106986\n",
            "M_0:0.5434551568984967 - 9.187297848526889\n",
            "M_1:0.6485474733210046 - 8.658590410233836\n",
            "M_2:0.607907301557034 - 11.333087885402048\n",
            "M_3:0.5137849923213835 - 14.47481716815893\n",
            "M_4:0.47523767368217196 - 8.20564323221558\n",
            "M_5:0.523340003049692 - 8.217398081687323\n",
            "M_6:0.6417082411561732 - 11.514680321346319\n",
            "M_7:0.5182472087004124 - 10.281011378994958\n",
            "M_8:0.6561097982941311 - 10.237554759781812\n",
            "M_9:0.761676992113371 - 13.944918650272783\n",
            "M:0.012314525410070463 - 777.2655426055893\n",
            "It 3 Delta: 0.04690415759823414  Loss: 17.74023980981879\n",
            "M_0:0.5198236106582175 - 9.069947260586753\n",
            "M_1:0.5883490426122409 - 8.549574225420569\n",
            "M_2:0.5853188191644769 - 11.104926203226706\n",
            "M_3:0.49828247906638223 - 14.624051173256959\n",
            "M_4:0.4651127259761625 - 8.335015048976441\n",
            "M_5:0.5070760995555943 - 7.9136517309695575\n",
            "M_6:0.6406205228307194 - 11.623570246603654\n",
            "M_7:0.5199139034622837 - 9.834182063187598\n",
            "M_8:0.6405635637558569 - 10.009662515852197\n",
            "M_9:0.6894035296435144 - 13.676348207232877\n",
            "M:0.011679460832983421 - 774.3098765159336\n",
            "It 4 Delta: 0.03535533905932711  Loss: 17.6872288962456\n",
            "M_0:0.5044242067340075 - 9.142641518824398\n",
            "M_1:0.5751135073295939 - 8.423990490000937\n",
            "M_2:0.5808044903895422 - 10.977786778365545\n",
            "M_3:0.49944994785465124 - 14.72892387866572\n",
            "M_4:0.4740725557723786 - 8.446900844454705\n",
            "M_5:0.5197572285988652 - 8.05903709079249\n",
            "M_6:0.6147834011918749 - 11.509172104476287\n",
            "M_7:0.5238933176619824 - 9.971369596531112\n",
            "M_8:0.6590217416970665 - 9.985985053905818\n",
            "M_9:0.6791040247935836 - 13.819211414812116\n",
            "M:0.011023280939837947 - 771.6096512241825\n",
            "It 5 Delta: 0.03535533905932708  Loss: 17.66227309692765\n",
            "M_0:0.5032901115851929 - 9.278036511048674\n",
            "M_1:0.565027014841404 - 8.445991232029872\n",
            "M_2:0.5795478608703677 - 10.974940696978205\n",
            "M_3:0.5023945058136285 - 14.93782403590832\n",
            "M_4:0.465663200652624 - 8.58378134959679\n",
            "M_5:0.5257289957772937 - 8.06670560996015\n",
            "M_6:0.6204955485803985 - 11.521452750460131\n",
            "M_7:0.5248293124016739 - 9.950527533021619\n",
            "M_8:0.6651954763564636 - 10.037242412307837\n",
            "M_9:0.6674363988188861 - 13.77401782033558\n",
            "M:0.010852125089859138 - 770.6013637330695\n",
            "It 6 Delta: 0.02449489742783161  Loss: 17.654150042395607\n",
            "M_0:0.5106739887446166 - 9.31113500854224\n",
            "M_1:0.5755496044632522 - 8.448556596197944\n",
            "M_2:0.5683323678155616 - 10.962517869011409\n",
            "M_3:0.5040761429730813 - 14.956488720496214\n",
            "M_4:0.46592461159940357 - 8.624868843079364\n",
            "M_5:0.5285290180325788 - 8.076997751907042\n",
            "M_6:0.6199731239683386 - 11.512542043779368\n",
            "M_7:0.5232628666267887 - 9.932140374144554\n",
            "M_8:0.670219137875883 - 10.040945096196687\n",
            "M_9:0.6800962010436238 - 13.728799015277174\n",
            "M:0.010839130333452986 - 769.4045063552644\n",
            "It 7 Delta: 0.01732050807568852  Loss: 17.64933477699615\n",
            "M_0:0.5099517171780925 - 9.309146762726076\n",
            "M_1:0.5716996339031679 - 8.446588005000633\n",
            "M_2:0.5662577427653399 - 10.955837326585364\n",
            "M_3:0.5051270895143125 - 14.955123961524146\n",
            "M_4:0.46419994435853473 - 8.614281614821055\n",
            "M_5:0.5276920374121836 - 8.077816841425722\n",
            "M_6:0.6200830667934811 - 11.509324728184374\n",
            "M_7:0.5224687673153401 - 9.933108449929826\n",
            "M_8:0.6680271256690815 - 10.04158641639835\n",
            "M_9:0.6781299324302525 - 13.731185162391188\n",
            "M:0.010911706556911496 - 769.2826961747242\n",
            "It 8 Delta: 4.491560549236345e-16  Loss: 17.649257141918916\n",
            "M_0:0.5099517171780925 - 9.309146762726073\n",
            "M_1:0.571699633903169 - 8.446588005000637\n",
            "M_2:0.5662577427653395 - 10.955837326585335\n",
            "M_3:0.5051270895143125 - 14.955123961524158\n",
            "M_4:0.46419994435853495 - 8.614281614821065\n",
            "M_5:0.5276920374121836 - 8.077816841425722\n",
            "M_6:0.6200830667934816 - 11.509324728184376\n",
            "M_7:0.5224687673153408 - 9.933108449929822\n",
            "M_8:0.668027125669082 - 10.041586416398353\n",
            "M_9:0.6781299324302537 - 13.7311851623912\n",
            "M:0.010911706556911496 - 769.2826961747242\n",
            "It 9 Delta: 0.0  Loss: 17.649257141918916\n",
            "converged at iter  9\n"
          ]
        }
      ],
      "source": [
        "#COOT labeled\n",
        "cotl= [perturbot.match.get_coupling_cotl(\n",
        "        folds[i][\"data_train\"],\n",
        "    ) for i in range(len(folds))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "68b8fb4c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68b8fb4c",
        "outputId": "294db2df-6dc3-449a-8378-94094749748c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "calculating with eps 0.01\n",
            "M_0:2.681655854678219 - 10.588568142066041\n",
            "lse step\n",
            "M_1:2.6879365420784818 - 11.206690694341459\n",
            "lse step\n",
            "M_2:2.551115168056964 - 12.587408323173422\n",
            "lse step\n",
            "M_3:3.0134638950953736 - 13.125055993762468\n",
            "lse step\n",
            "M_4:3.043866030269688 - 13.903979301580849\n",
            "lse step\n",
            "M_5:2.523920643321646 - 11.653478548143733\n",
            "lse step\n",
            "M_6:2.913235280682311 - 16.53657575265407\n",
            "lse step\n",
            "M_7:2.8232613686742365 - 15.127454092680663\n",
            "lse step\n",
            "M_8:2.5162272236145498 - 10.640903265906534\n",
            "lse step\n",
            "M_9:2.836039545645148 - 13.457203319610564\n",
            "lse step\n",
            "M:8.116556698363331e-05 - 1596.986501753395\n",
            "lse step\n",
            "It 0 Delta: 0.013767839217219684  Loss: 42.418918655422\n",
            "M_0:1.1599790762433138 - 11.134167453337694\n",
            "lse step\n",
            "M_1:1.1774822964379092 - 8.648858133099928\n",
            "lse step\n",
            "M_2:1.228801757788319 - 12.439884767921182\n",
            "lse step\n",
            "M_3:1.379328107177072 - 11.400863323819623\n",
            "lse step\n",
            "M_4:1.5504132897141307 - 13.039834572399785\n",
            "lse step\n",
            "M_5:1.2646746218477307 - 10.24598963736837\n",
            "lse step\n",
            "M_6:1.4503250382529038 - 14.425931771315765\n",
            "lse step\n",
            "M_7:1.370434384365678 - 13.082450636216688\n",
            "lse step\n",
            "M_8:1.5155610748734964 - 10.669700845848539\n",
            "lse step\n",
            "M_9:1.4493849299154942 - 11.432057276183786\n",
            "lse step\n",
            "M:6.894540368980595e-05 - 1659.6006544046818\n",
            "lse step\n",
            "It 1 Delta: 0.005537957884371281  Loss: 38.43103216740159\n",
            "M_0:1.185564172541489 - 12.253672795425281\n",
            "lse step\n",
            "M_1:1.2048054604198482 - 8.849379931658492\n",
            "lse step\n",
            "M_2:1.2100385253172719 - 12.643264340714824\n",
            "lse step\n",
            "M_3:1.407328894675816 - 11.977902353923486\n",
            "lse step\n",
            "M_4:1.4902890497774757 - 14.197435470100137\n",
            "lse step\n",
            "M_5:1.2796782629198118 - 10.63219095558819\n",
            "lse step\n",
            "M_6:1.5014724961536867 - 15.112167731772965\n",
            "lse step\n",
            "M_7:1.4126287722224375 - 13.063647537628515\n",
            "lse step\n",
            "M_8:1.5002618225505506 - 11.648256871542323\n",
            "lse step\n",
            "M_9:1.464046418211964 - 12.439002968937405\n",
            "lse step\n",
            "M:6.826744947034726e-05 - 1679.9424454132948\n",
            "lse step\n",
            "It 2 Delta: 0.0025958027690649033  Loss: 36.96001442798061\n",
            "M_0:1.1882381690816677 - 12.319506351088394\n",
            "lse step\n",
            "M_1:1.222438270090059 - 9.050711278255301\n",
            "lse step\n",
            "M_2:1.227665488132462 - 12.953069829442573\n",
            "lse step\n",
            "M_3:1.437215867863736 - 11.989940749178912\n",
            "lse step\n",
            "M_4:1.4793050108106107 - 13.970305273241658\n",
            "lse step\n",
            "M_5:1.309247333330495 - 10.65417220959445\n",
            "lse step\n",
            "M_6:1.5119830780922894 - 15.096524286715908\n",
            "lse step\n",
            "M_7:1.4262598254340804 - 13.059334639371697\n",
            "lse step\n",
            "M_8:1.5056368044246666 - 11.710907065639836\n",
            "lse step\n",
            "M_9:1.4733953375367008 - 12.515410095698092\n",
            "lse step\n",
            "M:6.883284685491555e-05 - 1682.764396241877\n",
            "lse step\n",
            "It 3 Delta: 0.001682577538304031  Loss: 36.453914351899314\n",
            "M_0:1.1823532724664254 - 12.30425307466834\n",
            "lse step\n",
            "M_1:1.2221951257843113 - 9.190686051303844\n",
            "lse step\n",
            "M_2:1.2303000319517619 - 13.131305664681454\n",
            "lse step\n",
            "M_3:1.443282290285181 - 11.957506502501658\n",
            "lse step\n",
            "M_4:1.464312221082511 - 13.717004877436286\n",
            "lse step\n",
            "M_5:1.3094824325665186 - 10.707740360795952\n",
            "lse step\n",
            "M_6:1.5115188079162802 - 15.001073973638306\n",
            "lse step\n",
            "M_7:1.4335481668831767 - 13.086907649712028\n",
            "lse step\n",
            "M_8:1.5111292010908874 - 11.663303376035834\n",
            "lse step\n",
            "M_9:1.4675527509986563 - 12.616315917131331\n",
            "lse step\n",
            "M:6.975622019797525e-05 - 1682.376168962675\n",
            "lse step\n",
            "It 4 Delta: 0.0010613886406645179  Loss: 36.20410829206654\n",
            "M_0:1.1794409079900006 - 12.300504870530062\n",
            "lse step\n",
            "M_1:1.2195055612597938 - 9.230862017970814\n",
            "lse step\n",
            "M_2:1.2265119892121998 - 13.190999409893077\n",
            "lse step\n",
            "M_3:1.4403656854199849 - 11.930805583824613\n",
            "lse step\n",
            "M_4:1.4592019046160787 - 13.565853352208343\n",
            "lse step\n",
            "M_5:1.3056140185425824 - 10.764422358203163\n",
            "lse step\n",
            "M_6:1.5135161221645643 - 14.927045093615567\n",
            "lse step\n",
            "M_7:1.4363385525486574 - 13.15614487562344\n",
            "lse step\n",
            "M_8:1.5145817026189778 - 11.617582395404625\n",
            "lse step\n",
            "M_9:1.4602882854362185 - 12.69157066775731\n",
            "lse step\n",
            "M:7.076540661376682e-05 - 1681.6678908619542\n",
            "lse step\n",
            "It 5 Delta: 0.0008385566179640591  Loss: 36.07218620447613\n",
            "M_0:1.1769852609091973 - 12.294501583688437\n",
            "lse step\n",
            "M_1:1.2169739051016681 - 9.22302963550579\n",
            "lse step\n",
            "M_2:1.2218703137876712 - 13.196177922850556\n",
            "lse step\n",
            "M_3:1.4349330408402627 - 11.910433814870085\n",
            "lse step\n",
            "M_4:1.4593239031299512 - 13.476204605632166\n",
            "lse step\n",
            "M_5:1.2991938788219741 - 10.813758460816485\n",
            "lse step\n",
            "M_6:1.5142951207686823 - 14.87539102189211\n",
            "lse step\n",
            "M_7:1.4316548745664295 - 13.261132026920972\n",
            "lse step\n",
            "M_8:1.5142453891424439 - 11.587792838066951\n",
            "lse step\n",
            "M_9:1.4547659596719642 - 12.751462712930639\n",
            "lse step\n",
            "M:7.168032591465711e-05 - 1681.1304034100222\n",
            "lse step\n",
            "It 6 Delta: 0.0007397670997306705  Loss: 35.98180837518028\n",
            "M_0:1.174788727307051 - 12.283677126350057\n",
            "lse step\n",
            "M_1:1.215007080273903 - 9.202189230793383\n",
            "lse step\n",
            "M_2:1.2181671867599795 - 13.187057261489329\n",
            "lse step\n",
            "M_3:1.4291990382053323 - 11.89589680198058\n",
            "lse step\n",
            "M_4:1.4604698525927176 - 13.422028582085018\n",
            "lse step\n",
            "M_5:1.2933640437890244 - 10.858254651664586\n",
            "lse step\n",
            "M_6:1.5140913560067104 - 14.834645432525694\n",
            "lse step\n",
            "M_7:1.4266886751692318 - 13.386392274470877\n",
            "lse step\n",
            "M_8:1.511819686390417 - 11.571578652439\n",
            "lse step\n",
            "M_9:1.4507463237891878 - 12.801006225868408\n",
            "lse step\n",
            "M:7.247071538923634e-05 - 1680.74971052328\n",
            "lse step\n",
            "It 7 Delta: 0.0006542416522279382  Loss: 35.90888210947982\n",
            "M_0:1.172969775275399 - 12.27100350526282\n",
            "lse step\n",
            "M_1:1.213404314766896 - 9.18177975735269\n",
            "lse step\n",
            "M_2:1.2156941163009074 - 13.171949522753044\n",
            "lse step\n",
            "M_3:1.4238975094421766 - 11.88600443103153\n",
            "lse step\n",
            "M_4:1.4611489529310369 - 13.391783535528372\n",
            "lse step\n",
            "M_5:1.2886582245898421 - 10.901268317149894\n",
            "lse step\n",
            "M_6:1.5131998207809585 - 14.798977456509203\n",
            "lse step\n",
            "M_7:1.4219082046442038 - 13.52137062586557\n",
            "lse step\n",
            "M_8:1.5087199649942922 - 11.564896769272323\n",
            "lse step\n",
            "M_9:1.4476261309687708 - 12.841530116674896\n",
            "lse step\n",
            "M:7.314918875567834e-05 - 1680.4738231826161\n",
            "lse step\n",
            "It 8 Delta: 0.0005829131114296615  Loss: 35.847878526302196\n",
            "M_0:1.1715657397338595 - 12.258954210902273\n",
            "lse step\n",
            "M_1:1.2119572723051677 - 9.165767120517074\n",
            "lse step\n",
            "M_2:1.2143014130079546 - 13.152250143586894\n",
            "lse step\n",
            "M_3:1.4192637675543516 - 11.87943658919095\n",
            "lse step\n",
            "M_4:1.4611030461414556 - 13.379337881093665\n",
            "lse step\n",
            "M_5:1.2849435838575984 - 10.944784116694365\n",
            "lse step\n",
            "M_6:1.511807796205337 - 14.766397745339903\n",
            "lse step\n",
            "M_7:1.417450473793796 - 13.659368779271917\n",
            "lse step\n",
            "M_8:1.505707796679156 - 11.564936699598395\n",
            "lse step\n",
            "M_9:1.4450473675103426 - 12.873092782774995\n",
            "lse step\n",
            "M:7.373116057191002e-05 - 1680.2542294939644\n",
            "lse step\n",
            "It 9 Delta: 0.0005242488696239889  Loss: 35.79724035223155\n",
            "M_0:1.1705563852700676 - 12.248987552481838\n",
            "lse step\n",
            "M_1:1.2105172836047207 - 9.154388204419883\n",
            "lse step\n",
            "M_2:1.2136764711822725 - 13.138360971000562\n",
            "lse step\n",
            "M_3:1.415270309806435 - 11.875286439311886\n",
            "lse step\n",
            "M_4:1.4604622842870087 - 13.379919584320726\n",
            "lse step\n",
            "M_5:1.2819330680006487 - 10.98876746766072\n",
            "lse step\n",
            "M_6:1.5101522416698017 - 14.736650773093949\n",
            "lse step\n",
            "M_7:1.413334393604835 - 13.79543459103617\n",
            "lse step\n",
            "M_8:1.5030935677243649 - 11.569470472643427\n",
            "lse step\n",
            "M_9:1.4428113758928556 - 12.895813207709182\n",
            "lse step\n",
            "M:7.422389823891962e-05 - 1680.0572477122244\n",
            "lse step\n",
            "It 10 Delta: 0.0004696560790762305  Loss: 35.75626628047953\n",
            "M_0:1.1698891474883506 - 12.241557898416035\n",
            "lse step\n",
            "M_1:1.2090316287121892 - 9.146788246434001\n",
            "lse step\n",
            "M_2:1.2135389829792513 - 13.154234729868694\n",
            "lse step\n",
            "M_3:1.4118547482338575 - 11.872883012690817\n",
            "lse step\n",
            "M_4:1.459459155416246 - 13.389331784446469\n",
            "lse step\n",
            "M_5:1.279413695722397 - 11.0319060463427\n",
            "lse step\n",
            "M_6:1.5084388085836307 - 14.709910865991805\n",
            "lse step\n",
            "M_7:1.4095941261846716 - 13.925120706236402\n",
            "lse step\n",
            "M_8:1.5009929339292774 - 11.576658222230503\n",
            "lse step\n",
            "M_9:1.4408532495771802 - 12.910848761507934\n",
            "lse step\n",
            "M:7.463213339456404e-05 - 1679.8592225398854\n",
            "lse step\n",
            "It 11 Delta: 0.0004150700115133077  Loss: 35.724494807133304\n",
            "M_0:1.1694920495176724 - 12.236457039443948\n",
            "lse step\n",
            "M_1:1.207509721529245 - 9.142109807258528\n",
            "lse step\n",
            "M_2:1.2136801543694113 - 13.169833391333984\n",
            "lse step\n",
            "M_3:1.4089688353084158 - 11.871775057227188\n",
            "lse step\n",
            "M_4:1.4582978623118374 - 13.404139317199519\n",
            "lse step\n",
            "M_5:1.2772599415487327 - 11.072656355519864\n",
            "lse step\n",
            "M_6:1.5068110259029175 - 14.686218753556103\n",
            "lse step\n",
            "M_7:1.4062639806464574 - 14.044944902996567\n",
            "lse step\n",
            "M_8:1.4994055840528078 - 11.585063052025838\n",
            "lse step\n",
            "M_9:1.4391577431841585 - 12.919864526290224\n",
            "lse step\n",
            "M:7.496031344465118e-05 - 1679.6549067042722\n",
            "lse step\n",
            "It 12 Delta: 0.00036171244573779404  Loss: 35.70050775072874\n",
            "M_0:1.1692936110407388 - 12.233168333599666\n",
            "lse step\n",
            "M_1:1.2059979109728012 - 9.139733645695202\n",
            "lse step\n",
            "M_2:1.2139556525558977 - 13.184767766585878\n",
            "lse step\n",
            "M_3:1.406574895029839 - 11.871663558855865\n",
            "lse step\n",
            "M_4:1.457122501994586 - 13.421760822760511\n",
            "lse step\n",
            "M_5:1.2754048311717194 - 11.109966001333827\n",
            "lse step\n",
            "M_6:1.5053525296329613 - 14.665366163957339\n",
            "lse step\n",
            "M_7:1.4033695598965181 - 14.152729034463393\n",
            "lse step\n",
            "M_8:1.4982714197210327 - 11.59368518091616\n",
            "lse step\n",
            "M_9:1.4377131873979512 - 12.924578750427678\n",
            "lse step\n",
            "M:7.521621233505612e-05 - 1679.4433281411193\n",
            "lse step\n",
            "It 13 Delta: 0.00031201724777929485  Loss: 35.6829630668349\n",
            "M_0:1.1692262603650334 - 12.231088194082108\n",
            "lse step\n",
            "M_1:1.2045437226817584 - 9.13912662615177\n",
            "lse step\n",
            "M_2:1.2142712788249206 - 13.198708274167416\n",
            "lse step\n",
            "M_3:1.4046220372497262 - 11.872282090155052\n",
            "lse step\n",
            "M_4:1.456023187358292 - 13.440340250911069\n",
            "lse step\n",
            "M_5:1.2738050520670239 - 11.143352415123086\n",
            "lse step\n",
            "M_6:1.5040948019424647 - 14.64701160984773\n",
            "lse step\n",
            "M_7:1.40090505153577 - 14.247581935198708\n",
            "lse step\n",
            "M_8:1.497501701983608 - 11.601899143650307\n",
            "lse step\n",
            "M_9:1.4364972906342015 - 12.92639386042262\n",
            "lse step\n",
            "M:7.540993143457141e-05 - 1679.2286045195033\n",
            "lse step\n",
            "It 14 Delta: 0.0002674950519576669  Loss: 35.670446399958934\n",
            "M_0:1.1692370915527428 - 12.229718629773819\n",
            "lse step\n",
            "M_1:1.2031856511074754 - 9.139796291473035\n",
            "lse step\n",
            "M_2:1.214573046174047 - 13.211418087239336\n",
            "lse step\n",
            "M_3:1.403049802234597 - 11.873385557916635\n",
            "lse step\n",
            "M_4:1.4550482986434576 - 13.458640891522203\n",
            "lse step\n",
            "M_5:1.2724286596651542 - 11.172777003131937\n",
            "lse step\n",
            "M_6:1.5030377766535032 - 14.630831102295234\n",
            "lse step\n",
            "M_7:1.3988421058896907 - 14.329674742277264\n",
            "lse step\n",
            "M_8:1.4970065785663833 - 11.609377369336894\n",
            "lse step\n",
            "M_9:1.435481204968656 - 12.926358719703194\n",
            "lse step\n",
            "M:7.55545339655685e-05 - 1679.0201185640713\n",
            "lse step\n",
            "It 15 Delta: 0.00022838314180262387  Loss: 35.66159885618152\n",
            "M_0:1.169282799781155 - 12.228816569788586\n",
            "lse step\n",
            "M_1:1.2019496941851242 - 9.141308008296988\n",
            "lse step\n",
            "M_2:1.214836437401188 - 13.222774241859023\n",
            "lse step\n",
            "M_3:1.4018018569829862 - 11.874770781433513\n",
            "lse step\n",
            "M_4:1.4542170273891322 - 13.475999693075266\n",
            "lse step\n",
            "M_5:1.2712483123358829 - 11.198449781056095\n",
            "lse step\n",
            "M_6:1.5021609904142719 - 14.616657657209885\n",
            "lse step\n",
            "M_7:1.3971403576916945 - 14.39988012597447\n",
            "lse step\n",
            "M_8:1.496707880251356 - 11.61595776361506\n",
            "lse step\n",
            "M_9:1.4346268962462052 - 12.925170795691836\n",
            "lse step\n",
            "M:7.565730619479258e-05 - 1678.8175749064335\n",
            "lse step\n",
            "It 16 Delta: 0.00019451366097200662  Loss: 35.655475200160886\n",
            "M_0:1.1693524534637676 - 12.228036985505542\n",
            "lse step\n",
            "M_1:1.2008437848327769 - 9.143263435308569\n",
            "lse step\n",
            "M_2:1.2150531553654806 - 13.232723483129549\n",
            "lse step\n",
            "M_3:1.4008042525253326 - 11.876243228260998\n",
            "lse step\n",
            "M_4:1.4535248882133955 - 13.491831687615647\n",
            "lse step\n",
            "M_5:1.2702419893008483 - 11.220657951280254\n",
            "lse step\n",
            "M_6:1.5014479845133182 - 14.604156272377498\n",
            "lse step\n",
            "M_7:1.3957414294036101 - 14.459399351574069\n",
            "lse step\n",
            "M_8:1.496542822115748 - 11.621707098601805\n",
            "lse step\n",
            "M_9:1.4339228096493266 - 12.923408887061312\n",
            "lse step\n",
            "M:7.572926924111902e-05 - 1678.6338972297704\n",
            "lse step\n",
            "It 17 Delta: 0.00016528299602214247  Loss: 35.651652625148664\n",
            "M_0:1.169434222473921 - 12.227203389255477\n",
            "lse step\n",
            "M_1:1.1998690207770777 - 9.145364568382119\n",
            "lse step\n",
            "M_2:1.2152180294285262 - 13.241182484683907\n",
            "lse step\n",
            "M_3:1.4000007273722717 - 11.877738822839223\n",
            "lse step\n",
            "M_4:1.4529458673369273 - 13.506035464130818\n",
            "lse step\n",
            "M_5:1.2693735110340225 - 11.239892801394763\n",
            "lse step\n",
            "M_6:1.5008937567001563 - 14.59345142094076\n",
            "lse step\n",
            "M_7:1.394598513705658 - 14.509524351793816\n",
            "lse step\n",
            "M_8:1.4964715566032214 - 11.62663371224712\n",
            "lse step\n",
            "M_9:1.433346288488436 - 12.921037359354884\n",
            "lse step\n",
            "M:7.577767428965469e-05 - 1678.463620115062\n",
            "lse step\n",
            "It 18 Delta: 0.0001398927124682814  Loss: 35.64874094941688\n",
            "M_0:1.1695034096028278 - 12.226437020609945\n",
            "lse step\n",
            "M_1:1.1990234896030345 - 9.1475171240748\n",
            "lse step\n",
            "M_2:1.215352817875645 - 13.248468638656952\n",
            "lse step\n",
            "M_3:1.3993668490242168 - 11.87913598006318\n",
            "lse step\n",
            "M_4:1.4524918535793816 - 13.518540738626061\n",
            "lse step\n",
            "M_5:1.2686486183421666 - 11.256347389645077\n",
            "lse step\n",
            "M_6:1.500428994742189 - 14.583985145426206\n",
            "lse step\n",
            "M_7:1.3936721760208692 - 14.551615825173988\n",
            "lse step\n",
            "M_8:1.4964506337158618 - 11.63082758969411\n",
            "lse step\n",
            "M_9:1.4328683463573557 - 12.918846717156482\n",
            "lse step\n",
            "M:7.58099945774266e-05 - 1678.3103804431253\n",
            "lse step\n",
            "It 19 Delta: 0.00011818326311185956  Loss: 35.64668011132065\n",
            "M_0:1.1695640038802049 - 12.22571533118808\n",
            "lse step\n",
            "M_1:1.1982944285390935 - 9.149560664495032\n",
            "lse step\n",
            "M_2:1.2154554619824074 - 13.254679217735786\n",
            "lse step\n",
            "M_3:1.3988566068628592 - 11.88040657823359\n",
            "lse step\n",
            "M_4:1.4521273669217867 - 13.529440911461206\n",
            "lse step\n",
            "M_5:1.2680355278743898 - 11.27036630430253\n",
            "lse step\n",
            "M_6:1.5000500707266755 - 14.57572928509216\n",
            "lse step\n",
            "M_7:1.3929182111241944 - 14.586894074590127\n",
            "lse step\n",
            "M_8:1.4964625693919407 - 11.634368907799939\n",
            "lse step\n",
            "M_9:1.4324708459153153 - 12.916772016525028\n",
            "lse step\n",
            "M:7.583084425362235e-05 - 1678.174577438373\n",
            "lse step\n",
            "It 20 Delta: 9.961871546693146e-05  Loss: 35.64519542411212\n",
            "M_0:1.1696174506858619 - 12.225027093863067\n",
            "lse step\n",
            "M_1:1.1976717578006781 - 9.151428947184812\n",
            "lse step\n",
            "M_2:1.215534207771267 - 13.259938350450271\n",
            "lse step\n",
            "M_3:1.3984451119911085 - 11.881537611427222\n",
            "lse step\n",
            "M_4:1.451838894567865 - 13.538852270222328\n",
            "lse step\n",
            "M_5:1.2675197765051416 - 11.282270538901438\n",
            "lse step\n",
            "M_6:1.4997437407344378 - 14.568576220005182\n",
            "lse step\n",
            "M_7:1.392305853215856 - 14.616424491350408\n",
            "lse step\n",
            "M_8:1.496493567097436 - 11.637349709539109\n",
            "lse step\n",
            "M_9:1.4321425907248106 - 12.914866223775478\n",
            "lse step\n",
            "M:7.584368874746088e-05 - 1678.05535915804\n",
            "lse step\n",
            "It 21 Delta: 8.380000508623198e-05  Loss: 35.64411878887118\n",
            "M_0:1.169663142778004 - 12.224373886495504\n",
            "lse step\n",
            "M_1:1.1971424920732787 - 9.153095299581391\n",
            "lse step\n",
            "M_2:1.2155934963312136 - 13.264368401687504\n",
            "lse step\n",
            "M_3:1.3981100720081505 - 11.882530255238803\n",
            "lse step\n",
            "M_4:1.4516097226658022 - 13.546916495677939\n",
            "lse step\n",
            "M_5:1.2670858144391204 - 11.2923491054544\n",
            "lse step\n",
            "M_6:1.4994956922345262 - 14.562407134458184\n",
            "lse step\n",
            "M_7:1.3918071962365512 - 14.641117853097366\n",
            "lse step\n",
            "M_8:1.4965329234614668 - 11.639845751017823\n",
            "lse step\n",
            "M_9:1.4318711875446644 - 12.913154460827242\n",
            "lse step\n",
            "M:7.58510583235083e-05 - 1677.9517498760704\n",
            "lse step\n",
            "It 22 Delta: 7.037059549475089e-05  Loss: 35.64332872164017\n",
            "M_0:1.1697011329093814 - 12.22376390036852\n",
            "lse step\n",
            "M_1:1.1966938376734326 - 9.154554452507933\n",
            "lse step\n",
            "M_2:1.215637370193519 - 13.268092038410755\n",
            "lse step\n",
            "M_3:1.3978352828088878 - 11.88339335486292\n",
            "lse step\n",
            "M_4:1.451427363599238 - 13.553785138763395\n",
            "lse step\n",
            "M_5:1.266720124001739 - 11.300860046519674\n",
            "lse step\n",
            "M_6:1.4992938681719887 - 14.557109710910922\n",
            "lse step\n",
            "M_7:1.3914000601283996 - 14.661760060214283\n",
            "lse step\n",
            "M_8:1.4965747140555796 - 11.641932965755975\n",
            "lse step\n",
            "M_9:1.4316466793507594 - 12.91164716463203\n",
            "lse step\n",
            "M:7.58547361713758e-05 - 1677.8622677357591\n",
            "lse step\n",
            "It 23 Delta: 5.9005724324379116e-05  Loss: 35.64273095441318\n",
            "M_0:1.1697326217045643 - 12.22320287764116\n",
            "lse step\n",
            "M_1:1.1963149491517862 - 9.155817438594791\n",
            "lse step\n",
            "M_2:1.2156697904437996 - 13.271218263995372\n",
            "lse step\n",
            "M_3:1.3976097632257678 - 11.88413789459125\n",
            "lse step\n",
            "M_4:1.4512832970857685 - 13.559607416368902\n",
            "lse step\n",
            "M_5:1.2664126934589137 - 11.308033172204148\n",
            "lse step\n",
            "M_6:1.4991294138945315 - 14.552575556615734\n",
            "lse step\n",
            "M_7:1.39106710821014 - 14.679010456257782\n",
            "lse step\n",
            "M_8:1.496615851916804 - 11.643673362148785\n",
            "lse step\n",
            "M_9:1.4314614545073332 - 12.910341526620291\n",
            "lse step\n",
            "M:7.58560093610311e-05 - 1677.7853717027533\n",
            "lse step\n",
            "It 24 Delta: 4.9416557885706425e-05  Loss: 35.64228050189696\n",
            "M_0:1.1697586488539828 - 12.222693144333268\n",
            "lse step\n",
            "M_1:1.195995892938497 - 9.156900234957215\n",
            "lse step\n",
            "M_2:1.215692876706478 - 13.273840339736548\n",
            "lse step\n",
            "M_3:1.3974228770779575 - 11.884777424419054\n",
            "lse step\n",
            "M_4:1.4511680998541894 - 13.564524336870045\n",
            "lse step\n",
            "M_5:1.2661540060231258 - 11.314067395577693\n",
            "lse step\n",
            "M_6:1.498995463728039 - 14.548704678853957\n",
            "lse step\n",
            "M_7:1.3907943007407502 - 14.693425050293722\n",
            "lse step\n",
            "M_8:1.4966541004228866 - 11.645121042282787\n",
            "lse step\n",
            "M_9:1.431308204509419 - 12.909223443772348\n",
            "lse step\n",
            "M:7.585574063951575e-05 - 1677.7196347831962\n",
            "lse step\n",
            "It 25 Delta: 4.134197297389619e-05  Loss: 35.641916968421995\n",
            "M_0:1.16978052484463 - 12.222232158830149\n",
            "lse step\n",
            "M_1:1.1957281589823283 - 9.157822694104032\n",
            "lse step\n",
            "M_2:1.2157094697968338 - 13.27603754244027\n",
            "lse step\n",
            "M_3:1.3972683963744246 - 11.88532420952962\n",
            "lse step\n",
            "M_4:1.4510774952835104 - 13.568658835624195\n",
            "lse step\n",
            "M_5:1.2659368912613753 - 11.319137058238633\n",
            "lse step\n",
            "M_6:1.4988867049208765 - 14.545405766196513\n",
            "lse step\n",
            "M_7:1.390570833963167 - 14.7054715421756\n",
            "lse step\n",
            "M_8:1.4966889805543357 - 11.646321600938737\n",
            "lse step\n",
            "M_9:1.4311820699332172 - 12.908274423570004\n",
            "lse step\n",
            "M:7.585454323426739e-05 - 1677.6635258106103\n",
            "lse step\n",
            "It 26 Delta: 3.4559256164357066e-05  Loss: 35.64164430458344\n",
            "M_0:1.16979708229506 - 12.221822569228657\n",
            "lse step\n",
            "M_1:1.1955020786806034 - 9.158603768148538\n",
            "lse step\n",
            "M_2:1.2157198137492289 - 13.27787858501449\n",
            "lse step\n",
            "M_3:1.3971393137508499 - 11.8857919309899\n",
            "lse step\n",
            "M_4:1.4510042315068379 - 13.572129508864515\n",
            "lse step\n",
            "M_5:1.2657529914986916 - 11.323391690022333\n",
            "lse step\n",
            "M_6:1.4987961088144846 - 14.542602467799837\n",
            "lse step\n",
            "M_7:1.3903858120531603 - 14.715537036837791\n",
            "lse step\n",
            "M_8:1.4967194057843056 - 11.647319072557103\n",
            "lse step\n",
            "M_9:1.4310772020909226 - 12.907476296734814\n",
            "lse step\n",
            "M:7.585280515281403e-05 - 1677.6158820550854\n",
            "lse step\n",
            "It 27 Delta: 2.8873226256109774e-05  Loss: 35.64140003731297\n",
            "M_0:1.1698118342214552 - 12.2214569789263\n",
            "lse step\n",
            "M_1:1.1953137885617175 - 9.159263219260152\n",
            "lse step\n",
            "M_2:1.2157268820446003 - 13.279421642591357\n",
            "lse step\n",
            "M_3:1.3970321745528045 - 11.886189846394256\n",
            "lse step\n",
            "M_4:1.4509473618720814 - 13.575033070149635\n",
            "lse step\n",
            "M_5:1.2655994092854348 - 11.326958857583124\n",
            "lse step\n",
            "M_6:1.4987237477962565 - 14.540220788691366\n",
            "lse step\n",
            "M_7:1.3902346713457052 - 14.723952029726046\n",
            "lse step\n",
            "M_8:1.4967464335937075 - 11.648142364933957\n",
            "lse step\n",
            "M_9:1.4309912204848743 - 12.906808963559122\n",
            "lse step\n",
            "M:7.58507988704381e-05 - 1677.5754031093556\n",
            "lse step\n",
            "It 28 Delta: 2.4107706849463284e-05  Loss: 35.64121324037844\n",
            "M_0:1.1698233877685646 - 12.221135031179834\n",
            "lse step\n",
            "M_1:1.1951555688288273 - 9.159818966403144\n",
            "lse step\n",
            "M_2:1.2157303869327813 - 13.280715490372506\n",
            "lse step\n",
            "M_3:1.3969424902892877 - 11.886529611098037\n",
            "lse step\n",
            "M_4:1.450901504098809 - 13.577458063594293\n",
            "lse step\n",
            "M_5:1.2654694834173599 - 11.329949472572833\n",
            "lse step\n",
            "M_6:1.4986637816726347 - 14.538200633529344\n",
            "lse step\n",
            "M_7:1.3901094591490115 - 14.730986391892793\n",
            "lse step\n",
            "M_8:1.4967696222511748 - 11.64882247279501\n",
            "lse step\n",
            "M_9:1.4309201013931394 - 12.90625525620581\n",
            "lse step\n",
            "M:7.584869100061524e-05 - 1677.541154359204\n",
            "lse step\n",
            "It 29 Delta: 2.012199183809571e-05  Loss: 35.64105384303216\n",
            "M_0:1.16983250516585 - 12.220851404479463\n",
            "lse step\n",
            "M_1:1.19502290501446 - 9.160285395130057\n",
            "lse step\n",
            "M_2:1.2157312800169984 - 13.281801313412556\n",
            "lse step\n",
            "M_3:1.396867221915486 - 11.886818909371334\n",
            "lse step\n",
            "M_4:1.4508652115978347 - 13.579479759588395\n",
            "lse step\n",
            "M_5:1.2653597132565735 - 11.332454888230757\n",
            "lse step\n",
            "M_6:1.4986145749581885 - 14.536486696643442\n",
            "lse step\n",
            "M_7:1.39000606258605 - 14.736870046483762\n",
            "lse step\n",
            "M_8:1.4967893591066401 - 11.64938307820418\n",
            "lse step\n",
            "M_9:1.430861159634197 - 12.905798012911303\n",
            "lse step\n",
            "M:7.584658702084639e-05 - 1677.5122207056545\n",
            "lse step\n",
            "It 30 Delta: 1.679285560385324e-05  Loss: 35.64091891229141\n",
            "M_0:1.169839319067682 - 12.220601709150259\n",
            "lse step\n",
            "M_1:1.1949112252505445 - 9.160676871149299\n",
            "lse step\n",
            "M_2:1.2157299191677364 - 13.282710645238796\n",
            "lse step\n",
            "M_3:1.3968035344059904 - 11.887066664007909\n",
            "lse step\n",
            "M_4:1.4508354632958649 - 13.581159403897454\n",
            "lse step\n",
            "M_5:1.2652663949645322 - 11.334553698265388\n",
            "lse step\n",
            "M_6:1.49857358039369 - 14.535034152844178\n",
            "lse step\n",
            "M_7:1.3899199282631844 - 14.741790658692768\n",
            "lse step\n",
            "M_8:1.496805953512946 - 11.649842852807113\n",
            "lse step\n",
            "M_9:1.4308120504510735 - 12.905419488441186\n",
            "lse step\n",
            "M:7.584457090132136e-05 - 1677.4876554089585\n",
            "lse step\n",
            "It 31 Delta: 1.4006249330122955e-05  Loss: 35.64080575844319\n",
            "M_0:1.169845453117419 - 12.2203834973674\n",
            "lse step\n",
            "M_1:1.1948184629503118 - 9.16100528280748\n",
            "lse step\n",
            "M_2:1.215727428857792 - 13.283475774661246\n",
            "lse step\n",
            "M_3:1.3967503484936226 - 11.887277570378668\n",
            "lse step\n",
            "M_4:1.45081239864377 - 13.58255641657636\n",
            "lse step\n",
            "M_5:1.2651882097873237 - 11.33631296548156\n",
            "lse step\n",
            "M_6:1.4985410167365147 - 14.533804117622374\n",
            "lse step\n",
            "M_7:1.3898491749862207 - 14.745907087281683\n",
            "lse step\n",
            "M_8:1.496820122711729 - 11.650222220861043\n",
            "lse step\n",
            "M_9:1.4307719455561447 - 12.905114158891857\n",
            "lse step\n",
            "M:7.584265106655364e-05 - 1677.4669477679327\n",
            "lse step\n",
            "It 32 Delta: 1.1683163393172435e-05  Loss: 35.64070842650479\n",
            "M_0:1.1698493973429864 - 12.220189973738982\n",
            "lse step\n",
            "M_1:1.1947398636883686 - 9.16128003126514\n",
            "lse step\n",
            "M_2:1.215723677802406 - 13.284115825110936\n",
            "lse step\n",
            "M_3:1.3967055647394935 - 11.887458018388386\n",
            "lse step\n",
            "M_4:1.4507939523418498 - 13.583711408335704\n",
            "lse step\n",
            "M_5:1.2651210751704243 - 11.337785033875175\n",
            "lse step\n",
            "M_6:1.4985129994250541 - 14.53276029354226\n",
            "lse step\n",
            "M_7:1.3897899401572449 - 14.749352746862025\n",
            "lse step\n",
            "M_8:1.4968320859958886 - 11.650529915124629\n",
            "lse step\n",
            "M_9:1.430738513498785 - 12.904861106300283\n",
            "lse step\n",
            "M:7.584086366033198e-05 - 1677.4495229167856\n",
            "lse step\n",
            "It 33 Delta: 9.74503291217843e-06  Loss: 35.640611679476194\n",
            "M_0:1.1698538706598511 - 12.220021112794338\n",
            "lse step\n",
            "M_1:1.1946755502771516 - 9.161510223361164\n",
            "lse step\n",
            "M_2:1.2157199520093556 - 13.284655544304817\n",
            "lse step\n",
            "M_3:1.3966682230443839 - 11.887611614653151\n",
            "lse step\n",
            "M_4:1.45078051238412 - 13.584669349560588\n",
            "lse step\n",
            "M_5:1.2650656294447913 - 11.339019931687485\n",
            "lse step\n",
            "M_6:1.4984920956488397 - 14.531875956227182\n",
            "lse step\n",
            "M_7:1.3897421274987796 - 14.752238827540038\n",
            "lse step\n",
            "M_8:1.496842352536659 - 11.650782316408062\n",
            "lse step\n",
            "M_9:1.430711635927131 - 12.904660036055015\n",
            "lse step\n",
            "M:7.583920655316944e-05 - 1677.4348176664162\n",
            "lse step\n",
            "It 34 Delta: 8.133214578265324e-06  Loss: 35.64052850724016\n",
            "M_0:1.16985657467767 - 12.219874315251612\n",
            "lse step\n",
            "M_1:1.1946207052746822 - 9.161702778971186\n",
            "lse step\n",
            "M_2:1.215715253955323 - 13.285110348758565\n",
            "lse step\n",
            "M_3:1.396636515564586 - 11.887743622948834\n",
            "lse step\n",
            "M_4:1.4507695773968128 - 13.585460894032774\n",
            "lse step\n",
            "M_5:1.2650176862118072 - 11.340054954203453\n",
            "lse step\n",
            "M_6:1.4984738681741279 - 14.531125911772305\n",
            "lse step\n",
            "M_7:1.3897016991672375 - 14.754656948866597\n",
            "lse step\n",
            "M_8:1.49685082522213 - 11.650988101461031\n",
            "lse step\n",
            "M_9:1.4306891112489726 - 12.904497245983684\n",
            "lse step\n",
            "M:7.583769119493663e-05 - 1677.422321210361\n",
            "lse step\n",
            "It 35 Delta: 6.784610377508216e-06  Loss: 35.64045897649795\n",
            "M_0:1.1698587379278866 - 12.219743708838326\n",
            "lse step\n",
            "M_1:1.1945750473925123 - 9.161863355832295\n",
            "lse step\n",
            "M_2:1.2157103479441422 - 13.285490960269447\n",
            "lse step\n",
            "M_3:1.3966097073432924 - 11.887856554566529\n",
            "lse step\n",
            "M_4:1.4507613416028335 - 13.586112266515402\n",
            "lse step\n",
            "M_5:1.264977123477471 - 11.34092214815908\n",
            "lse step\n",
            "M_6:1.4984592798776726 - 14.530489541452898\n",
            "lse step\n",
            "M_7:1.3896682912515426 - 14.75668285091433\n",
            "lse step\n",
            "M_8:1.4968578350205857 - 11.65115463036755\n",
            "lse step\n",
            "M_9:1.4306705959881147 - 12.904366477155444\n",
            "lse step\n",
            "M:7.583631511155321e-05 - 1677.4118338344808\n",
            "lse step\n",
            "It 36 Delta: 5.659615453623701e-06  Loss: 35.640394752012476\n",
            "M_0:1.169860820644387 - 12.219629646743027\n",
            "lse step\n",
            "M_1:1.1945371888315912 - 9.161997855983799\n",
            "lse step\n",
            "M_2:1.215705384350878 - 13.285812041457643\n",
            "lse step\n",
            "M_3:1.3965872074550776 - 11.887953513354391\n",
            "lse step\n",
            "M_4:1.4507551159531347 - 13.586648953910831\n",
            "lse step\n",
            "M_5:1.2649428653091497 - 11.34164943947575\n",
            "lse step\n",
            "M_6:1.498447843588294 - 14.529950334266113\n",
            "lse step\n",
            "M_7:1.3896408913904643 - 14.758380265870636\n",
            "lse step\n",
            "M_8:1.4968639041120326 - 11.651288665036164\n",
            "lse step\n",
            "M_9:1.430655619289911 - 12.904263973556727\n",
            "lse step\n",
            "M:7.583506141975686e-05 - 1677.4029585651226\n",
            "lse step\n",
            "It 37 Delta: 4.724015525425784e-06  Loss: 35.64034560894048\n",
            "M_0:1.1698611011995532 - 12.219529864896097\n",
            "lse step\n",
            "M_1:1.194504120248337 - 9.162111112730996\n",
            "lse step\n",
            "M_2:1.215699318399634 - 13.286081987066414\n",
            "lse step\n",
            "M_3:1.3965672389509765 - 11.888038558487503\n",
            "lse step\n",
            "M_4:1.4507486026352114 - 13.587090667621448\n",
            "lse step\n",
            "M_5:1.2649123058370924 - 11.342260230734128\n",
            "lse step\n",
            "M_6:1.4984369487364306 - 14.529490976118627\n",
            "lse step\n",
            "M_7:1.389616939194772 - 14.759802261386481\n",
            "lse step\n",
            "M_8:1.4968683978224557 - 11.651396588936063\n",
            "lse step\n",
            "M_9:1.4306423440657297 - 12.904181470219639\n",
            "lse step\n",
            "M:7.583391222316974e-05 - 1677.3954779743426\n",
            "lse step\n",
            "It 38 Delta: 3.946755896322429e-06  Loss: 35.64029582351841\n",
            "M_0:1.1698620888081313 - 12.219441552347405\n",
            "lse step\n",
            "M_1:1.194477316217206 - 9.16220549771343\n",
            "lse step\n",
            "M_2:1.215694255858137 - 13.286311112629189\n",
            "lse step\n",
            "M_3:1.3965511707706777 - 11.88811098277949\n",
            "lse step\n",
            "M_4:1.4507452208777667 - 13.587452113710956\n",
            "lse step\n",
            "M_5:1.2648872380316845 - 11.34277279895236\n",
            "lse step\n",
            "M_6:1.4984292370879126 - 14.52910024273999\n",
            "lse step\n",
            "M_7:1.3895980648084545 - 14.760997610163741\n",
            "lse step\n",
            "M_8:1.496872754986335 - 11.651483013883851\n",
            "lse step\n",
            "M_9:1.4306322045982083 - 12.904118245344309\n",
            "lse step\n",
            "M:7.583286939009248e-05 - 1677.3891086647895\n",
            "lse step\n",
            "It 39 Delta: 3.297679540992249e-06  Loss: 35.64025279257426\n",
            "M_0:1.1698623648271849 - 12.219364115188933\n",
            "lse step\n",
            "M_1:1.1944544022929 - 9.162284109119577\n",
            "lse step\n",
            "M_2:1.2156889669628017 - 13.286503644697895\n",
            "lse step\n",
            "M_3:1.3965373730306567 - 11.888173652737976\n",
            "lse step\n",
            "M_4:1.4507424709385477 - 13.587747623960983\n",
            "lse step\n",
            "M_5:1.2648653692095193 - 11.343202769383428\n",
            "lse step\n",
            "M_6:1.4984223937351886 - 14.52876793185074\n",
            "lse step\n",
            "M_7:1.389581926770951 - 14.761999594302669\n",
            "lse step\n",
            "M_8:1.4968761859811115 - 11.651551439722883\n",
            "lse step\n",
            "M_9:1.4306236420396246 - 12.904069405587672\n",
            "lse step\n",
            "M:7.583192625610944e-05 - 1677.383814249923\n",
            "lse step\n",
            "It 40 Delta: 2.762631993391551e-06  Loss: 35.6402083241122\n",
            "M_0:1.1698631382171225 - 12.219295329525183\n",
            "lse step\n",
            "M_1:1.1944358503019836 - 9.16235010813253\n",
            "lse step\n",
            "M_2:1.215684263776732 - 13.286667774080547\n",
            "lse step\n",
            "M_3:1.3965257758533907 - 11.888227454576182\n",
            "lse step\n",
            "M_4:1.450741208144367 - 13.587989869198736\n",
            "lse step\n",
            "M_5:1.2648471568745356 - 11.34356483251275\n",
            "lse step\n",
            "M_6:1.4984178546803522 - 14.52848325907131\n",
            "lse step\n",
            "M_7:1.3895691068718918 - 14.762844232071886\n",
            "lse step\n",
            "M_8:1.4968791034498434 - 11.651605485819843\n",
            "lse step\n",
            "M_9:1.4306170388912045 - 12.9040335712618\n",
            "lse step\n",
            "M:7.583106870431409e-05 - 1677.3792327427427\n",
            "lse step\n",
            "It 41 Delta: 2.310388481419068e-06  Loss: 35.640177658503326\n",
            "M_0:1.1698635856886648 - 12.219235923090936\n",
            "lse step\n",
            "M_1:1.1944201614326033 - 9.162405591328653\n",
            "lse step\n",
            "M_2:1.2156794297620033 - 13.286806901066239\n",
            "lse step\n",
            "M_3:1.3965155631425663 - 11.8882743384505\n",
            "lse step\n",
            "M_4:1.4507398414199701 - 13.588188314037774\n",
            "lse step\n",
            "M_5:1.2648313601417907 - 11.343870036775801\n",
            "lse step\n",
            "M_6:1.4984142472902695 - 14.528242019788767\n",
            "lse step\n",
            "M_7:1.3895584424437215 - 14.763552776849439\n",
            "lse step\n",
            "M_8:1.4968813566551287 - 11.651649069663325\n",
            "lse step\n",
            "M_9:1.430611405939861 - 12.90400875006619\n",
            "lse step\n",
            "M:7.583029346011245e-05 - 1677.3753775623502\n",
            "lse step\n",
            "It 42 Delta: 1.9376502677914687e-06  Loss: 35.64014019344506\n",
            "M_0:1.1698635320521873 - 12.219179644909529\n",
            "lse step\n",
            "M_1:1.1944067462369938 - 9.162452336353766\n",
            "lse step\n",
            "M_2:1.2156748424949768 - 13.286921379830858\n",
            "lse step\n",
            "M_3:1.3965071147136912 - 11.888315437972242\n",
            "lse step\n",
            "M_4:1.450739150425941 - 13.588345768122304\n",
            "lse step\n",
            "M_5:1.264817622345289 - 11.3441268956781\n",
            "lse step\n",
            "M_6:1.4984109252327942 - 14.528033917742919\n",
            "lse step\n",
            "M_7:1.3895493494050903 - 14.764148859489646\n",
            "lse step\n",
            "M_8:1.496883429887948 - 11.65168031402703\n",
            "lse step\n",
            "M_9:1.4306067636781672 - 12.903986674961562\n",
            "lse step\n",
            "M:7.582959188125316e-05 - 1677.3721356630767\n",
            "lse step\n",
            "It 43 Delta: 1.6283745480905054e-06  Loss: 35.64010858393215\n",
            "M_0:1.1698638872354246 - 12.21913266500904\n",
            "lse step\n",
            "M_1:1.194395783271485 - 9.16249098696828\n",
            "lse step\n",
            "M_2:1.215670914382285 - 13.287021434738316\n",
            "lse step\n",
            "M_3:1.3965004370958412 - 11.8883508106565\n",
            "lse step\n",
            "M_4:1.4507396812039297 - 13.588475331002686\n",
            "lse step\n",
            "M_5:1.2648062514086664 - 11.344343140377214\n",
            "lse step\n",
            "M_6:1.4984087355561253 - 14.52785738902791\n",
            "lse step\n",
            "M_7:1.389542125170402 - 14.764651805720833\n",
            "lse step\n",
            "M_8:1.4968854234502684 - 11.65170688500742\n",
            "lse step\n",
            "M_9:1.4306036226244865 - 12.90397590405512\n",
            "lse step\n",
            "M:7.582895588227201e-05 - 1677.3694056817048\n",
            "lse step\n",
            "It 44 Delta: 1.3719435401071678e-06  Loss: 35.64007988967992\n",
            "M_0:1.1698641179947402 - 12.219089400788082\n",
            "lse step\n",
            "M_1:1.1943867083849622 - 9.162523509691864\n",
            "lse step\n",
            "M_2:1.2156668644699258 - 13.287105532211957\n",
            "lse step\n",
            "M_3:1.3964941327471054 - 11.88838164702793\n",
            "lse step\n",
            "M_4:1.4507398860849094 - 13.588579434353079\n",
            "lse step\n",
            "M_5:1.26479631035123 - 11.344525756745988\n",
            "lse step\n",
            "M_6:1.4984073138016778 - 14.527704744649498\n",
            "lse step\n",
            "M_7:1.3895362959041218 - 14.765076054773985\n",
            "lse step\n",
            "M_8:1.4968868075879291 - 11.651725280814034\n",
            "lse step\n",
            "M_9:1.430600626185364 - 12.903967562717861\n",
            "lse step\n",
            "M:7.582837864647913e-05 - 1677.3670917001523\n",
            "lse step\n",
            "It 45 Delta: 1.1554446928130346e-06  Loss: 35.64005926236138\n",
            "M_0:1.169863116487258 - 12.219052369482679\n",
            "lse step\n",
            "M_1:1.194377853980227 - 9.162550789795723\n",
            "lse step\n",
            "M_2:1.215662547305427 - 13.28717708606436\n",
            "lse step\n",
            "M_3:1.3964885748714224 - 11.888409196669693\n",
            "lse step\n",
            "M_4:1.4507396128394872 - 13.588661738879159\n",
            "lse step\n",
            "M_5:1.2647866704773478 - 11.344679753015438\n",
            "lse step\n",
            "M_6:1.4984047400090104 - 14.527574031646285\n",
            "lse step\n",
            "M_7:1.3895303791229774 - 14.76543339987632\n",
            "lse step\n",
            "M_8:1.4968876780049227 - 11.651737938474636\n",
            "lse step\n",
            "M_9:1.4305977762534172 - 12.90396122218606\n",
            "lse step\n",
            "M:7.582785257497095e-05 - 1677.3650311277906\n",
            "lse step\n",
            "It 46 Delta: 9.76657247520052e-07  Loss: 35.64003763397338\n",
            "M_0:1.1698636501522843 - 12.219019594814263\n",
            "lse step\n",
            "M_1:1.1943718053954464 - 9.162573469555689\n",
            "lse step\n",
            "M_2:1.2156592950344238 - 13.287239440414938\n",
            "lse step\n",
            "M_3:1.3964844879123597 - 11.888432631698912\n",
            "lse step\n",
            "M_4:1.4507406215547043 - 13.58872927426797\n",
            "lse step\n",
            "M_5:1.2647795778821886 - 11.34481022167918\n",
            "lse step\n",
            "M_6:1.498404334886728 - 14.527462438576835\n",
            "lse step\n",
            "M_7:1.3895267526479347 - 14.765736229532244\n",
            "lse step\n",
            "M_8:1.4968890440829987 - 11.65174849869879\n",
            "lse step\n",
            "M_9:1.4305963283731031 - 12.903962059980316\n",
            "lse step\n",
            "M:7.582738382903643e-05 - 1677.3633322878575\n",
            "lse step\n",
            "It 47 Delta: 8.284673640446272e-07  Loss: 35.64001565606902\n",
            "M_0:1.169863390597316 - 12.218988923524236\n",
            "lse step\n",
            "M_1:1.194366155050167 - 9.162592714591879\n",
            "lse step\n",
            "M_2:1.2156559096056192 - 13.287291586783459\n",
            "lse step\n",
            "M_3:1.396480631657262 - 11.888453657766306\n",
            "lse step\n",
            "M_4:1.450741390949613 - 13.588780961421767\n",
            "lse step\n",
            "M_5:1.2647730251377136 - 11.344920108576845\n",
            "lse step\n",
            "M_6:1.498403461768738 - 14.527364060124931\n",
            "lse step\n",
            "M_7:1.3895231897348523 - 14.765991790350656\n",
            "lse step\n",
            "M_8:1.4968897825400425 - 11.651754627698931\n",
            "lse step\n",
            "M_9:1.430594791716107 - 12.903963524508734\n",
            "lse step\n",
            "M:7.582694626318519e-05 - 1677.3618893263606\n",
            "lse step\n",
            "It 48 Delta: 7.03359376075241e-07  Loss: 35.640004092499204\n",
            "M_0:1.1698622946310742 - 12.218961046775709\n",
            "lse step\n",
            "M_1:1.194360562948128 - 9.162608903895658\n",
            "lse step\n",
            "M_2:1.2156522230827564 - 13.287334037347902\n",
            "lse step\n",
            "M_3:1.396476596891407 - 11.888472877460076\n",
            "lse step\n",
            "M_4:1.450740840325298 - 13.588820613314091\n",
            "lse step\n",
            "M_5:1.2647662313066692 - 11.345012754257993\n",
            "lse step\n",
            "M_6:1.4984016916097884 - 14.527279355480736\n",
            "lse step\n",
            "M_7:1.3895194710462946 - 14.766206792456964\n",
            "lse step\n",
            "M_8:1.4968902009623313 - 11.651757033329542\n",
            "lse step\n",
            "M_9:1.4305930494976362 - 12.903962825888819\n",
            "lse step\n",
            "M:7.582655533060965e-05 - 1677.3606565630323\n",
            "lse step\n",
            "It 49 Delta: 6.02911029545794e-07  Loss: 35.63998774417086\n",
            "M_0:1.1698620541937728 - 12.218937925985626\n",
            "lse step\n",
            "M_1:1.1943564465593215 - 9.162622700363434\n",
            "lse step\n",
            "M_2:1.2156493022537758 - 13.287373217353908\n",
            "lse step\n",
            "M_3:1.3964738483448833 - 11.888489055915437\n",
            "lse step\n",
            "M_4:1.4507417741202633 - 13.588853592084737\n",
            "lse step\n",
            "M_5:1.264761218664515 - 11.34509291453691\n",
            "lse step\n",
            "M_6:1.4984012843662557 - 14.527206935900784\n",
            "lse step\n",
            "M_7:1.3895170476179466 - 14.766390437065539\n",
            "lse step\n",
            "M_8:1.4968907532909075 - 11.651759841292627\n",
            "lse step\n",
            "M_9:1.4305921876385674 - 12.903967594542618\n",
            "lse step\n",
            "M:7.582619790057554e-05 - 1677.3596114969548\n",
            "lse step\n",
            "It 50 Delta: 5.15874319262366e-07  Loss: 35.639972512469406\n",
            "M_0:1.169862023856818 - 12.218915681839643\n",
            "lse step\n",
            "M_1:1.1943532720878873 - 9.162634242507526\n",
            "lse step\n",
            "M_2:1.2156467005871676 - 13.28740493395707\n",
            "lse step\n",
            "M_3:1.3964714034487211 - 11.888503295788189\n",
            "lse step\n",
            "M_4:1.4507426016365286 - 13.588877678837997\n",
            "lse step\n",
            "M_5:1.2647569803136758 - 11.345160604205867\n",
            "lse step\n",
            "M_6:1.4984013542395198 - 14.527144009648055\n",
            "lse step\n",
            "M_7:1.3895152744086139 - 14.76654573462358\n",
            "lse step\n",
            "M_8:1.4968912730182973 - 11.651759373250602\n",
            "lse step\n",
            "M_9:1.4305915597633931 - 12.90397196069957\n",
            "lse step\n",
            "M:7.582587816538416e-05 - 1677.3586722291936\n",
            "lse step\n",
            "It 51 Delta: 4.4661294396064477e-07  Loss: 35.639944511044206\n",
            "M_0:1.1698628900739236 - 12.218896408301216\n",
            "lse step\n",
            "M_1:1.1943514889055178 - 9.162643814722506\n",
            "lse step\n",
            "M_2:1.215645015284646 - 13.287434967817529\n",
            "lse step\n",
            "M_3:1.3964702629571792 - 11.8885147442805\n",
            "lse step\n",
            "M_4:1.450745235984694 - 13.58889751877508\n",
            "lse step\n",
            "M_5:1.2647544575048504 - 11.34521810771669\n",
            "lse step\n",
            "M_6:1.498402641041483 - 14.527088454890489\n",
            "lse step\n",
            "M_7:1.3895147814119513 - 14.76668033496997\n",
            "lse step\n",
            "M_8:1.4968921429271134 - 11.651759958071214\n",
            "lse step\n",
            "M_9:1.4305918094855687 - 12.903981210045044\n",
            "lse step\n",
            "M:7.582558156120333e-05 - 1677.3578290261694\n",
            "lse step\n",
            "It 52 Delta: 3.85692146664951e-07  Loss: 35.63994576058562\n",
            "M_0:1.1698618622387702 - 12.218878872171464\n",
            "lse step\n",
            "M_1:1.1943482600044277 - 9.16265219004605\n",
            "lse step\n",
            "M_2:1.2156420320183647 - 13.287457958034734\n",
            "lse step\n",
            "M_3:1.396467715627347 - 11.888526829048777\n",
            "lse step\n",
            "M_4:1.4507447358205963 - 13.58891054762766\n",
            "lse step\n",
            "M_5:1.26475016413863 - 11.345266867710142\n",
            "lse step\n",
            "M_6:1.4984015770466694 - 14.5270402866394\n",
            "lse step\n",
            "M_7:1.3895126423573894 - 14.766791442134318\n",
            "lse step\n",
            "M_8:1.496892185668424 - 11.651757302917106\n",
            "lse step\n",
            "M_9:1.4305907488480718 - 12.903985038620537\n",
            "lse step\n",
            "M:7.582531969009612e-05 - 1677.3571890821947\n",
            "lse step\n",
            "It 53 Delta: 3.353416104800999e-07  Loss: 35.63993378831138\n",
            "M_0:1.1698613393180426 - 12.218864849827808\n",
            "lse step\n",
            "M_1:1.1943458304463175 - 9.162658950857546\n",
            "lse step\n",
            "M_2:1.2156396197897843 - 13.287480086797581\n",
            "lse step\n",
            "M_3:1.396465836779329 - 11.888537213813555\n",
            "lse step\n",
            "M_4:1.4507451674742726 - 13.588921963218752\n",
            "lse step\n",
            "M_5:1.2647467984738525 - 11.345308928293903\n",
            "lse step\n",
            "M_6:1.498401202702052 - 14.526998956003302\n",
            "lse step\n",
            "M_7:1.3895112329398793 - 14.766888401682104\n",
            "lse step\n",
            "M_8:1.4968921895642386 - 11.651755824306473\n",
            "lse step\n",
            "M_9:1.4305902596506157 - 12.903992163828146\n",
            "lse step\n",
            "M:7.582506928807118e-05 - 1677.3566598299938\n",
            "lse step\n",
            "It 54 Delta: 2.931494975655369e-07  Loss: 35.639919404053934\n",
            "M_0:1.1698613371431126 - 12.218850136546342\n",
            "lse step\n",
            "M_1:1.1943443312559054 - 9.162665139538039\n",
            "lse step\n",
            "M_2:1.2156376896447854 - 13.287498077006472\n",
            "lse step\n",
            "M_3:1.3964644212560506 - 11.888546305075474\n",
            "lse step\n",
            "M_4:1.4507458311486046 - 13.588927590911908\n",
            "lse step\n",
            "M_5:1.2647443632415736 - 11.34534501889296\n",
            "lse step\n",
            "M_6:1.49840162313816 - 14.526962397429536\n",
            "lse step\n",
            "M_7:1.3895104806699823 - 14.766970126787424\n",
            "lse step\n",
            "M_8:1.4968924890269009 - 11.651752346054353\n",
            "lse step\n",
            "M_9:1.4305901171975386 - 12.903998077872693\n",
            "lse step\n",
            "M:7.582485645418121e-05 - 1677.356087969661\n",
            "lse step\n",
            "It 55 Delta: 2.5770836487026827e-07  Loss: 35.6399209410622\n",
            "M_0:1.1698602372422575 - 12.218837147751621\n",
            "lse step\n",
            "M_1:1.1943419015295196 - 9.162670016384435\n",
            "lse step\n",
            "M_2:1.215635171367861 - 13.287512834453093\n",
            "lse step\n",
            "M_3:1.3964626207000403 - 11.888554805358009\n",
            "lse step\n",
            "M_4:1.4507455652091328 - 13.588931262963388\n",
            "lse step\n",
            "M_5:1.2647410667423964 - 11.345375016985093\n",
            "lse step\n",
            "M_6:1.4984005848711104 - 14.526929537789576\n",
            "lse step\n",
            "M_7:1.3895089682689261 - 14.767039195928087\n",
            "lse step\n",
            "M_8:1.4968923428727634 - 11.65174816840625\n",
            "lse step\n",
            "M_9:1.4305893142882808 - 12.904002233167319\n",
            "lse step\n",
            "M:7.582464741594193e-05 - 1677.3556697841088\n",
            "lse step\n",
            "It 56 Delta: 2.2822038658887323e-07  Loss: 35.63990347720875\n",
            "M_0:1.1698609890424068 - 12.218824979195265\n",
            "lse step\n",
            "M_1:1.1943415665386559 - 9.162674550208727\n",
            "lse step\n",
            "M_2:1.2156341084366118 - 13.287526652174357\n",
            "lse step\n",
            "M_3:1.3964621299616116 - 11.888561618622088\n",
            "lse step\n",
            "M_4:1.4507472629086569 - 13.588933571080442\n",
            "lse step\n",
            "M_5:1.2647399398164332 - 11.345402035319655\n",
            "lse step\n",
            "M_6:1.4984019430301045 - 14.526902188810338\n",
            "lse step\n",
            "M_7:1.3895091459734783 - 14.767100184145969\n",
            "lse step\n",
            "M_8:1.4968928302604019 - 11.651745006616649\n",
            "lse step\n",
            "M_9:1.4305899185767659 - 12.904009175478054\n",
            "lse step\n",
            "M:7.582446546041493e-05 - 1677.3553167993673\n",
            "lse step\n",
            "It 57 Delta: 2.0137817102749977e-07  Loss: 35.639887553593745\n",
            "M_0:1.1698612551729992 - 12.218812272529469\n",
            "lse step\n",
            "M_1:1.1943410000185795 - 9.162678211852231\n",
            "lse step\n",
            "M_2:1.215633146678725 - 13.287537459417873\n",
            "lse step\n",
            "M_3:1.396461878081923 - 11.888567217302725\n",
            "lse step\n",
            "M_4:1.4507489069141961 - 13.58893217038878\n",
            "lse step\n",
            "M_5:1.2647388304028246 - 11.345424184846033\n",
            "lse step\n",
            "M_6:1.4984026656558185 - 14.526875476177228\n",
            "lse step\n",
            "M_7:1.389509166482411 - 14.767152812741205\n",
            "lse step\n",
            "M_8:1.496893478598415 - 11.651740631191018\n",
            "lse step\n",
            "M_9:1.4305903559915254 - 12.904014065706193\n",
            "lse step\n",
            "M:7.58242909661903e-05 - 1677.3548787533416\n",
            "lse step\n",
            "It 58 Delta: 1.792369488384793e-07  Loss: 35.6398873774844\n",
            "M_0:1.1698609868966123 - 12.218803964915649\n",
            "lse step\n",
            "M_1:1.1943399495173082 - 9.162681362914574\n",
            "lse step\n",
            "M_2:1.2156313921636217 - 13.287548328456348\n",
            "lse step\n",
            "M_3:1.3964606362437777 - 11.88857329960299\n",
            "lse step\n",
            "M_4:1.4507489290965894 - 13.588933500759122\n",
            "lse step\n",
            "M_5:1.2647369183530697 - 11.345444163218733\n",
            "lse step\n",
            "M_6:1.4984027416043584 - 14.52685453687453\n",
            "lse step\n",
            "M_7:1.3895086044434588 - 14.767195959976265\n",
            "lse step\n",
            "M_8:1.496893338497966 - 11.65173748959918\n",
            "lse step\n",
            "M_9:1.4305901668466239 - 12.904020639977244\n",
            "lse step\n",
            "M:7.582413966369276e-05 - 1677.3545793694277\n",
            "lse step\n",
            "It 59 Delta: 1.5999520996956562e-07  Loss: 35.63988169586757\n",
            "M_0:1.1698610223621946 - 12.218793548381525\n",
            "lse step\n",
            "M_1:1.1943393768133737 - 9.162683729031656\n",
            "lse step\n",
            "M_2:1.2156303607078611 - 13.287555545826775\n",
            "lse step\n",
            "M_3:1.3964602650353295 - 11.888578012523656\n",
            "lse step\n",
            "M_4:1.4507500590425595 - 13.588929872487364\n",
            "lse step\n",
            "M_5:1.2647357344105945 - 11.345460193567\n",
            "lse step\n",
            "M_6:1.4984030939141073 - 14.526834802044913\n",
            "lse step\n",
            "M_7:1.3895085072702957 - 14.767234056803211\n",
            "lse step\n",
            "M_8:1.4968937037075756 - 11.651733257853508\n",
            "lse step\n",
            "M_9:1.4305904692575881 - 12.904024166554308\n",
            "lse step\n",
            "M:7.58240002661147e-05 - 1677.3544250825032\n",
            "lse step\n",
            "It 60 Delta: 1.4585972962777305e-07  Loss: 35.63988006066039\n",
            "M_0:1.1698595994405976 - 12.218786503418023\n",
            "lse step\n",
            "M_1:1.194337451094261 - 9.162686026818506\n",
            "lse step\n",
            "M_2:1.2156281829436306 - 13.287564119396537\n",
            "lse step\n",
            "M_3:1.3964585834847882 - 11.88858380023093\n",
            "lse step\n",
            "M_4:1.4507490465630979 - 13.588927940117724\n",
            "lse step\n",
            "M_5:1.264733007152778 - 11.34547473405885\n",
            "lse step\n",
            "M_6:1.4984016753099012 - 14.526815978942702\n",
            "lse step\n",
            "M_7:1.389507163658878 - 14.767266653795549\n",
            "lse step\n",
            "M_8:1.4968931677906545 - 11.65172949356436\n",
            "lse step\n",
            "M_9:1.4305895828260815 - 12.90402886928106\n",
            "lse step\n",
            "M:7.582386671494876e-05 - 1677.3541442086616\n",
            "lse step\n",
            "It 61 Delta: 1.2857712761160656e-07  Loss: 35.639867999083634\n",
            "M_0:1.1698602792551196 - 12.218779089616882\n",
            "lse step\n",
            "M_1:1.1943376652535975 - 9.16268837022522\n",
            "lse step\n",
            "M_2:1.2156276164206394 - 13.287571056128996\n",
            "lse step\n",
            "M_3:1.3964584870150707 - 11.888587562113273\n",
            "lse step\n",
            "M_4:1.4507503315297878 - 13.58892615789861\n",
            "lse step\n",
            "M_5:1.264732739903553 - 11.3454875146155\n",
            "lse step\n",
            "M_6:1.4984030012910603 - 14.526801326688267\n",
            "lse step\n",
            "M_7:1.38950762499003 - 14.767295820635848\n",
            "lse step\n",
            "M_8:1.4968934498308974 - 11.651726175636469\n",
            "lse step\n",
            "M_9:1.4305901684419264 - 12.904034329296795\n",
            "lse step\n",
            "M:7.582374777719922e-05 - 1677.3539731453438\n",
            "lse step\n",
            "It 62 Delta: 1.1756673501395198e-07  Loss: 35.639865258962296\n",
            "M_0:1.1698604598318272 - 12.21877380268224\n",
            "lse step\n",
            "M_1:1.1943374825362412 - 9.162689956948595\n",
            "lse step\n",
            "M_2:1.2156267544394344 - 13.287578556096603\n",
            "lse step\n",
            "M_3:1.3964582398045262 - 11.888591132495433\n",
            "lse step\n",
            "M_4:1.4507511849678996 - 13.588925080918333\n",
            "lse step\n",
            "M_5:1.2647321069743507 - 11.345499583214112\n",
            "lse step\n",
            "M_6:1.498403604187252 - 14.526788176879057\n",
            "lse step\n",
            "M_7:1.3895078722831038 - 14.76732091273719\n",
            "lse step\n",
            "M_8:1.4968936814270462 - 11.651724259111107\n",
            "lse step\n",
            "M_9:1.4305903884961262 - 12.904040585863589\n",
            "lse step\n",
            "M:7.582364735359949e-05 - 1677.3538257254104\n",
            "lse step\n",
            "It 63 Delta: 1.065131414179632e-07  Loss: 35.639867106819565\n",
            "M_0:1.1698593173965974 - 12.218767897925613\n",
            "lse step\n",
            "M_1:1.194336095822223 - 9.16269099700677\n",
            "lse step\n",
            "M_2:1.2156250996675046 - 13.287582752189634\n",
            "lse step\n",
            "M_3:1.3964570683302 - 11.88859524707097\n",
            "lse step\n",
            "M_4:1.4507504549756223 - 13.58892098595905\n",
            "lse step\n",
            "M_5:1.2647300441246503 - 11.345508206355241\n",
            "lse step\n",
            "M_6:1.4984023977647842 - 14.526775617061674\n",
            "lse step\n",
            "M_7:1.3895068360596452 - 14.76734089608976\n",
            "lse step\n",
            "M_8:1.496893308209246 - 11.651720581465336\n",
            "lse step\n",
            "M_9:1.4305898188401869 - 12.904043454741009\n",
            "lse step\n",
            "M:7.582355039030893e-05 - 1677.353681765014\n",
            "lse step\n",
            "It 64 Delta: 9.606057460587181e-08  Loss: 35.63986364858064\n",
            "M_0:1.169859229759632 - 12.218763184518096\n",
            "lse step\n",
            "M_1:1.1943356858245262 - 9.162692553169094\n",
            "lse step\n",
            "M_2:1.2156243167706093 - 13.287587975100925\n",
            "lse step\n",
            "M_3:1.396456936892892 - 11.888598110321736\n",
            "lse step\n",
            "M_4:1.4507510760320983 - 13.588917597187706\n",
            "lse step\n",
            "M_5:1.2647293818171343 - 11.345516996075435\n",
            "lse step\n",
            "M_6:1.4984026656908416 - 14.526765561742438\n",
            "lse step\n",
            "M_7:1.3895067455774708 - 14.76735994840021\n",
            "lse step\n",
            "M_8:1.4968934085651466 - 11.651718047338992\n",
            "lse step\n",
            "M_9:1.4305899644630982 - 12.904048329262045\n",
            "lse step\n",
            "M:7.582346413527585e-05 - 1677.353613767803\n",
            "lse step\n",
            "It 65 Delta: 8.613648105892935e-08  Loss: 35.63985523531019\n",
            "M_0:1.1698592528551286 - 12.218757823096547\n",
            "lse step\n",
            "M_1:1.1943354435497846 - 9.162693594604715\n",
            "lse step\n",
            "M_2:1.2156236843277433 - 13.287591873529133\n",
            "lse step\n",
            "M_3:1.396456733699751 - 11.888600856282945\n",
            "lse step\n",
            "M_4:1.4507516605041668 - 13.588914330341607\n",
            "lse step\n",
            "M_5:1.2647287795257531 - 11.345523792477154\n",
            "lse step\n",
            "M_6:1.4984029598456678 - 14.526755148235551\n",
            "lse step\n",
            "M_7:1.38950675599628 - 14.767376609685561\n",
            "lse step\n",
            "M_8:1.4968936103849007 - 11.651714838824752\n",
            "lse step\n",
            "M_9:1.4305901442598177 - 12.904050922443737\n",
            "lse step\n",
            "M:7.582338728866058e-05 - 1677.3534317295776\n",
            "lse step\n",
            "It 66 Delta: 8.140146690038819e-08  Loss: 35.63984849034321\n",
            "M_0:1.1698598741787216 - 12.218755022549196\n",
            "lse step\n",
            "M_1:1.1943358886039142 - 9.162694287400955\n",
            "lse step\n",
            "M_2:1.2156233158624197 - 13.287597515438542\n",
            "lse step\n",
            "M_3:1.3964567549847369 - 11.8886026248806\n",
            "lse step\n",
            "M_4:1.4507527837106822 - 13.588913829439457\n",
            "lse step\n",
            "M_5:1.2647288509283663 - 11.345531392597605\n",
            "lse step\n",
            "M_6:1.4984041460018727 - 14.526747793223706\n",
            "lse step\n",
            "M_7:1.3895073857373792 - 14.767392644163326\n",
            "lse step\n",
            "M_8:1.4968937541635952 - 11.651714476268616\n",
            "lse step\n",
            "M_9:1.4305906001780933 - 12.904058342215729\n",
            "lse step\n",
            "M:7.582331125265704e-05 - 1677.3533108750755\n",
            "lse step\n",
            "It 67 Delta: 7.233256127392451e-08  Loss: 35.63985398509752\n",
            "M_0:1.1698588877551896 - 12.218751698412477\n",
            "lse step\n",
            "M_1:1.194334731632394 - 9.162694885709541\n",
            "lse step\n",
            "M_2:1.2156219553455778 - 13.287600135505073\n",
            "lse step\n",
            "M_3:1.3964556992561186 - 11.88860576504068\n",
            "lse step\n",
            "M_4:1.4507516736157187 - 13.588910616465181\n",
            "lse step\n",
            "M_5:1.2647271738994958 - 11.345535936020173\n",
            "lse step\n",
            "M_6:1.4984030527487326 - 14.526739853444298\n",
            "lse step\n",
            "M_7:1.3895064102276926 - 14.767403880711292\n",
            "lse step\n",
            "M_8:1.496893356018353 - 11.651711147653343\n",
            "lse step\n",
            "M_9:1.4305900527485325 - 12.904059882372955\n",
            "lse step\n",
            "M:7.5823250700475e-05 - 1677.353199527783\n",
            "lse step\n",
            "It 68 Delta: 6.649039363537668e-08  Loss: 35.63984406897232\n",
            "M_0:1.1698598641601303 - 12.218747774034217\n",
            "lse step\n",
            "M_1:1.194335539281885 - 9.16269531584678\n",
            "lse step\n",
            "M_2:1.215622083697035 - 13.287603606055423\n",
            "lse step\n",
            "M_3:1.396456202144397 - 11.888606956741551\n",
            "lse step\n",
            "M_4:1.450753400206418 - 13.58890818585118\n",
            "lse step\n",
            "M_5:1.2647277142786462 - 11.345540732868972\n",
            "lse step\n",
            "M_6:1.4984045126944796 - 14.526733372631671\n",
            "lse step\n",
            "M_7:1.3895073374827787 - 14.76741583277926\n",
            "lse step\n",
            "M_8:1.4968938048486107 - 11.651709550676598\n",
            "lse step\n",
            "M_9:1.4305908782604353 - 12.90406458972547\n",
            "lse step\n",
            "M:7.582319061194915e-05 - 1677.3531548016774\n",
            "lse step\n",
            "It 69 Delta: 6.323624290871521e-08  Loss: 35.639850988152276\n",
            "M_0:1.169858786905324 - 12.21874460334881\n",
            "lse step\n",
            "M_1:1.1943343081036502 - 9.162695876207996\n",
            "lse step\n",
            "M_2:1.215620872348339 - 13.287606127716419\n",
            "lse step\n",
            "M_3:1.396455355398767 - 11.888609568522128\n",
            "lse step\n",
            "M_4:1.450752561348538 - 13.588904569599578\n",
            "lse step\n",
            "M_5:1.2647261937031709 - 11.345544829403368\n",
            "lse step\n",
            "M_6:1.4984033132441272 - 14.526726536539776\n",
            "lse step\n",
            "M_7:1.3895064369410188 - 14.767425896608685\n",
            "lse step\n",
            "M_8:1.496893522651823 - 11.65170724606422\n",
            "lse step\n",
            "M_9:1.4305902404472173 - 12.90406512164682\n",
            "lse step\n",
            "M:7.582313101152206e-05 - 1677.3530685799217\n",
            "lse step\n",
            "It 70 Delta: 5.652204748685108e-08  Loss: 35.63983737801621\n",
            "M_0:1.1698593859824722 - 12.218741382177814\n",
            "lse step\n",
            "M_1:1.1943348802186082 - 9.162696658981883\n",
            "lse step\n",
            "M_2:1.2156208262355468 - 13.287608167493564\n",
            "lse step\n",
            "M_3:1.3964556043356378 - 11.88861054695036\n",
            "lse step\n",
            "M_4:1.4507536320871868 - 13.588902295179476\n",
            "lse step\n",
            "M_5:1.2647265645817134 - 11.34554975449631\n",
            "lse step\n",
            "M_6:1.4984044888364894 - 14.526721631857498\n",
            "lse step\n",
            "M_7:1.3895070465784056 - 14.76743442156473\n",
            "lse step\n",
            "M_8:1.496893655512321 - 11.651705498478794\n",
            "lse step\n",
            "M_9:1.4305907611621267 - 12.90406940573103\n",
            "lse step\n",
            "M:7.582308292184431e-05 - 1677.353050978424\n",
            "lse step\n",
            "It 71 Delta: 4.9902070031748735e-08  Loss: 35.63983561296736\n",
            "M_0:1.16985926587001 - 12.218738329586188\n",
            "lse step\n",
            "M_1:1.1943347198962293 - 9.16269690712399\n",
            "lse step\n",
            "M_2:1.2156202613268399 - 13.287610515154249\n",
            "lse step\n",
            "M_3:1.3964550909069877 - 11.888612212629917\n",
            "lse step\n",
            "M_4:1.4507536527488427 - 13.588899543745304\n",
            "lse step\n",
            "M_5:1.2647259200838379 - 11.345552444678772\n",
            "lse step\n",
            "M_6:1.498404505027915 - 14.526715521585444\n",
            "lse step\n",
            "M_7:1.3895070378784493 - 14.767442094009262\n",
            "lse step\n",
            "M_8:1.4968935839349589 - 11.651702967263551\n",
            "lse step\n",
            "M_9:1.4305907256506196 - 12.904070990554779\n",
            "lse step\n",
            "M:7.582303947304852e-05 - 1677.3529394841166\n",
            "lse step\n",
            "It 72 Delta: 4.8882160541552366e-08  Loss: 35.63983239349571\n",
            "M_0:1.1698594063223648 - 12.218736402913919\n",
            "lse step\n",
            "M_1:1.1943347739569137 - 9.162697268890444\n",
            "lse step\n",
            "M_2:1.2156201811876404 - 13.28761278990223\n",
            "lse step\n",
            "M_3:1.3964555770763278 - 11.888613257731874\n",
            "lse step\n",
            "M_4:1.4507545875889352 - 13.588896876212795\n",
            "lse step\n",
            "M_5:1.2647260302543355 - 11.345555377635996\n",
            "lse step\n",
            "M_6:1.4984047984894109 - 14.526712140829313\n",
            "lse step\n",
            "M_7:1.3895072025056576 - 14.767448824782411\n",
            "lse step\n",
            "M_8:1.496893919502163 - 11.651702632000262\n",
            "lse step\n",
            "M_9:1.4305911412623704 - 12.90407504129036\n",
            "lse step\n",
            "M:7.582299422977501e-05 - 1677.3528778536593\n",
            "lse step\n",
            "It 73 Delta: 4.325496050228139e-08  Loss: 35.63984019779572\n",
            "M_0:1.1698589564897843 - 12.218734207327945\n",
            "lse step\n",
            "M_1:1.1943342275074416 - 9.162697602626597\n",
            "lse step\n",
            "M_2:1.215619495394695 - 13.287613701955502\n",
            "lse step\n",
            "M_3:1.3964552266408372 - 11.888614918068987\n",
            "lse step\n",
            "M_4:1.4507541991834763 - 13.588894179900215\n",
            "lse step\n",
            "M_5:1.2647252477109525 - 11.345557558978916\n",
            "lse step\n",
            "M_6:1.498404279091611 - 14.526708237168473\n",
            "lse step\n",
            "M_7:1.3895067592897614 - 14.767453194523409\n",
            "lse step\n",
            "M_8:1.4968938878232882 - 11.651700308329575\n",
            "lse step\n",
            "M_9:1.430590926834773 - 12.904074756399902\n",
            "lse step\n",
            "M:7.58229597078757e-05 - 1677.3529396877334\n",
            "lse step\n",
            "It 74 Delta: 4.111271323381516e-08  Loss: 35.63982615442096\n",
            "M_0:1.169859183271345 - 12.218729409081671\n",
            "lse step\n",
            "M_1:1.1943344670177207 - 9.162697939242245\n",
            "lse step\n",
            "M_2:1.2156194348485378 - 13.287614228796816\n",
            "lse step\n",
            "M_3:1.3964552160698724 - 11.888615452980881\n",
            "lse step\n",
            "M_4:1.4507550375387952 - 13.58889012411495\n",
            "lse step\n",
            "M_5:1.264725173914627 - 11.345559329136227\n",
            "lse step\n",
            "M_6:1.4984047416244866 - 14.526701806794179\n",
            "lse step\n",
            "M_7:1.3895071075760699 - 14.767460480719873\n",
            "lse step\n",
            "M_8:1.4968939029731072 - 11.651697121934678\n",
            "lse step\n",
            "M_9:1.4305912257637343 - 12.904075527119545\n",
            "lse step\n",
            "M:7.582292283544979e-05 - 1677.3528324054416\n",
            "lse step\n",
            "It 75 Delta: 3.885072885623231e-08  Loss: 35.63983291984597\n",
            "M_0:1.1698585623766318 - 12.218730211597885\n",
            "lse step\n",
            "M_1:1.1943337867915798 - 9.162698058411562\n",
            "lse step\n",
            "M_2:1.2156186148571206 - 13.287616475092921\n",
            "lse step\n",
            "M_3:1.396454801561074 - 11.888617128434914\n",
            "lse step\n",
            "M_4:1.450754325039552 - 13.588890249385353\n",
            "lse step\n",
            "M_5:1.264724345979857 - 11.345562244513083\n",
            "lse step\n",
            "M_6:1.4984040733138455 - 14.526700739856103\n",
            "lse step\n",
            "M_7:1.3895066146364827 - 14.767464047132968\n",
            "lse step\n",
            "M_8:1.4968937305878907 - 11.651697328374972\n",
            "lse step\n",
            "M_9:1.4305908278198276 - 12.904078513657055\n",
            "lse step\n",
            "M:7.582288609133981e-05 - 1677.3527798549026\n",
            "lse step\n",
            "It 76 Delta: 3.397686043626891e-08  Loss: 35.639832490933046\n",
            "M_0:1.1698585727302588 - 12.218727011330015\n",
            "lse step\n",
            "M_1:1.194333768419444 - 9.162698629690176\n",
            "lse step\n",
            "M_2:1.2156182606319215 - 13.287615915076108\n",
            "lse step\n",
            "M_3:1.3964544311941403 - 11.888618216755235\n",
            "lse step\n",
            "M_4:1.4507540987667413 - 13.5888877539207\n",
            "lse step\n",
            "M_5:1.264724075570824 - 11.345564110281778\n",
            "lse step\n",
            "M_6:1.4984043208976332 - 14.52669689117641\n",
            "lse step\n",
            "M_7:1.3895066159678948 - 14.7674671391901\n",
            "lse step\n",
            "M_8:1.4968937195287961 - 11.65169461090633\n",
            "lse step\n",
            "M_9:1.430590775554847 - 12.904077454610865\n",
            "lse step\n",
            "M:7.582285988276217e-05 - 1677.3527931173774\n",
            "lse step\n",
            "It 77 Delta: 3.407387794140959e-08  Loss: 35.63983394944765\n",
            "M_0:1.1698584662581497 - 12.218726071977676\n",
            "lse step\n",
            "M_1:1.194333610517899 - 9.162698861437134\n",
            "lse step\n",
            "M_2:1.2156178257676025 - 13.287619415974554\n",
            "lse step\n",
            "M_3:1.3964541890426643 - 11.888619367138466\n",
            "lse step\n",
            "M_4:1.4507541864547169 - 13.58888663595506\n",
            "lse step\n",
            "M_5:1.264723689638851 - 11.345566529591053\n",
            "lse step\n",
            "M_6:1.4984043033733179 - 14.526693012949215\n",
            "lse step\n",
            "M_7:1.3895066006289178 - 14.767472765136604\n",
            "lse step\n",
            "M_8:1.4968936538576538 - 11.651694406185806\n",
            "lse step\n",
            "M_9:1.4305907133470286 - 12.904080960660457\n",
            "lse step\n",
            "M:7.582282564368539e-05 - 1677.3526999851072\n",
            "lse step\n",
            "It 78 Delta: 3.102177714708887e-08  Loss: 35.63982684180814\n",
            "M_0:1.1698591063283166 - 12.218725168606916\n",
            "lse step\n",
            "M_1:1.1943342211864298 - 9.162699381099257\n",
            "lse step\n",
            "M_2:1.2156179960600255 - 13.28762138809919\n",
            "lse step\n",
            "M_3:1.3964544590113421 - 11.888619713701761\n",
            "lse step\n",
            "M_4:1.4507550573002348 - 13.588885982112732\n",
            "lse step\n",
            "M_5:1.2647242116341402 - 11.345568580239155\n",
            "lse step\n",
            "M_6:1.4984052546546915 - 14.526690771546189\n",
            "lse step\n",
            "M_7:1.3895071827983754 - 14.767476637812358\n",
            "lse step\n",
            "M_8:1.496893770884806 - 11.651693991128019\n",
            "lse step\n",
            "M_9:1.4305911972667147 - 12.904084317602566\n",
            "lse step\n",
            "M:7.58228078436385e-05 - 1677.352737123059\n",
            "lse step\n",
            "It 79 Delta: 2.7001458846598325e-08  Loss: 35.639822534692755\n",
            "M_0:1.1698588108440298 - 12.218721629746696\n",
            "lse step\n",
            "M_1:1.194333967368658 - 9.162699126979762\n",
            "lse step\n",
            "M_2:1.2156178229067953 - 13.287620019991767\n",
            "lse step\n",
            "M_3:1.3964542434619516 - 11.88862025124905\n",
            "lse step\n",
            "M_4:1.450755156956276 - 13.58888166918276\n",
            "lse step\n",
            "M_5:1.2647237895123582 - 11.345568946074224\n",
            "lse step\n",
            "M_6:1.4984050196329242 - 14.52668745086499\n",
            "lse step\n",
            "M_7:1.3895070321314298 - 14.767480646386003\n",
            "lse step\n",
            "M_8:1.496893725642699 - 11.65169182109947\n",
            "lse step\n",
            "M_9:1.430591206003092 - 12.904082802592473\n",
            "lse step\n",
            "M:7.582278840311572e-05 - 1677.3526532539288\n",
            "lse step\n",
            "It 80 Delta: 2.451780112266988e-08  Loss: 35.639821188319075\n",
            "M_0:1.1698591687486464 - 12.218722255444662\n",
            "lse step\n",
            "M_1:1.1943343033609477 - 9.16269941425006\n",
            "lse step\n",
            "M_2:1.215617752514052 - 13.287621612223845\n",
            "lse step\n",
            "M_3:1.396454429667459 - 11.88862082042811\n",
            "lse step\n",
            "M_4:1.4507554313400055 - 13.588882155724345\n",
            "lse step\n",
            "M_5:1.2647241215014642 - 11.345570444598176\n",
            "lse step\n",
            "M_6:1.4984055083924548 - 14.526687099391838\n",
            "lse step\n",
            "M_7:1.3895073510594562 - 14.767481609785156\n",
            "lse step\n",
            "M_8:1.4968938598206625 - 11.65169214597904\n",
            "lse step\n",
            "M_9:1.4305914471103207 - 12.904086625345649\n",
            "lse step\n",
            "M:7.582276357675393e-05 - 1677.3526239254184\n",
            "lse step\n",
            "It 81 Delta: 2.408668109410428e-08  Loss: 35.63982221901553\n",
            "M_0:1.1698592106784378 - 12.218721259930074\n",
            "lse step\n",
            "M_1:1.1943343212712971 - 9.162699248991434\n",
            "lse step\n",
            "M_2:1.215617689447287 - 13.28762249828637\n",
            "lse step\n",
            "M_3:1.3964547115051889 - 11.888621061676988\n",
            "lse step\n",
            "M_4:1.450755941628862 - 13.588880902733765\n",
            "lse step\n",
            "M_5:1.2647241387201107 - 11.345571193299197\n",
            "lse step\n",
            "M_6:1.498405623040131 - 14.526685616507235\n",
            "lse step\n",
            "M_7:1.3895074603171726 - 14.767483289662277\n",
            "lse step\n",
            "M_8:1.4968940536614779 - 11.651691653822331\n",
            "lse step\n",
            "M_9:1.4305916373707852 - 12.904088143792777\n",
            "lse step\n",
            "M:7.582274441127101e-05 - 1677.3526281980962\n",
            "lse step\n",
            "It 82 Delta: 2.1425003993158498e-08  Loss: 35.63982382930321\n",
            "M_0:1.1698590437191283 - 12.21872095584099\n",
            "lse step\n",
            "M_1:1.1943341590111043 - 9.162699425943172\n",
            "lse step\n",
            "M_2:1.2156173187799826 - 13.287623576438701\n",
            "lse step\n",
            "M_3:1.3964542838437932 - 11.88862156477861\n",
            "lse step\n",
            "M_4:1.450755595663217 - 13.588880351927608\n",
            "lse step\n",
            "M_5:1.264723790581384 - 11.345571898482628\n",
            "lse step\n",
            "M_6:1.4984056317067425 - 14.526683722817882\n",
            "lse step\n",
            "M_7:1.3895073209384388 - 14.767485190728605\n",
            "lse step\n",
            "M_8:1.4968937913190816 - 11.651690360079717\n",
            "lse step\n",
            "M_9:1.4305914290770143 - 12.904088545499423\n",
            "lse step\n",
            "M:7.582272755949711e-05 - 1677.3525458453282\n",
            "lse step\n",
            "It 83 Delta: 1.9722641297903465e-08  Loss: 35.63982677533419\n",
            "M_0:1.1698588615424457 - 12.218719870890244\n",
            "lse step\n",
            "M_1:1.1943339191219307 - 9.162699531994626\n",
            "lse step\n",
            "M_2:1.215617043762087 - 13.287623983091793\n",
            "lse step\n",
            "M_3:1.396454089145052 - 11.888622500735202\n",
            "lse step\n",
            "M_4:1.450755475003541 - 13.588878681310339\n",
            "lse step\n",
            "M_5:1.2647233475807198 - 11.345572457486801\n",
            "lse step\n",
            "M_6:1.4984053699085282 - 14.526682737674776\n",
            "lse step\n",
            "M_7:1.3895071758077002 - 14.767487445638377\n",
            "lse step\n",
            "M_8:1.4968937398712414 - 11.65168983226092\n",
            "lse step\n",
            "M_9:1.4305913674742576 - 12.904088686876188\n",
            "lse step\n",
            "M:7.582271011911613e-05 - 1677.3525507038223\n",
            "lse step\n",
            "It 84 Delta: 2.1369093161638375e-08  Loss: 35.63983259287235\n",
            "M_0:1.1698581036126927 - 12.218719724951466\n",
            "lse step\n",
            "M_1:1.194333122713911 - 9.162699459151344\n",
            "lse step\n",
            "M_2:1.2156163584538282 - 13.287624021881335\n",
            "lse step\n",
            "M_3:1.3964535634954212 - 11.888623483756113\n",
            "lse step\n",
            "M_4:1.450754558400397 - 13.588877775934943\n",
            "lse step\n",
            "M_5:1.2647225208471897 - 11.345573130469962\n",
            "lse step\n",
            "M_6:1.4984044361875228 - 14.526681097660108\n",
            "lse step\n",
            "M_7:1.3895064586301675 - 14.767487662548412\n",
            "lse step\n",
            "M_8:1.4968934734107897 - 11.65168916625359\n",
            "lse step\n",
            "M_9:1.4305908693608405 - 12.904088507318054\n",
            "lse step\n",
            "M:7.582269770966007e-05 - 1677.3526152730678\n",
            "lse step\n",
            "It 85 Delta: 2.145988453605696e-08  Loss: 35.639819850113234\n",
            "M_0:1.1698589472999028 - 12.21871780101748\n",
            "lse step\n",
            "M_1:1.1943340225286119 - 9.16269938766366\n",
            "lse step\n",
            "M_2:1.2156168700590975 - 13.287623816713548\n",
            "lse step\n",
            "M_3:1.3964540821911484 - 11.888623190265447\n",
            "lse step\n",
            "M_4:1.4507558011566024 - 13.588876256291972\n",
            "lse step\n",
            "M_5:1.2647233283099832 - 11.345573970240284\n",
            "lse step\n",
            "M_6:1.498405692291369 - 14.52668014125911\n",
            "lse step\n",
            "M_7:1.3895073024522335 - 14.767489783487962\n",
            "lse step\n",
            "M_8:1.4968938206743205 - 11.651688217238572\n",
            "lse step\n",
            "M_9:1.4305914830499686 - 12.904089367081287\n",
            "lse step\n",
            "M:7.58226893215059e-05 - 1677.3526341991512\n",
            "lse step\n",
            "It 86 Delta: 2.0587462401522316e-08  Loss: 35.639826991795424\n",
            "M_0:1.1698579840744623 - 12.218718193388025\n",
            "lse step\n",
            "M_1:1.194332963658633 - 9.162699860121608\n",
            "lse step\n",
            "M_2:1.2156160829988292 - 13.28762505906227\n",
            "lse step\n",
            "M_3:1.396453471407843 - 11.888624544780933\n",
            "lse step\n",
            "M_4:1.4507547044451656 - 13.588875909334586\n",
            "lse step\n",
            "M_5:1.2647221889812257 - 11.345574743535693\n",
            "lse step\n",
            "M_6:1.4984043859966496 - 14.526678555162341\n",
            "lse step\n",
            "M_7:1.3895064379572986 - 14.767491106781316\n",
            "lse step\n",
            "M_8:1.4968934941338021 - 11.651687801369196\n",
            "lse step\n",
            "M_9:1.4305908813810468 - 12.904089121627184\n",
            "lse step\n",
            "M:7.582267899378183e-05 - 1677.3525487598356\n",
            "lse step\n",
            "It 87 Delta: 1.6760624887979247e-08  Loss: 35.6398241589653\n",
            "M_0:1.1698581756359576 - 12.218717044205663\n",
            "lse step\n",
            "M_1:1.194333146869035 - 9.16269994397107\n",
            "lse step\n",
            "M_2:1.2156162215149762 - 13.287624795953747\n",
            "lse step\n",
            "M_3:1.3964538463291203 - 11.888624584797983\n",
            "lse step\n",
            "M_4:1.4507552415284484 - 13.588874999907357\n",
            "lse step\n",
            "M_5:1.2647224886594346 - 11.34557521905128\n",
            "lse step\n",
            "M_6:1.498404663661621 - 14.526677247986107\n",
            "lse step\n",
            "M_7:1.3895066014014272 - 14.76749217195113\n",
            "lse step\n",
            "M_8:1.496893686347899 - 11.65168761525877\n",
            "lse step\n",
            "M_9:1.4305911461872134 - 12.90409027057201\n",
            "lse step\n",
            "M:7.582266545727618e-05 - 1677.3525379896837\n",
            "lse step\n",
            "It 88 Delta: 1.6363030042043647e-08  Loss: 35.63982627570613\n",
            "M_0:1.1698581260667804 - 12.21871698026306\n",
            "lse step\n",
            "M_1:1.1943331431152875 - 9.16269980327132\n",
            "lse step\n",
            "M_2:1.2156160111426981 - 13.287625189419595\n",
            "lse step\n",
            "M_3:1.396453538325484 - 11.888625054932108\n",
            "lse step\n",
            "M_4:1.4507549308016454 - 13.588874811946758\n",
            "lse step\n",
            "M_5:1.2647222353817837 - 11.345575751333211\n",
            "lse step\n",
            "M_6:1.4984046972835592 - 14.526677415255103\n",
            "lse step\n",
            "M_7:1.3895065927269803 - 14.767493203434759\n",
            "lse step\n",
            "M_8:1.4968936340383037 - 11.651686887092758\n",
            "lse step\n",
            "M_9:1.4305910664702146 - 12.904090437200749\n",
            "lse step\n",
            "M:7.582265183201484e-05 - 1677.3524767164593\n",
            "lse step\n",
            "It 89 Delta: 1.5505149164596332e-08  Loss: 35.639825882857465\n",
            "M_0:1.1698582302600327 - 12.218715956518654\n",
            "lse step\n",
            "M_1:1.1943332419908423 - 9.162700131847505\n",
            "lse step\n",
            "M_2:1.2156159066351695 - 13.287626344595193\n",
            "lse step\n",
            "M_3:1.3964535575498507 - 11.888625130453068\n",
            "lse step\n",
            "M_4:1.4507551236337228 - 13.588873685083966\n",
            "lse step\n",
            "M_5:1.2647223563598846 - 11.34557654596465\n",
            "lse step\n",
            "M_6:1.4984049636626149 - 14.526675297584728\n",
            "lse step\n",
            "M_7:1.3895067099425373 - 14.767494825809743\n",
            "lse step\n",
            "M_8:1.4968936261579 - 11.65168676538956\n",
            "lse step\n",
            "M_9:1.4305910476279737 - 12.904091596229776\n",
            "lse step\n",
            "M:7.582264327637246e-05 - 1677.3525257455517\n",
            "lse step\n",
            "It 90 Delta: 1.8174619142996562e-08  Loss: 35.63982074697176\n",
            "M_0:1.169858201114668 - 12.218715747725039\n",
            "lse step\n",
            "M_1:1.1943331377149529 - 9.16270022690356\n",
            "lse step\n",
            "M_2:1.2156159122258186 - 13.287626284865823\n",
            "lse step\n",
            "M_3:1.396453728884559 - 11.888625564600016\n",
            "lse step\n",
            "M_4:1.4507554067966955 - 13.588873175248569\n",
            "lse step\n",
            "M_5:1.2647222411504906 - 11.34557745345742\n",
            "lse step\n",
            "M_6:1.4984048789688216 - 14.52667501148325\n",
            "lse step\n",
            "M_7:1.3895066520684811 - 14.76749598987991\n",
            "lse step\n",
            "M_8:1.4968937150321262 - 11.651686469915123\n",
            "lse step\n",
            "M_9:1.430591237874358 - 12.904091835484614\n",
            "lse step\n",
            "M:7.582262457545077e-05 - 1677.352464773123\n",
            "lse step\n",
            "It 91 Delta: 2.1480694556430535e-08  Loss: 35.6398210280047\n",
            "M_0:1.1698587336664947 - 12.218715108919918\n",
            "lse step\n",
            "M_1:1.194333701779559 - 9.162699865873137\n",
            "lse step\n",
            "M_2:1.215616079428671 - 13.287627005329625\n",
            "lse step\n",
            "M_3:1.3964536926809283 - 11.888625266526883\n",
            "lse step\n",
            "M_4:1.4507557876883155 - 13.588872684224501\n",
            "lse step\n",
            "M_5:1.2647225895741134 - 11.345577996738637\n",
            "lse step\n",
            "M_6:1.4984056156989638 - 14.526674732037725\n",
            "lse step\n",
            "M_7:1.3895071051660954 - 14.767497581219308\n",
            "lse step\n",
            "M_8:1.4968937212364353 - 11.651686054258175\n",
            "lse step\n",
            "M_9:1.4305915141223569 - 12.904093296228124\n",
            "lse step\n",
            "M:7.582262399105885e-05 - 1677.3525366211124\n",
            "lse step\n",
            "It 92 Delta: 1.9670226336643282e-08  Loss: 35.63982039010609\n",
            "M_0:1.1698581820639966 - 12.218715351218808\n",
            "lse step\n",
            "M_1:1.1943331164941522 - 9.162700125960933\n",
            "lse step\n",
            "M_2:1.215615794962864 - 13.287627361520732\n",
            "lse step\n",
            "M_3:1.3964536784913 - 11.888625990860424\n",
            "lse step\n",
            "M_4:1.45075547132044 - 13.58887248111109\n",
            "lse step\n",
            "M_5:1.2647220890633837 - 11.345577896542427\n",
            "lse step\n",
            "M_6:1.4984048046787977 - 14.526674225164495\n",
            "lse step\n",
            "M_7:1.3895066757720687 - 14.767497942762242\n",
            "lse step\n",
            "M_8:1.496893722679825 - 11.65168587811732\n",
            "lse step\n",
            "M_9:1.4305912864166355 - 12.904092958410166\n",
            "lse step\n",
            "M:7.582262282393765e-05 - 1677.3525529687583\n",
            "lse step\n",
            "It 93 Delta: 2.0131846412141385e-08  Loss: 35.63981351337289\n",
            "M_0:1.169858858074081 - 12.218712786605964\n",
            "lse step\n",
            "M_1:1.194333891845982 - 9.162700205085072\n",
            "lse step\n",
            "M_2:1.2156161687565386 - 13.28762617857688\n",
            "lse step\n",
            "M_3:1.3964538231785204 - 11.888625483673124\n",
            "lse step\n",
            "M_4:1.4507561245276186 - 13.58887037088783\n",
            "lse step\n",
            "M_5:1.2647227689455565 - 11.34557788362864\n",
            "lse step\n",
            "M_6:1.498405867164633 - 14.526672272606074\n",
            "lse step\n",
            "M_7:1.3895073415703458 - 14.767499057214387\n",
            "lse step\n",
            "M_8:1.4968938569454626 - 11.651684561221957\n",
            "lse step\n",
            "M_9:1.4305916153210254 - 12.904092846947249\n",
            "lse step\n",
            "M:7.582262070308166e-05 - 1677.3524726085393\n",
            "lse step\n",
            "It 94 Delta: 1.9184531296900786e-08  Loss: 35.63982238110662\n",
            "M_0:1.1698579834679483 - 12.218713977505836\n",
            "lse step\n",
            "M_1:1.1943329457665313 - 9.162700438677755\n",
            "lse step\n",
            "M_2:1.2156154469177958 - 13.287626853436777\n",
            "lse step\n",
            "M_3:1.3964532271208552 - 11.888626551657147\n",
            "lse step\n",
            "M_4:1.4507549812266558 - 13.588870666267868\n",
            "lse step\n",
            "M_5:1.264721779882747 - 11.345578363040765\n",
            "lse step\n",
            "M_6:1.4984047360430601 - 14.526672373830568\n",
            "lse step\n",
            "M_7:1.3895065322972522 - 14.767498712408672\n",
            "lse step\n",
            "M_8:1.4968935179269833 - 11.651684872464847\n",
            "lse step\n",
            "M_9:1.4305910196974019 - 12.904093313127875\n",
            "lse step\n",
            "M:7.582261666005513e-05 - 1677.3524584230809\n",
            "lse step\n",
            "It 95 Delta: 1.6576947814428422e-08  Loss: 35.639818084129494\n",
            "M_0:1.169858773461939 - 12.218713529120057\n",
            "lse step\n",
            "M_1:1.1943337493352357 - 9.162700620641882\n",
            "lse step\n",
            "M_2:1.2156159518266232 - 13.287628491883552\n",
            "lse step\n",
            "M_3:1.3964537284185046 - 11.888626136045627\n",
            "lse step\n",
            "M_4:1.4507559936420433 - 13.588870886874739\n",
            "lse step\n",
            "M_5:1.2647225946054448 - 11.345579312022862\n",
            "lse step\n",
            "M_6:1.4984058257309294 - 14.52667182520336\n",
            "lse step\n",
            "M_7:1.3895072963767334 - 14.767501387370862\n",
            "lse step\n",
            "M_8:1.496893768523566 - 11.651685553484784\n",
            "lse step\n",
            "M_9:1.4305915603640866 - 12.904095591298423\n",
            "lse step\n",
            "M:7.582260825008813e-05 - 1677.3524333718194\n",
            "lse step\n",
            "It 96 Delta: 1.0130050576151461e-08  Loss: 35.639820365937496\n",
            "M_0:1.1698586344458692 - 12.218713016493394\n",
            "lse step\n",
            "M_1:1.194333650980631 - 9.162700397846821\n",
            "lse step\n",
            "M_2:1.2156158083254711 - 13.28762778810237\n",
            "lse step\n",
            "M_3:1.3964536010631419 - 11.888626311337934\n",
            "lse step\n",
            "M_4:1.4507559597069322 - 13.588870205003948\n",
            "lse step\n",
            "M_5:1.2647224179375303 - 11.345579141038945\n",
            "lse step\n",
            "M_6:1.4984057026605986 - 14.526671590193772\n",
            "lse step\n",
            "M_7:1.3895071987853882 - 14.767500979034441\n",
            "lse step\n",
            "M_8:1.4968936777730022 - 11.651684966049512\n",
            "lse step\n",
            "M_9:1.4305915159036606 - 12.904095360012288\n",
            "lse step\n",
            "M:7.582260041576659e-05 - 1677.352487598026\n",
            "lse step\n",
            "It 97 Delta: 1.1551185785663165e-08  Loss: 35.63981547964406\n",
            "M_0:1.169858728251472 - 12.218713062000004\n",
            "lse step\n",
            "M_1:1.1943337439297002 - 9.16269996011227\n",
            "lse step\n",
            "M_2:1.2156158378100639 - 13.287627741553024\n",
            "lse step\n",
            "M_3:1.3964536254532944 - 11.88862642164088\n",
            "lse step\n",
            "M_4:1.4507560849707888 - 13.588869987295391\n",
            "lse step\n",
            "M_5:1.264722472057192 - 11.345579472069499\n",
            "lse step\n",
            "M_6:1.498405849380703 - 14.526671765292198\n",
            "lse step\n",
            "M_7:1.3895072584037935 - 14.767501690463025\n",
            "lse step\n",
            "M_8:1.4968937102811777 - 11.651684727799712\n",
            "lse step\n",
            "M_9:1.4305916051475207 - 12.904095307925061\n",
            "lse step\n",
            "M:7.582259660825509e-05 - 1677.3524539237303\n",
            "lse step\n",
            "It 98 Delta: 1.5369305828016877e-08  Loss: 35.639812030932355\n",
            "M_0:1.169859119899748 - 12.218712582308275\n",
            "lse step\n",
            "M_1:1.1943341103523248 - 9.162699933864081\n",
            "lse step\n",
            "M_2:1.2156161594994763 - 13.287628826501003\n",
            "lse step\n",
            "M_3:1.396454182319277 - 11.888625807046353\n",
            "lse step\n",
            "M_4:1.4507569225650672 - 13.588869347647965\n",
            "lse step\n",
            "M_5:1.2647229771145736 - 11.34557906642593\n",
            "lse step\n",
            "M_6:1.49840626148983 - 14.526670457899089\n",
            "lse step\n",
            "M_7:1.3895076647729079 - 14.767502419651965\n",
            "lse step\n",
            "M_8:1.4968940565220965 - 11.651684591647069\n",
            "lse step\n",
            "M_9:1.4305919618465048 - 12.904096866298488\n",
            "lse step\n",
            "M:7.582259029424612e-05 - 1677.3524276944854\n",
            "lse step\n",
            "It 99 Delta: 1.309250841075027e-08  Loss: 35.63981541697286\n",
            "M_0:1.1698588704318764 - 12.218712594461131\n",
            "lse step\n",
            "M_1:1.1943338555375043 - 9.16270022654403\n",
            "lse step\n",
            "M_2:1.215615835978626 - 13.287629042654427\n",
            "lse step\n",
            "M_3:1.396453743533799 - 11.888626504023055\n",
            "lse step\n",
            "M_4:1.4507562405079248 - 13.588869510311467\n",
            "lse step\n",
            "M_5:1.264722588415998 - 11.345578890149467\n",
            "lse step\n",
            "M_6:1.4984059653726902 - 14.526670585222151\n",
            "lse step\n",
            "M_7:1.3895073754365546 - 14.767502314663728\n",
            "lse step\n",
            "M_8:1.4968938584032283 - 11.651684592017894\n",
            "lse step\n",
            "M_9:1.430591720283274 - 12.904096819479483\n",
            "lse step\n",
            "M:7.582258770516683e-05 - 1677.3524964181308\n",
            "lse step\n",
            "It 100 Delta: 1.9271576334745077e-08  Loss: 35.639820895027526\n",
            "M_0:1.169858090786421 - 12.218712386613062\n",
            "lse step\n",
            "M_1:1.194333101904407 - 9.16269988970753\n",
            "lse step\n",
            "M_2:1.2156152266492029 - 13.287628389403688\n",
            "lse step\n",
            "M_3:1.3964532011452098 - 11.888626883938434\n",
            "lse step\n",
            "M_4:1.4507554251470252 - 13.58886924473715\n",
            "lse step\n",
            "M_5:1.2647216407301718 - 11.345578821450687\n",
            "lse step\n",
            "M_6:1.4984049541158933 - 14.526669097329366\n",
            "lse step\n",
            "M_7:1.3895067937088381 - 14.767502037444881\n",
            "lse step\n",
            "M_8:1.4968936097660472 - 11.651683595661641\n",
            "lse step\n",
            "M_9:1.4305911273976202 - 12.904095023797563\n",
            "lse step\n",
            "M:7.582258458856063e-05 - 1677.3524909231871\n",
            "lse step\n",
            "It 101 Delta: 1.790809278645611e-08  Loss: 35.6398160579622\n",
            "M_0:1.1698587659214565 - 12.218710919632585\n",
            "lse step\n",
            "M_1:1.1943337828482876 - 9.16269993426401\n",
            "lse step\n",
            "M_2:1.2156158045607175 - 13.28762813393563\n",
            "lse step\n",
            "M_3:1.3964536295834376 - 11.88862653558777\n",
            "lse step\n",
            "M_4:1.4507563946445865 - 13.588867210133479\n",
            "lse step\n",
            "M_5:1.264722417137242 - 11.345579106463513\n",
            "lse step\n",
            "M_6:1.498405939759242 - 14.526668573359858\n",
            "lse step\n",
            "M_7:1.3895073839524554 - 14.767504033368754\n",
            "lse step\n",
            "M_8:1.4968938658780184 - 11.651682988983065\n",
            "lse step\n",
            "M_9:1.4305916326889456 - 12.904095319946215\n",
            "lse step\n",
            "M:7.582258424631621e-05 - 1677.352440331216\n",
            "lse step\n",
            "It 102 Delta: 1.6661534374406983e-08  Loss: 35.63982089286677\n",
            "M_0:1.1698579786205776 - 12.218711992090697\n",
            "lse step\n",
            "M_1:1.1943329797066713 - 9.162700045473157\n",
            "lse step\n",
            "M_2:1.2156151612125314 - 13.287627594696824\n",
            "lse step\n",
            "M_3:1.3964531318955449 - 11.888627392490278\n",
            "lse step\n",
            "M_4:1.4507552610657881 - 13.588868551295883\n",
            "lse step\n",
            "M_5:1.2647215681139836 - 11.34557990459303\n",
            "lse step\n",
            "M_6:1.4984048788726043 - 14.52666953974188\n",
            "lse step\n",
            "M_7:1.3895065851459512 - 14.767502689981736\n",
            "lse step\n",
            "M_8:1.496893524919747 - 11.651683523502712\n",
            "lse step\n",
            "M_9:1.4305911135639138 - 12.904094592569198\n",
            "lse step\n",
            "M:7.582257784868003e-05 - 1677.352465225361\n",
            "lse step\n",
            "It 103 Delta: 1.9434509113125387e-08  Loss: 35.6398104596018\n",
            "M_0:1.1698591222398438 - 12.218711584542637\n",
            "lse step\n",
            "M_1:1.194334080604038 - 9.162699802059798\n",
            "lse step\n",
            "M_2:1.2156160394521163 - 13.287628504478725\n",
            "lse step\n",
            "M_3:1.3964541732856153 - 11.888626160364797\n",
            "lse step\n",
            "M_4:1.4507570641257064 - 13.588868528066387\n",
            "lse step\n",
            "M_5:1.2647229292808033 - 11.345580404888327\n",
            "lse step\n",
            "M_6:1.4984064205678442 - 14.526669379084655\n",
            "lse step\n",
            "M_7:1.3895076711129652 - 14.767503846776256\n",
            "lse step\n",
            "M_8:1.496894120181521 - 11.651684474519344\n",
            "lse step\n",
            "M_9:1.4305919988728883 - 12.904096999261764\n",
            "lse step\n",
            "M:7.582258170579004e-05 - 1677.352431190278\n",
            "lse step\n",
            "It 104 Delta: 1.4519137003787819e-08  Loss: 35.639817963429536\n",
            "M_0:1.1698587443448427 - 12.218711618136982\n",
            "lse step\n",
            "M_1:1.1943337455128944 - 9.162700130998278\n",
            "lse step\n",
            "M_2:1.215615636865761 - 13.287629259474818\n",
            "lse step\n",
            "M_3:1.3964535878742212 - 11.888626621721842\n",
            "lse step\n",
            "M_4:1.4507562814427675 - 13.588868076777228\n",
            "lse step\n",
            "M_5:1.264722317698662 - 11.345580213181927\n",
            "lse step\n",
            "M_6:1.498405926525495 - 14.526668830334636\n",
            "lse step\n",
            "M_7:1.3895073075467743 - 14.767504950536782\n",
            "lse step\n",
            "M_8:1.4968937567416842 - 11.651683545777848\n",
            "lse step\n",
            "M_9:1.4305916332269994 - 12.904096510336682\n",
            "lse step\n",
            "M:7.582257644383274e-05 - 1677.352455613699\n",
            "lse step\n",
            "It 105 Delta: 1.2200795929118158e-08  Loss: 35.639810352591276\n",
            "M_0:1.169859117780257 - 12.218711639890808\n",
            "lse step\n",
            "M_1:1.194334115562448 - 9.16270012302448\n",
            "lse step\n",
            "M_2:1.2156160015200703 - 13.287628724647869\n",
            "lse step\n",
            "M_3:1.3964541195622155 - 11.888626296514103\n",
            "lse step\n",
            "M_4:1.4507569555184543 - 13.588868159000912\n",
            "lse step\n",
            "M_5:1.2647228466537466 - 11.345580456268278\n",
            "lse step\n",
            "M_6:1.498406330211568 - 14.526669283129152\n",
            "lse step\n",
            "M_7:1.3895076265952053 - 14.767504420094177\n",
            "lse step\n",
            "M_8:1.4968940316487356 - 11.651683825695075\n",
            "lse step\n",
            "M_9:1.4305920495832274 - 12.904097210532074\n",
            "lse step\n",
            "M:7.582258065439702e-05 - 1677.3524133824196\n",
            "lse step\n",
            "It 106 Delta: 1.6207483355401564e-08  Loss: 35.63981884380344\n",
            "M_0:1.1698582377403894 - 12.218711404101954\n",
            "lse step\n",
            "M_1:1.1943332070003363 - 9.162700033407717\n",
            "lse step\n",
            "M_2:1.2156153782664043 - 13.287627788319076\n",
            "lse step\n",
            "M_3:1.396453552508902 - 11.888626997495603\n",
            "lse step\n",
            "M_4:1.4507559263711478 - 13.588867429181343\n",
            "lse step\n",
            "M_5:1.2647219354094617 - 11.345580167794957\n",
            "lse step\n",
            "M_6:1.4984052313533276 - 14.526668578837503\n",
            "lse step\n",
            "M_7:1.3895067566596218 - 14.767503386278033\n",
            "lse step\n",
            "M_8:1.4968937322507052 - 11.651683394296214\n",
            "lse step\n",
            "M_9:1.430591432673839 - 12.904095331852243\n",
            "lse step\n",
            "M:7.58225758285403e-05 - 1677.3524684704273\n",
            "lse step\n",
            "It 107 Delta: 1.8723939732012695e-08  Loss: 35.63981453288378\n",
            "M_0:1.1698588091164046 - 12.21871146714188\n",
            "lse step\n",
            "M_1:1.1943337991687832 - 9.162700110365318\n",
            "lse step\n",
            "M_2:1.2156156224861587 - 13.287628753202815\n",
            "lse step\n",
            "M_3:1.396453595927685 - 11.88862689172612\n",
            "lse step\n",
            "M_4:1.4507562631789104 - 13.588868165915933\n",
            "lse step\n",
            "M_5:1.2647223109120986 - 11.345580630550332\n",
            "lse step\n",
            "M_6:1.4984059982783506 - 14.526669084296737\n",
            "lse step\n",
            "M_7:1.3895073268527474 - 14.767504107767767\n",
            "lse step\n",
            "M_8:1.4968937956742143 - 11.65168354005534\n",
            "lse step\n",
            "M_9:1.4305917279891904 - 12.904096810433389\n",
            "lse step\n",
            "M:7.582257268191522e-05 - 1677.3524286127204\n",
            "lse step\n",
            "It 108 Delta: 8.395653949833104e-09  Loss: 35.639815360180165\n",
            "M_0:1.1698588021277825 - 12.218712100967458\n",
            "lse step\n",
            "M_1:1.194333786294241 - 9.162700187931438\n",
            "lse step\n",
            "M_2:1.2156155601972147 - 13.287629014485557\n",
            "lse step\n",
            "M_3:1.3964536119998547 - 11.888627005025729\n",
            "lse step\n",
            "M_4:1.4507562385962696 - 13.588868622830946\n",
            "lse step\n",
            "M_5:1.264722377898051 - 11.345580719648906\n",
            "lse step\n",
            "M_6:1.4984060501845917 - 14.526669859837025\n",
            "lse step\n",
            "M_7:1.3895073709385244 - 14.767503524337352\n",
            "lse step\n",
            "M_8:1.4968937678963317 - 11.651683840819928\n",
            "lse step\n",
            "M_9:1.43059162643897 - 12.904097112122892\n",
            "lse step\n",
            "M:7.582257902764069e-05 - 1677.3524470717498\n",
            "lse step\n",
            "It 109 Delta: 8.705844045664435e-09  Loss: 35.639814068132004\n",
            "M_0:1.1698589279389575 - 12.218711526567239\n",
            "lse step\n",
            "M_1:1.194333867211428 - 9.162700377564608\n",
            "lse step\n",
            "M_2:1.2156156314553135 - 13.287628835803506\n",
            "lse step\n",
            "M_3:1.3964536348034065 - 11.888627081795654\n",
            "lse step\n",
            "M_4:1.4507562680407733 - 13.588867948394476\n",
            "lse step\n",
            "M_5:1.2647224307478329 - 11.34558070044384\n",
            "lse step\n",
            "M_6:1.4984062053023988 - 14.526669620699707\n",
            "lse step\n",
            "M_7:1.3895074024657879 - 14.767504096679312\n",
            "lse step\n",
            "M_8:1.496893813717701 - 11.6516838678279\n",
            "lse step\n",
            "M_9:1.4305917458876865 - 12.904097393000926\n",
            "lse step\n",
            "M:7.582257566706845e-05 - 1677.3524752305007\n",
            "lse step\n",
            "It 110 Delta: 1.1875394889671043e-08  Loss: 35.639809583628626\n",
            "M_0:1.1698590539860991 - 12.21871205611271\n",
            "lse step\n",
            "M_1:1.1943339590046333 - 9.16270012335974\n",
            "lse step\n",
            "M_2:1.2156158536706343 - 13.287629783934761\n",
            "lse step\n",
            "M_3:1.3964540314693563 - 11.888626762126378\n",
            "lse step\n",
            "M_4:1.4507568743493842 - 13.588867874912932\n",
            "lse step\n",
            "M_5:1.264722662929684 - 11.345580368229648\n",
            "lse step\n",
            "M_6:1.498406306318405 - 14.526669439993777\n",
            "lse step\n",
            "M_7:1.3895075567843922 - 14.767504558017698\n",
            "lse step\n",
            "M_8:1.4968940200792513 - 11.651683957838463\n",
            "lse step\n",
            "M_9:1.4305919874216462 - 12.904098023429414\n",
            "lse step\n",
            "M:7.582257385732367e-05 - 1677.3524225461888\n",
            "lse step\n",
            "It 111 Delta: 1.906275493013254e-08  Loss: 35.639821753773724\n",
            "M_0:1.169858147662661 - 12.21871038728695\n",
            "lse step\n",
            "M_1:1.1943331461262618 - 9.16270031216309\n",
            "lse step\n",
            "M_2:1.2156152128689786 - 13.287627931378935\n",
            "lse step\n",
            "M_3:1.3964533563447408 - 11.888627399564259\n",
            "lse step\n",
            "M_4:1.4507556190456659 - 13.588866014877855\n",
            "lse step\n",
            "M_5:1.264721680502619 - 11.345580366399647\n",
            "lse step\n",
            "M_6:1.4984051684639073 - 14.526668132451846\n",
            "lse step\n",
            "M_7:1.3895067593365353 - 14.767504090250165\n",
            "lse step\n",
            "M_8:1.4968937133208156 - 11.651682234973665\n",
            "lse step\n",
            "M_9:1.4305913277841424 - 12.904095060746078\n",
            "lse step\n",
            "M:7.582256860718313e-05 - 1677.3524592927067\n",
            "lse step\n",
            "It 112 Delta: 1.6536091607122216e-08  Loss: 35.63981332228918\n",
            "M_0:1.169858782882875 - 12.218710739711037\n",
            "lse step\n",
            "M_1:1.194333754425915 - 9.162700487751758\n",
            "lse step\n",
            "M_2:1.2156155440522087 - 13.287628935493133\n",
            "lse step\n",
            "M_3:1.396453589564154 - 11.888627258739506\n",
            "lse step\n",
            "M_4:1.450756187519438 - 13.588867463842899\n",
            "lse step\n",
            "M_5:1.26472222493635 - 11.345580782037949\n",
            "lse step\n",
            "M_6:1.498405951103138 - 14.526668354653731\n",
            "lse step\n",
            "M_7:1.389507273462644 - 14.767505029413647\n",
            "lse step\n",
            "M_8:1.49689376950032 - 11.651683207284947\n",
            "lse step\n",
            "M_9:1.4305917467986344 - 12.904097420145266\n",
            "lse step\n",
            "M:7.582256936926344e-05 - 1677.3524641988672\n",
            "lse step\n",
            "It 113 Delta: 9.411059487263174e-09  Loss: 35.63981417248088\n",
            "M_0:1.1698586957042665 - 12.218711016555323\n",
            "lse step\n",
            "M_1:1.1943336250260128 - 9.162700596950344\n",
            "lse step\n",
            "M_2:1.215615422253103 - 13.287629397565812\n",
            "lse step\n",
            "M_3:1.3964534836436389 - 11.888627382314844\n",
            "lse step\n",
            "M_4:1.4507561251518584 - 13.588867642128548\n",
            "lse step\n",
            "M_5:1.2647221673858327 - 11.345581196202975\n",
            "lse step\n",
            "M_6:1.4984059276931725 - 14.526668274388255\n",
            "lse step\n",
            "M_7:1.3895072401919892 - 14.767505713258656\n",
            "lse step\n",
            "M_8:1.496893667113206 - 11.65168337832539\n",
            "lse step\n",
            "M_9:1.4305916110895416 - 12.90409773523401\n",
            "lse step\n",
            "M:7.582256499886479e-05 - 1677.352430476863\n",
            "lse step\n",
            "It 114 Delta: 7.048608363646736e-09  Loss: 35.639813308348174\n",
            "M_0:1.1698586805699038 - 12.21871099382005\n",
            "lse step\n",
            "M_1:1.1943336469686523 - 9.162700313670731\n",
            "lse step\n",
            "M_2:1.2156154176685534 - 13.287629390497946\n",
            "lse step\n",
            "M_3:1.3964534814200509 - 11.888627271393396\n",
            "lse step\n",
            "M_4:1.4507561707762666 - 13.58886720288887\n",
            "lse step\n",
            "M_5:1.2647221708650411 - 11.345580814962915\n",
            "lse step\n",
            "M_6:1.4984059206683078 - 14.526668414332192\n",
            "lse step\n",
            "M_7:1.3895072271064532 - 14.767505447408613\n",
            "lse step\n",
            "M_8:1.4968936984559196 - 11.651683309677637\n",
            "lse step\n",
            "M_9:1.4305916160537921 - 12.904097897002568\n",
            "lse step\n",
            "M:7.58225730440054e-05 - 1677.3523886360053\n",
            "lse step\n",
            "It 115 Delta: 1.5745341030992677e-08  Loss: 35.63982243723113\n",
            "M_0:1.169857990026297 - 12.218711512378343\n",
            "lse step\n",
            "M_1:1.194332945770366 - 9.162700524522013\n",
            "lse step\n",
            "M_2:1.215614885595822 - 13.287628588121049\n",
            "lse step\n",
            "M_3:1.3964530593684858 - 11.888628095648018\n",
            "lse step\n",
            "M_4:1.450755207703811 - 13.5888669883719\n",
            "lse step\n",
            "M_5:1.2647214078330835 - 11.345581137681918\n",
            "lse step\n",
            "M_6:1.4984049850038508 - 14.526669199905246\n",
            "lse step\n",
            "M_7:1.389506546350661 - 14.767503830036663\n",
            "lse step\n",
            "M_8:1.4968934525400257 - 11.651682798613152\n",
            "lse step\n",
            "M_9:1.4305911613406885 - 12.904096519049395\n",
            "lse step\n",
            "M:7.582256625173717e-05 - 1677.3523965383883\n",
            "lse step\n",
            "It 116 Delta: 1.3400851983647044e-08  Loss: 35.63982073364826\n",
            "M_0:1.1698579638540374 - 12.218710877997548\n",
            "lse step\n",
            "M_1:1.19433290958945 - 9.162700541411544\n",
            "lse step\n",
            "M_2:1.2156150390800444 - 13.287628733675039\n",
            "lse step\n",
            "M_3:1.3964534031998277 - 11.88862793072248\n",
            "lse step\n",
            "M_4:1.4507556866546527 - 13.588866418154042\n",
            "lse step\n",
            "M_5:1.2647215575551636 - 11.345580760424602\n",
            "lse step\n",
            "M_6:1.4984049284944576 - 14.526667961533667\n",
            "lse step\n",
            "M_7:1.3895066233565054 - 14.767504413357203\n",
            "lse step\n",
            "M_8:1.4968936290206436 - 11.651682730510796\n",
            "lse step\n",
            "M_9:1.4305912847600195 - 12.90409655111707\n",
            "lse step\n",
            "M:7.582256587600086e-05 - 1677.3524571533\n",
            "lse step\n",
            "It 117 Delta: 1.6357757814944307e-08  Loss: 35.63980966358395\n",
            "M_0:1.1698591309615363 - 12.21871021073665\n",
            "lse step\n",
            "M_1:1.1943341073345333 - 9.162700314892657\n",
            "lse step\n",
            "M_2:1.2156157975430586 - 13.287629189604143\n",
            "lse step\n",
            "M_3:1.396454120212155 - 11.888626886851428\n",
            "lse step\n",
            "M_4:1.450757073170481 - 13.588866239218339\n",
            "lse step\n",
            "M_5:1.264722730224043 - 11.34558103849944\n",
            "lse step\n",
            "M_6:1.4984064841589646 - 14.526668639206209\n",
            "lse step\n",
            "M_7:1.3895077381527672 - 14.767505697709518\n",
            "lse step\n",
            "M_8:1.4968941492527934 - 11.651683074599894\n",
            "lse step\n",
            "M_9:1.4305920421905371 - 12.904098107647156\n",
            "lse step\n",
            "M:7.58225605483099e-05 - 1677.3524352985003\n",
            "lse step\n",
            "It 118 Delta: 1.2121040171564346e-08  Loss: 35.639812050343686\n",
            "M_0:1.1698588197461286 - 12.218710873445476\n",
            "lse step\n",
            "M_1:1.1943338244273172 - 9.162700334092783\n",
            "lse step\n",
            "M_2:1.215615435703553 - 13.287629160719426\n",
            "lse step\n",
            "M_3:1.396453556854061 - 11.88862736136038\n",
            "lse step\n",
            "M_4:1.4507563294669232 - 13.588866903262906\n",
            "lse step\n",
            "M_5:1.2647222804791682 - 11.34558143640618\n",
            "lse step\n",
            "M_6:1.4984061911698805 - 14.52666836428077\n",
            "lse step\n",
            "M_7:1.389507376169279 - 14.767505239539792\n",
            "lse step\n",
            "M_8:1.496893801759846 - 11.65168275922576\n",
            "lse step\n",
            "M_9:1.430591719269089 - 12.904098288140808\n",
            "lse step\n",
            "M:7.582256185861182e-05 - 1677.3524047900755\n",
            "lse step\n",
            "It 119 Delta: 1.2768355261982833e-08  Loss: 35.63980771819686\n",
            "M_0:1.1698591469134136 - 12.218710378258148\n",
            "lse step\n",
            "M_1:1.1943341302921022 - 9.16270043293495\n",
            "lse step\n",
            "M_2:1.215615777475753 - 13.287629079019975\n",
            "lse step\n",
            "M_3:1.396454107969762 - 11.888626808302487\n",
            "lse step\n",
            "M_4:1.4507570746948133 - 13.588866404100878\n",
            "lse step\n",
            "M_5:1.2647227714106162 - 11.34558185473076\n",
            "lse step\n",
            "M_6:1.4984065402614526 - 14.526668132119793\n",
            "lse step\n",
            "M_7:1.3895077456243146 - 14.76750573900292\n",
            "lse step\n",
            "M_8:1.4968940840344491 - 11.651682714131967\n",
            "lse step\n",
            "M_9:1.4305920600836897 - 12.904098715118307\n",
            "lse step\n",
            "M:7.582256430285613e-05 - 1677.352464265316\n",
            "lse step\n",
            "It 120 Delta: 8.631682923976314e-09  Loss: 35.63980765369598\n",
            "converged at iter  120\n"
          ]
        }
      ],
      "source": [
        "# Ram will explod\n",
        "\n",
        "#ECOOT labeled\n",
        "#ecotl = [perturbot.match.get_coupling_cotl_sinkhorn(\n",
        "#        folds[i][\"data_train\"],\n",
        "#        eps=1e-2\n",
        "#    ) for i in range(len(folds))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bda9268e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bda9268e",
        "outputId": "8fa80e2b-2831-4ba0-e32c-92e27bf9a42b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Delta: 0.11993746088859492  Loss: 2.49764674096519\n",
            "Delta: 0.1492831966024853  Loss: 1.6041744980884043\n",
            "Delta: 0.09426406484516758  Loss: 1.5574699467813164\n",
            "Delta: 0.08126575420626933  Loss: 1.5405351029550278\n",
            "Delta: 0.06325021371669094  Loss: 1.5377404485540809\n",
            "Delta: 0.058722813232689665  Loss: 1.535788727904237\n",
            "Delta: 0.06306640938985257  Loss: 1.5326769842014576\n",
            "Delta: 0.050367962909822775  Loss: 1.5318723455075802\n",
            "Delta: 0.0  Loss: 1.5318723455075802\n",
            "converged at iter  8\n",
            "Delta: 0.11993746088859497  Loss: 2.5847412651707047\n",
            "Delta: 0.15053323475353314  Loss: 1.7517536634045496\n",
            "Delta: 0.1063680924774783  Loss: 1.6622027771349046\n",
            "Delta: 0.09443131218761702  Loss: 1.627044565110887\n",
            "Delta: 0.10116561600250473  Loss: 1.5743897332615346\n",
            "Delta: 0.09204998997439034  Loss: 1.5508775169748912\n",
            "Delta: 0.07510705129197362  Loss: 1.5472178073407246\n",
            "Delta: 0.059850236487159064  Loss: 1.5453849707800489\n",
            "Delta: 0.04878865319652406  Loss: 1.5444512892344306\n",
            "Delta: 0.037838821814150075  Loss: 1.5441392229757946\n",
            "Delta: 0.022079402165820064  Loss: 1.544115421497937\n",
            "Delta: 0.0  Loss: 1.544115421497937\n",
            "converged at iter  11\n",
            "Delta: 0.11993746088859497  Loss: 2.551627705986266\n",
            "Delta: 0.150264518868906  Loss: 1.7638471482067508\n",
            "Delta: 0.10822531725075774  Loss: 1.6095772556096608\n",
            "Delta: 0.08046677310093579  Loss: 1.594413573453509\n",
            "Delta: 0.09332691478805438  Loss: 1.5639288470330845\n",
            "Delta: 0.09790289189140747  Loss: 1.5210773842877823\n",
            "Delta: 0.0749992492302575  Loss: 1.5139612450081568\n",
            "Delta: 0.07343420458864669  Loss: 1.5100376740270296\n",
            "Delta: 0.06807123107922244  Loss: 1.5063312129631663\n",
            "Delta: 0.052196588463112786  Loss: 1.5049121374547536\n",
            "Delta: 0.06395797497107752  Loss: 1.5016820595698814\n",
            "Delta: 0.06948469592751749  Loss: 1.4983833609114035\n",
            "Delta: 0.060766408514340786  Loss: 1.4942151123360432\n",
            "Delta: 0.06856812155135358  Loss: 1.4902690350271806\n",
            "Delta: 0.05243819342758825  Loss: 1.4877244983329527\n",
            "Delta: 0.05656114536731209  Loss: 1.4870280874639723\n",
            "Delta: 0.03986085125688363  Loss: 1.4864969464254871\n",
            "Delta: 0.0  Loss: 1.4864969464254871\n",
            "converged at iter  17\n",
            "Delta: 0.11993746088859505  Loss: 2.567998266845097\n",
            "Delta: 0.15194275656310527  Loss: 1.7382531299012576\n",
            "Delta: 0.1155093930792633  Loss: 1.5759032925177752\n",
            "Delta: 0.09604543334501074  Loss: 1.5362731718682427\n",
            "Delta: 0.0862310562561763  Loss: 1.5254747101209563\n",
            "Delta: 0.04821509179288108  Loss: 1.5241295210071475\n",
            "Delta: 0.008660254037844635  Loss: 1.5241165115830217\n",
            "Delta: 0.0  Loss: 1.5241165115830217\n",
            "converged at iter  7\n",
            "Delta: 0.1199374608885949  Loss: 2.538128166846278\n",
            "Delta: 0.14977611609692687  Loss: 1.8315898551438496\n",
            "Delta: 0.1052950399390255  Loss: 1.7783182613724522\n",
            "Delta: 0.08138450170851853  Loss: 1.757033753905906\n",
            "Delta: 0.07705363488324021  Loss: 1.7449832584728686\n",
            "Delta: 0.10416305677161888  Loss: 1.6999330273720772\n",
            "Delta: 0.09409240369532429  Loss: 1.6734339062532801\n",
            "Delta: 0.0757397138183698  Loss: 1.659013367474357\n",
            "Delta: 0.04622022120425376  Loss: 1.658386822170344\n",
            "Delta: 0.05370217876750294  Loss: 1.6580944546458105\n",
            "Delta: 0.058491886181447704  Loss: 1.6567823275558\n",
            "Delta: 0.0780788655293192  Loss: 1.6472200337852918\n",
            "Delta: 0.07394001397930393  Loss: 1.640120311487906\n",
            "Delta: 0.05188102530308998  Loss: 1.639286288862595\n",
            "Delta: 0.04692932824524079  Loss: 1.6373256598430288\n",
            "Delta: 0.020310096011589937  Loss: 1.6372315012502487\n",
            "Delta: 0.0  Loss: 1.6372315012502487\n",
            "converged at iter  16\n"
          ]
        }
      ],
      "source": [
        "#COOT without labels\n",
        "cot=    [perturbot.match.get_coupling_cot(\n",
        "        folds[i][\"data_train\"],\n",
        "    ) for i in range(len(folds))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1a8e7375",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a8e7375",
        "outputId": "f1aa8a77-6851-41f1-a811-8878ac12c06e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "calculating with eps 0.5\n",
            "lse step\n",
            "lse step\n",
            "Delta: 0.00048459246771793274  Loss: 4.698151771105321\n",
            "lse step\n",
            "lse step\n",
            "Delta: 2.0483441858232254e-06  Loss: 4.698133917433545\n",
            "lse step\n",
            "lse step\n",
            "Delta: 1.0717863041520559e-08  Loss: 4.6981340788062065\n",
            "lse step\n",
            "lse step\n",
            "Delta: 2.8053090961321914e-09  Loss: 4.698134074360974\n",
            "converged at iter  3\n",
            "calculating with eps 0.5\n",
            "lse step\n",
            "lse step\n",
            "Delta: 0.0004657801196778213  Loss: 4.795991302579692\n",
            "lse step\n",
            "lse step\n",
            "Delta: 2.1942764760751743e-06  Loss: 4.795974513409747\n",
            "lse step\n",
            "lse step\n",
            "Delta: 9.33427113380958e-09  Loss: 4.795974895737295\n",
            "lse step\n",
            "lse step\n",
            "Delta: 8.017729258291695e-10  Loss: 4.795974894832713\n",
            "converged at iter  3\n",
            "calculating with eps 0.5\n",
            "lse step\n",
            "lse step\n",
            "Delta: 0.0004918393208294741  Loss: 4.724306080610469\n",
            "lse step\n",
            "lse step\n",
            "Delta: 2.18456852962845e-06  Loss: 4.724287346152258\n",
            "lse step\n",
            "lse step\n",
            "Delta: 9.55880619102345e-09  Loss: 4.724287440646741\n",
            "converged at iter  2\n",
            "calculating with eps 0.5\n",
            "lse step\n",
            "lse step\n",
            "Delta: 0.0005022818141798783  Loss: 4.721774264435604\n",
            "lse step\n",
            "lse step\n",
            "Delta: 2.0435820715647424e-06  Loss: 4.721754637752527\n",
            "lse step\n",
            "lse step\n",
            "Delta: 8.929244899036348e-09  Loss: 4.721754944272605\n",
            "lse step\n",
            "lse step\n",
            "Delta: 6.948037700738041e-10  Loss: 4.721754948301542\n",
            "converged at iter  3\n",
            "calculating with eps 0.5\n",
            "lse step\n",
            "lse step\n",
            "Delta: 0.0004630906010154559  Loss: 4.718107620813187\n",
            "lse step\n",
            "lse step\n",
            "Delta: 1.8254845599585678e-06  Loss: 4.718096923564392\n",
            "lse step\n",
            "lse step\n",
            "Delta: 6.794186546699166e-09  Loss: 4.718096790434985\n",
            "lse step\n",
            "lse step\n",
            "Delta: 1.2876183452803502e-09  Loss: 4.718096805980905\n",
            "converged at iter  3\n"
          ]
        }
      ],
      "source": [
        "#ECOOT without labels\n",
        "ecot = [perturbot.match.get_coupling_cot_sinkhorn(\n",
        "        folds[i][\"data_train\"],\n",
        "        eps=0.500\n",
        "    ) for i in range(len(folds))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "d2f9f334",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2f9f334",
        "outputId": "47087cd9-cffa-432d-83bc-f80a00231102"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calcul du transport plan Fused Gromov-Wasserstein (alpha=1 -> GW uniquement)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/ot/bregman/_sinkhorn.py:666: UserWarning: Sinkhorn did not converge. You might want to increase the number of iterations `numItermax` or the regularization parameter `reg`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It.  |Err         \n",
            "-------------------\n",
            "    0|1.516728e-02|\n",
            "   10|2.191249e-03|\n",
            "   20|8.704066e-04|\n",
            "   30|1.507325e-04|\n",
            "   40|3.021989e-05|\n",
            "   50|6.315276e-06|\n",
            "   60|1.332392e-06|\n",
            "   70|2.816917e-07|\n",
            "   80|5.958113e-08|\n",
            "   90|1.260330e-08|\n",
            "  100|2.666052e-09|\n",
            "  110|5.639682e-10|\n",
            "Calcul du transport plan Fused Gromov-Wasserstein (alpha=1 -> GW uniquement)...\n",
            "It.  |Err         \n",
            "-------------------\n",
            "    0|1.470532e-02|\n",
            "   10|3.191627e-04|\n",
            "   20|1.258381e-04|\n",
            "   30|1.055900e-04|\n",
            "   40|1.752555e-04|\n",
            "   50|4.854371e-04|\n",
            "   60|8.408451e-04|\n",
            "   70|3.860429e-04|\n",
            "   80|2.004740e-04|\n",
            "   90|1.392129e-04|\n",
            "  100|2.297872e-04|\n",
            "  110|2.129602e-04|\n",
            "  120|4.309297e-05|\n",
            "  130|8.864244e-06|\n",
            "  140|1.887187e-06|\n",
            "  150|4.028552e-07|\n",
            "  160|8.600182e-08|\n",
            "  170|1.835911e-08|\n",
            "  180|3.919136e-09|\n",
            "  190|8.366188e-10|\n",
            "Calcul du transport plan Fused Gromov-Wasserstein (alpha=1 -> GW uniquement)...\n",
            "It.  |Err         \n",
            "-------------------\n",
            "    0|1.444320e-02|\n",
            "   10|2.610911e-04|\n",
            "   20|1.162999e-05|\n",
            "   30|9.825475e-07|\n",
            "   40|9.281055e-08|\n",
            "   50|8.831273e-09|\n",
            "   60|8.404886e-10|\n",
            "Calcul du transport plan Fused Gromov-Wasserstein (alpha=1 -> GW uniquement)...\n",
            "It.  |Err         \n",
            "-------------------\n",
            "    0|1.437922e-02|\n",
            "   10|7.037795e-04|\n",
            "   20|1.079932e-03|\n",
            "   30|4.324680e-04|\n",
            "   40|1.872228e-05|\n",
            "   50|7.871658e-07|\n",
            "   60|3.387064e-08|\n",
            "   70|1.477870e-09|\n",
            "   80|6.495906e-11|\n",
            "Calcul du transport plan Fused Gromov-Wasserstein (alpha=1 -> GW uniquement)...\n",
            "It.  |Err         \n",
            "-------------------\n",
            "    0|1.435310e-02|\n",
            "   10|1.421051e-03|\n",
            "   20|2.265280e-04|\n",
            "   30|9.437912e-05|\n",
            "   40|5.139945e-05|\n",
            "   50|3.112142e-05|\n",
            "   60|1.983296e-05|\n",
            "   70|1.298863e-05|\n",
            "   80|8.639689e-06|\n",
            "   90|5.800773e-06|\n",
            "  100|3.917372e-06|\n",
            "  110|2.655310e-06|\n",
            "  120|1.804206e-06|\n",
            "  130|1.227867e-06|\n",
            "  140|8.365275e-07|\n",
            "  150|5.703229e-07|\n",
            "  160|3.890200e-07|\n",
            "  170|2.654396e-07|\n",
            "  180|1.811576e-07|\n",
            "  190|1.236555e-07|\n",
            "It.  |Err         \n",
            "-------------------\n",
            "  200|8.441414e-08|\n",
            "  210|5.762987e-08|\n",
            "  220|3.934603e-08|\n",
            "  230|2.686387e-08|\n",
            "  240|1.834196e-08|\n",
            "  250|1.252362e-08|\n",
            "  260|8.551027e-09|\n",
            "  270|5.838615e-09|\n",
            "  280|3.986608e-09|\n",
            "  290|2.722066e-09|\n",
            "  300|1.858638e-09|\n",
            "  310|1.269088e-09|\n",
            "  320|8.665405e-10|\n"
          ]
        }
      ],
      "source": [
        "#Fused GW\n",
        "def get_coupling_fused_gw(X_train, Y_train, labels_train, epsilon=0.001, alpha=0.99, max_iter=500):\n",
        "    \"\"\"\n",
        "    Compute the Fused Gromov-Wasserstein coupling between X_train and Y_train\n",
        "    using labels_train for the fused cost.\n",
        "    \"\"\"\n",
        "    # -----------------------------\n",
        "    # 1. Matrices de coût C1 et C2\n",
        "    # -----------------------------\n",
        "    # Calcul des matrices de coût quadratiques normalisées\n",
        "    C1 = ot.dist(X_train, X_train, metric='euclidean')**2\n",
        "    C2 = ot.dist(Y_train, Y_train, metric='euclidean')**2\n",
        "    C1=C1/C1.max()\n",
        "    C2=C2/C2.max()\n",
        "\n",
        "    # -----------------------------\n",
        "    # 2. Matrice de coût fused M\n",
        "    # -----------------------------\n",
        "    # Ici M[i,j] = 1 si labels différents, 0 sinon\n",
        "    start = time.time()\n",
        "    n_train = X_train.shape[0]\n",
        "    m_train = Y_train.shape[0]\n",
        "    labels_X = labels_train[:, np.newaxis]  # shape (n_train, 1)\n",
        "    labels_Y = labels_train[np.newaxis, :]  # shape (1, m_train)\n",
        "\n",
        "    # M = 1 si labels différents\n",
        "    M = (labels_X != labels_Y).astype(float)\n",
        "\n",
        "    print(\"Calcul du transport plan Fused Gromov-Wasserstein (alpha=1 -> GW uniquement)...\")\n",
        "    T = ot.gromov.entropic_fused_gromov_wasserstein(\n",
        "        M, C1, C2, alpha=alpha, epsilon=epsilon, max_iter=max_iter, verbose=True\n",
        "    )\n",
        "    end= time.time()\n",
        "    runtime = end - start\n",
        "    return T, {\"time\": runtime}\n",
        "fgw= [get_coupling_fused_gw(folds[i][\"X_train_raw\"], folds[i][\"Y_train_raw\"], folds[i][\"labels_train_concat\"], epsilon=0.001, alpha=0.99, max_iter=500) for i in range(len(folds))]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "489b5d85",
      "metadata": {
        "id": "489b5d85"
      },
      "source": [
        "## **Train MLP to evaluate performances on obtained couplings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "d75c94b3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d75c94b3",
        "outputId": "058f9c40-cc7a-4ed0-bb73-cb4db773cd0d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Evaluation of the prediction models\n",
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 184/2000:   9%|▉         | 184/2000 [00:08<01:28, 20.56it/s, v_num=70, train_loss_epoch=0.963]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 0.706. Signaling Trainer to stop.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/7] EGWOT Labeled (legw): MSE=1.0271, Pearson=0.8006\n",
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 122/2000:   6%|▌         | 122/2000 [00:05<01:25, 22.09it/s, v_num=71, train_loss_epoch=1.46]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 1.018. Signaling Trainer to stop.\n",
            "[2/7] EGWOT All (egw): MSE=1.5639, Pearson=0.6847\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 123/2000:   6%|▌         | 123/2000 [00:06<01:33, 20.12it/s, v_num=72, train_loss_epoch=1.54]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 0.883. Signaling Trainer to stop.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3/7] EGWOT Per Label (egwper): MSE=1.4573, Pearson=0.7109\n",
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 166/2000:   8%|▊         | 166/2000 [00:07<01:24, 21.63it/s, v_num=73, train_loss_epoch=0.527]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 0.775. Signaling Trainer to stop.\n",
            "[4/7] COOT Labeled (cotl): MSE=1.2810, Pearson=0.7534\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 284/2000:  14%|█▍        | 284/2000 [00:13<01:24, 20.29it/s, v_num=74, train_loss_epoch=0.157]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 0.319. Signaling Trainer to stop.\n",
            "[5/7] COOT All (cot): MSE=3.3427, Pearson=0.4131\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 93/2000:   5%|▍         | 93/2000 [00:05<01:44, 18.31it/s, v_num=75, train_loss_epoch=1.47]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 1.013. Signaling Trainer to stop.\n",
            "[6/7] ECOOT All (ecot): MSE=1.6096, Pearson=0.6722\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 163/2000:   8%|▊         | 163/2000 [00:07<01:30, 20.38it/s, v_num=76, train_loss_epoch=0.726]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 0.641. Signaling Trainer to stop.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[7/7] Fused GW (fgw): MSE=1.1441, Pearson=0.7748\n",
            "\n",
            "Summary Table of Prediction Metrics for fold0\n",
            "   Pearson_corr  Spearman_corr  Pearson_samples  Spearman_samples       MSE  \\\n",
            "0      0.800622       0.776271         0.509766          0.493502  1.027143   \n",
            "1      0.684694       0.671555         0.079354          0.077061  1.563907   \n",
            "2      0.710925       0.694591         0.262618          0.252481  1.457340   \n",
            "3      0.753402       0.729243         0.430479          0.415664  1.281006   \n",
            "4      0.413148       0.451287        -0.151932         -0.146565  3.342661   \n",
            "5      0.672182       0.663124        -0.004590         -0.006229  1.609624   \n",
            "6      0.774815       0.751191         0.446359          0.430379  1.144098   \n",
            "\n",
            "                     method                                               time  \n",
            "0      EGWOT Labeled (legw)  {'n_iters_outer': 5, 'converged_inner': True, ...  \n",
            "1           EGWOT All (egw)  {'n_iters_outer': 5, 'converged_inner': True, ...  \n",
            "2  EGWOT Per Label (egwper)  {0: {'n_iters_outer': 5, 'converged_inner': Tr...  \n",
            "3       COOT Labeled (cotl)  {'cost': [25.67649195831666, 19.99712314430541...  \n",
            "4            COOT All (cot)  {'cost': [2.49764674096519, 1.6041744980884043...  \n",
            "5          ECOOT All (ecot)  {'cost': [4.698151771105321, 4.698133917433545...  \n",
            "6            Fused GW (fgw)                       {'time': 23.932862520217896}  \n",
            "\n",
            " Evaluation of the prediction models\n",
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 147/2000:   7%|▋         | 147/2000 [00:07<01:30, 20.41it/s, v_num=77, train_loss_epoch=0.992]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 0.959. Signaling Trainer to stop.\n",
            "[1/7] EGWOT Labeled (legw): MSE=1.0234, Pearson=0.7803\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 156/2000:   8%|▊         | 156/2000 [00:08<01:34, 19.43it/s, v_num=78, train_loss_epoch=1.55]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 1.149. Signaling Trainer to stop.\n",
            "[2/7] EGWOT All (egw): MSE=1.4167, Pearson=0.6984\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 118/2000:   6%|▌         | 118/2000 [00:05<01:28, 21.37it/s, v_num=79, train_loss_epoch=1.65]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 1.096. Signaling Trainer to stop.\n",
            "[3/7] EGWOT Per Label (egwper): MSE=1.3077, Pearson=0.7263\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 184/2000:   9%|▉         | 184/2000 [00:08<01:28, 20.49it/s, v_num=80, train_loss_epoch=0.479]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 0.706. Signaling Trainer to stop.\n",
            "[4/7] COOT Labeled (cotl): MSE=1.2138, Pearson=0.7533\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 229/2000:  11%|█▏        | 229/2000 [00:10<01:24, 20.91it/s, v_num=81, train_loss_epoch=0.193]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 0.315. Signaling Trainer to stop.\n",
            "[5/7] COOT All (cot): MSE=2.9421, Pearson=0.4499\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 105/2000:   5%|▌         | 105/2000 [00:04<01:25, 22.22it/s, v_num=82, train_loss_epoch=1.56]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 1.117. Signaling Trainer to stop.\n",
            "[6/7] ECOOT All (ecot): MSE=1.4229, Pearson=0.6966\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 222/2000:  11%|█         | 222/2000 [00:11<01:28, 20.10it/s, v_num=83, train_loss_epoch=0.735]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 0.594. Signaling Trainer to stop.\n",
            "[7/7] Fused GW (fgw): MSE=1.1773, Pearson=0.7467\n",
            "\n",
            "Summary Table of Prediction Metrics for fold1\n",
            "   Pearson_corr  Spearman_corr  Pearson_samples  Spearman_samples       MSE  \\\n",
            "0      0.780305       0.757490         0.454507          0.412807  1.023418   \n",
            "1      0.698431       0.684333         0.023662          0.025860  1.416657   \n",
            "2      0.726289       0.705541         0.243902          0.225655  1.307737   \n",
            "3      0.753270       0.735876         0.440067          0.425613  1.213834   \n",
            "4      0.449900       0.484402        -0.053044         -0.055300  2.942125   \n",
            "5      0.696604       0.683544         0.028240          0.026618  1.422915   \n",
            "6      0.746741       0.728712         0.397436          0.361577  1.177309   \n",
            "\n",
            "                     method                                               time  \n",
            "0      EGWOT Labeled (legw)  {'n_iters_outer': 5, 'converged_inner': True, ...  \n",
            "1           EGWOT All (egw)  {'n_iters_outer': 5, 'converged_inner': True, ...  \n",
            "2  EGWOT Per Label (egwper)  {0: {'n_iters_outer': 5, 'converged_inner': Tr...  \n",
            "3       COOT Labeled (cotl)  {'cost': [26.373504429201724, 20.4439884160194...  \n",
            "4            COOT All (cot)  {'cost': [2.5847412651707047, 1.75175366340454...  \n",
            "5          ECOOT All (ecot)  {'cost': [4.795991302579692, 4.795974513409747...  \n",
            "6            Fused GW (fgw)                       {'time': 36.745893716812134}  \n",
            "\n",
            " Evaluation of the prediction models\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 151/2000:   8%|▊         | 151/2000 [00:08<01:41, 18.18it/s, v_num=84, train_loss_epoch=0.994]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 0.677. Signaling Trainer to stop.\n",
            "[1/7] EGWOT Labeled (legw): MSE=1.1610, Pearson=0.7738\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 115/2000:   6%|▌         | 115/2000 [00:05<01:31, 20.67it/s, v_num=85, train_loss_epoch=1.57]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 0.832. Signaling Trainer to stop.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2/7] EGWOT All (egw): MSE=1.5980, Pearson=0.6742\n",
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 156/2000:   8%|▊         | 156/2000 [00:08<01:35, 19.26it/s, v_num=86, train_loss_epoch=1.47]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 0.876. Signaling Trainer to stop.\n",
            "[3/7] EGWOT Per Label (egwper): MSE=1.3898, Pearson=0.7274\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 238/2000:  12%|█▏        | 238/2000 [00:12<01:29, 19.64it/s, v_num=87, train_loss_epoch=0.401]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 0.642. Signaling Trainer to stop.\n",
            "[4/7] COOT Labeled (cotl): MSE=1.6026, Pearson=0.6929\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 221/2000:  11%|█         | 221/2000 [00:11<01:30, 19.63it/s, v_num=88, train_loss_epoch=0.18]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 0.292. Signaling Trainer to stop.\n",
            "[5/7] COOT All (cot): MSE=3.3768, Pearson=0.3945\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 104/2000:   5%|▌         | 104/2000 [00:05<01:35, 19.89it/s, v_num=89, train_loss_epoch=1.48]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 1.153. Signaling Trainer to stop.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[6/7] ECOOT All (ecot): MSE=1.5672, Pearson=0.6811\n",
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 148/2000:   7%|▋         | 148/2000 [00:07<01:33, 19.81it/s, v_num=90, train_loss_epoch=0.803]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 0.558. Signaling Trainer to stop.\n",
            "[7/7] Fused GW (fgw): MSE=1.2714, Pearson=0.7554\n",
            "\n",
            "Summary Table of Prediction Metrics for fold2\n",
            "   Pearson_corr  Spearman_corr  Pearson_samples  Spearman_samples       MSE  \\\n",
            "0      0.773815       0.747606         0.434367          0.414779  1.160966   \n",
            "1      0.674191       0.663938        -0.018014         -0.011572  1.598013   \n",
            "2      0.727377       0.703116         0.318236          0.309425  1.389794   \n",
            "3      0.692897       0.675110         0.323306          0.318581  1.602633   \n",
            "4      0.394530       0.437790        -0.116077         -0.117253  3.376834   \n",
            "5      0.681096       0.666093        -0.009852         -0.009338  1.567201   \n",
            "6      0.755375       0.728088         0.395436          0.379626  1.271363   \n",
            "\n",
            "                     method                                               time  \n",
            "0      EGWOT Labeled (legw)  {'n_iters_outer': 5, 'converged_inner': True, ...  \n",
            "1           EGWOT All (egw)  {'n_iters_outer': 5, 'converged_inner': True, ...  \n",
            "2  EGWOT Per Label (egwper)  {0: {'n_iters_outer': 5, 'converged_inner': Tr...  \n",
            "3       COOT Labeled (cotl)  {'cost': [25.496164325207204, 19.2461648977921...  \n",
            "4            COOT All (cot)  {'cost': [2.551627705986266, 1.763847148206750...  \n",
            "5          ECOOT All (ecot)  {'cost': [4.724306080610469, 4.724287346152258...  \n",
            "6            Fused GW (fgw)                       {'time': 12.771081686019897}  \n",
            "\n",
            " Evaluation of the prediction models\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 162/2000:   8%|▊         | 162/2000 [00:08<01:37, 18.85it/s, v_num=91, train_loss_epoch=1.05]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 0.741. Signaling Trainer to stop.\n",
            "[1/7] EGWOT Labeled (legw): MSE=1.1471, Pearson=0.7689\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 136/2000:   7%|▋         | 136/2000 [00:06<01:29, 20.73it/s, v_num=92, train_loss_epoch=1.51]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 0.995. Signaling Trainer to stop.\n",
            "[2/7] EGWOT All (egw): MSE=1.4975, Pearson=0.6914\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 115/2000:   6%|▌         | 115/2000 [00:06<01:44, 18.05it/s, v_num=93, train_loss_epoch=1.61]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 0.872. Signaling Trainer to stop.\n",
            "[3/7] EGWOT Per Label (egwper): MSE=1.3501, Pearson=0.7295\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 173/2000:   9%|▊         | 173/2000 [00:08<01:34, 19.35it/s, v_num=94, train_loss_epoch=0.506]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 0.816. Signaling Trainer to stop.\n",
            "[4/7] COOT Labeled (cotl): MSE=1.3419, Pearson=0.7362\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 251/2000:  13%|█▎        | 251/2000 [00:12<01:28, 19.84it/s, v_num=95, train_loss_epoch=0.172]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 0.283. Signaling Trainer to stop.\n",
            "[5/7] COOT All (cot): MSE=2.5360, Pearson=0.5439\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 176/2000:   9%|▉         | 176/2000 [00:08<01:28, 20.57it/s, v_num=96, train_loss_epoch=1.61]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 0.900. Signaling Trainer to stop.\n",
            "[6/7] ECOOT All (ecot): MSE=1.4878, Pearson=0.6952\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 246/2000:  12%|█▏        | 246/2000 [00:12<01:28, 19.79it/s, v_num=97, train_loss_epoch=0.755]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 0.604. Signaling Trainer to stop.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[7/7] Fused GW (fgw): MSE=1.2285, Pearson=0.7597\n",
            "\n",
            "Summary Table of Prediction Metrics for fold3\n",
            "   Pearson_corr  Spearman_corr  Pearson_samples  Spearman_samples       MSE  \\\n",
            "0      0.768944       0.748035         0.413426          0.384563  1.147146   \n",
            "1      0.691398       0.679952        -0.003563         -0.008798  1.497462   \n",
            "2      0.729464       0.711522         0.260704          0.246269  1.350096   \n",
            "3      0.736159       0.715597         0.382388          0.367790  1.341881   \n",
            "4      0.543881       0.545694         0.071101          0.062603  2.536006   \n",
            "5      0.695229       0.687307         0.017760          0.023164  1.487841   \n",
            "6      0.759720       0.739254         0.406498          0.389093  1.228467   \n",
            "\n",
            "                     method                                               time  \n",
            "0      EGWOT Labeled (legw)  {'n_iters_outer': 5, 'converged_inner': True, ...  \n",
            "1           EGWOT All (egw)  {'n_iters_outer': 5, 'converged_inner': True, ...  \n",
            "2  EGWOT Per Label (egwper)  {0: {'n_iters_outer': 5, 'converged_inner': Tr...  \n",
            "3       COOT Labeled (cotl)  {'cost': [25.80475600489607, 18.98412235618036...  \n",
            "4            COOT All (cot)  {'cost': [2.567998266845097, 1.738253129901257...  \n",
            "5          ECOOT All (ecot)  {'cost': [4.721774264435604, 4.721754637752527...  \n",
            "6            Fused GW (fgw)                       {'time': 16.515815258026123}  \n",
            "\n",
            " Evaluation of the prediction models\n",
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 136/2000:   7%|▋         | 136/2000 [00:07<01:36, 19.27it/s, v_num=98, train_loss_epoch=0.987]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 0.630. Signaling Trainer to stop.\n",
            "[1/7] EGWOT Labeled (legw): MSE=1.0767, Pearson=0.7962\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 132/2000:   7%|▋         | 132/2000 [00:06<01:38, 18.93it/s, v_num=99, train_loss_epoch=1.59]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 1.013. Signaling Trainer to stop.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2/7] EGWOT All (egw): MSE=1.6037, Pearson=0.6817\n",
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 121/2000:   6%|▌         | 121/2000 [00:05<01:32, 20.21it/s, v_num=100, train_loss_epoch=1.51]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 1.012. Signaling Trainer to stop.\n",
            "[3/7] EGWOT Per Label (egwper): MSE=1.4029, Pearson=0.7315\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 189/2000:   9%|▉         | 189/2000 [00:09<01:32, 19.53it/s, v_num=101, train_loss_epoch=0.479]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 0.728. Signaling Trainer to stop.\n",
            "[4/7] COOT Labeled (cotl): MSE=1.3765, Pearson=0.7355\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 285/2000:  14%|█▍        | 285/2000 [00:14<01:27, 19.59it/s, v_num=102, train_loss_epoch=0.153]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 0.301. Signaling Trainer to stop.\n",
            "[5/7] COOT All (cot): MSE=3.2036, Pearson=0.4307\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 174/2000:   9%|▊         | 174/2000 [00:09<01:35, 19.08it/s, v_num=103, train_loss_epoch=1.52]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 1.013. Signaling Trainer to stop.\n",
            "[6/7] ECOOT All (ecot): MSE=1.5845, Pearson=0.6847\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: GPU available: True (cuda), used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "INFO: \n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | model        | Sequential | 15.5 K\n",
            "  | other params | n/a        | 200   \n",
            "--------------------------------------------\n",
            "15.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 K    Total params\n",
            "0.063     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mINFO    \u001b[0m Running sanity check on val set\u001b[33m...\u001b[0m                                                                        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 228/2000:  11%|█▏        | 228/2000 [00:12<01:36, 18.37it/s, v_num=104, train_loss_epoch=0.734]\n",
            "Monitored metric val_loss did not improve in the last 45 records. Best score: 0.540. Signaling Trainer to stop.\n",
            "[7/7] Fused GW (fgw): MSE=1.1060, Pearson=0.7885\n",
            "\n",
            "Summary Table of Prediction Metrics for fold4\n",
            "   Pearson_corr  Spearman_corr  Pearson_samples  Spearman_samples       MSE  \\\n",
            "0      0.796234       0.774503         0.468296          0.457856  1.076708   \n",
            "1      0.681674       0.665929        -0.030459         -0.030090  1.603715   \n",
            "2      0.731520       0.707762         0.305637          0.298811  1.402888   \n",
            "3      0.735495       0.710584         0.354215          0.349575  1.376545   \n",
            "4      0.430682       0.456679        -0.084386         -0.080491  3.203643   \n",
            "5      0.684730       0.673178         0.051322          0.044923  1.584544   \n",
            "6      0.788456       0.761155         0.466546          0.450340  1.105953   \n",
            "\n",
            "                     method                                               time  \n",
            "0      EGWOT Labeled (legw)  {'n_iters_outer': 5, 'converged_inner': True, ...  \n",
            "1           EGWOT All (egw)  {'n_iters_outer': 5, 'converged_inner': True, ...  \n",
            "2  EGWOT Per Label (egwper)  {0: {'n_iters_outer': 5, 'converged_inner': Tr...  \n",
            "3       COOT Labeled (cotl)  {'cost': [25.503288804432568, 18.7222194804155...  \n",
            "4            COOT All (cot)  {'cost': [2.538128166846278, 1.831589855143849...  \n",
            "5          ECOOT All (ecot)  {'cost': [4.718107620813187, 4.718096923564392...  \n",
            "6            Fused GW (fgw)                        {'time': 67.11971521377563}  \n",
            "[   Pearson_corr  Spearman_corr  Pearson_samples  Spearman_samples       MSE  \\\n",
            "0      0.800622       0.776271         0.509766          0.493502  1.027143   \n",
            "1      0.684694       0.671555         0.079354          0.077061  1.563907   \n",
            "2      0.710925       0.694591         0.262618          0.252481  1.457340   \n",
            "3      0.753402       0.729243         0.430479          0.415664  1.281006   \n",
            "4      0.413148       0.451287        -0.151932         -0.146565  3.342661   \n",
            "5      0.672182       0.663124        -0.004590         -0.006229  1.609624   \n",
            "6      0.774815       0.751191         0.446359          0.430379  1.144098   \n",
            "\n",
            "                     method                                               time  \n",
            "0      EGWOT Labeled (legw)  {'n_iters_outer': 5, 'converged_inner': True, ...  \n",
            "1           EGWOT All (egw)  {'n_iters_outer': 5, 'converged_inner': True, ...  \n",
            "2  EGWOT Per Label (egwper)  {0: {'n_iters_outer': 5, 'converged_inner': Tr...  \n",
            "3       COOT Labeled (cotl)  {'cost': [25.67649195831666, 19.99712314430541...  \n",
            "4            COOT All (cot)  {'cost': [2.49764674096519, 1.6041744980884043...  \n",
            "5          ECOOT All (ecot)  {'cost': [4.698151771105321, 4.698133917433545...  \n",
            "6            Fused GW (fgw)                       {'time': 23.932862520217896}  ,    Pearson_corr  Spearman_corr  Pearson_samples  Spearman_samples       MSE  \\\n",
            "0      0.780305       0.757490         0.454507          0.412807  1.023418   \n",
            "1      0.698431       0.684333         0.023662          0.025860  1.416657   \n",
            "2      0.726289       0.705541         0.243902          0.225655  1.307737   \n",
            "3      0.753270       0.735876         0.440067          0.425613  1.213834   \n",
            "4      0.449900       0.484402        -0.053044         -0.055300  2.942125   \n",
            "5      0.696604       0.683544         0.028240          0.026618  1.422915   \n",
            "6      0.746741       0.728712         0.397436          0.361577  1.177309   \n",
            "\n",
            "                     method                                               time  \n",
            "0      EGWOT Labeled (legw)  {'n_iters_outer': 5, 'converged_inner': True, ...  \n",
            "1           EGWOT All (egw)  {'n_iters_outer': 5, 'converged_inner': True, ...  \n",
            "2  EGWOT Per Label (egwper)  {0: {'n_iters_outer': 5, 'converged_inner': Tr...  \n",
            "3       COOT Labeled (cotl)  {'cost': [26.373504429201724, 20.4439884160194...  \n",
            "4            COOT All (cot)  {'cost': [2.5847412651707047, 1.75175366340454...  \n",
            "5          ECOOT All (ecot)  {'cost': [4.795991302579692, 4.795974513409747...  \n",
            "6            Fused GW (fgw)                       {'time': 36.745893716812134}  ,    Pearson_corr  Spearman_corr  Pearson_samples  Spearman_samples       MSE  \\\n",
            "0      0.773815       0.747606         0.434367          0.414779  1.160966   \n",
            "1      0.674191       0.663938        -0.018014         -0.011572  1.598013   \n",
            "2      0.727377       0.703116         0.318236          0.309425  1.389794   \n",
            "3      0.692897       0.675110         0.323306          0.318581  1.602633   \n",
            "4      0.394530       0.437790        -0.116077         -0.117253  3.376834   \n",
            "5      0.681096       0.666093        -0.009852         -0.009338  1.567201   \n",
            "6      0.755375       0.728088         0.395436          0.379626  1.271363   \n",
            "\n",
            "                     method                                               time  \n",
            "0      EGWOT Labeled (legw)  {'n_iters_outer': 5, 'converged_inner': True, ...  \n",
            "1           EGWOT All (egw)  {'n_iters_outer': 5, 'converged_inner': True, ...  \n",
            "2  EGWOT Per Label (egwper)  {0: {'n_iters_outer': 5, 'converged_inner': Tr...  \n",
            "3       COOT Labeled (cotl)  {'cost': [25.496164325207204, 19.2461648977921...  \n",
            "4            COOT All (cot)  {'cost': [2.551627705986266, 1.763847148206750...  \n",
            "5          ECOOT All (ecot)  {'cost': [4.724306080610469, 4.724287346152258...  \n",
            "6            Fused GW (fgw)                       {'time': 12.771081686019897}  ,    Pearson_corr  Spearman_corr  Pearson_samples  Spearman_samples       MSE  \\\n",
            "0      0.768944       0.748035         0.413426          0.384563  1.147146   \n",
            "1      0.691398       0.679952        -0.003563         -0.008798  1.497462   \n",
            "2      0.729464       0.711522         0.260704          0.246269  1.350096   \n",
            "3      0.736159       0.715597         0.382388          0.367790  1.341881   \n",
            "4      0.543881       0.545694         0.071101          0.062603  2.536006   \n",
            "5      0.695229       0.687307         0.017760          0.023164  1.487841   \n",
            "6      0.759720       0.739254         0.406498          0.389093  1.228467   \n",
            "\n",
            "                     method                                               time  \n",
            "0      EGWOT Labeled (legw)  {'n_iters_outer': 5, 'converged_inner': True, ...  \n",
            "1           EGWOT All (egw)  {'n_iters_outer': 5, 'converged_inner': True, ...  \n",
            "2  EGWOT Per Label (egwper)  {0: {'n_iters_outer': 5, 'converged_inner': Tr...  \n",
            "3       COOT Labeled (cotl)  {'cost': [25.80475600489607, 18.98412235618036...  \n",
            "4            COOT All (cot)  {'cost': [2.567998266845097, 1.738253129901257...  \n",
            "5          ECOOT All (ecot)  {'cost': [4.721774264435604, 4.721754637752527...  \n",
            "6            Fused GW (fgw)                       {'time': 16.515815258026123}  ,    Pearson_corr  Spearman_corr  Pearson_samples  Spearman_samples       MSE  \\\n",
            "0      0.796234       0.774503         0.468296          0.457856  1.076708   \n",
            "1      0.681674       0.665929        -0.030459         -0.030090  1.603715   \n",
            "2      0.731520       0.707762         0.305637          0.298811  1.402888   \n",
            "3      0.735495       0.710584         0.354215          0.349575  1.376545   \n",
            "4      0.430682       0.456679        -0.084386         -0.080491  3.203643   \n",
            "5      0.684730       0.673178         0.051322          0.044923  1.584544   \n",
            "6      0.788456       0.761155         0.466546          0.450340  1.105953   \n",
            "\n",
            "                     method                                               time  \n",
            "0      EGWOT Labeled (legw)  {'n_iters_outer': 5, 'converged_inner': True, ...  \n",
            "1           EGWOT All (egw)  {'n_iters_outer': 5, 'converged_inner': True, ...  \n",
            "2  EGWOT Per Label (egwper)  {0: {'n_iters_outer': 5, 'converged_inner': Tr...  \n",
            "3       COOT Labeled (cotl)  {'cost': [25.503288804432568, 18.7222194804155...  \n",
            "4            COOT All (cot)  {'cost': [2.538128166846278, 1.831589855143849...  \n",
            "5          ECOOT All (ecot)  {'cost': [4.718107620813187, 4.718096923564392...  \n",
            "6            Fused GW (fgw)                        {'time': 67.11971521377563}  ]\n"
          ]
        }
      ],
      "source": [
        "#We train the MLP models for each coupling and evaluate their performance on the test set compared with the Ground Truth\n",
        "folds_results=[]\n",
        "for k in range(len(folds)):\n",
        "\n",
        "  couplings = [legw, egw, egwper, cotl, cot, ecot, fgw]\n",
        "  coupling_names = [\n",
        "      \"EGWOT Labeled (legw)\",\n",
        "      \"EGWOT All (egw)\",\n",
        "      \"EGWOT Per Label (egwper)\",\n",
        "      \"COOT Labeled (cotl)\",\n",
        "      \"COOT All (cot)\",\n",
        "      \"ECOOT All (ecot)\",\n",
        "      \"Fused GW (fgw)\"\n",
        "  ]\n",
        "\n",
        "  data_train = folds[k][\"data_train\"]\n",
        "  Y_test_concatenated = folds[k][\"Y_test_concat\"]\n",
        "  X_test_t = folds[k][\"X_test_torch\"]\n",
        "  results_pred = []\n",
        "\n",
        "  print(\"\\n Evaluation of the prediction models\")\n",
        "\n",
        "  for i, coupling in enumerate(couplings):\n",
        "      coupling_name = coupling_names[i]\n",
        "      coupling_for_train = coupling[k][0]\n",
        "      time_for_train = coupling[k][1]\n",
        "      model, pred_log = perturbot.predict.train_mlp(data_train, coupling_for_train)\n",
        "\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "          Y_pred = model(X_test_t).cpu().numpy().astype(np.float64)\n",
        "\n",
        "\n",
        "      Y_test_concatenated = Y_test_concatenated.astype(np.float64)\n",
        "      metrics_df_pred= get_evals_preds(\n",
        "          Y_test_concatenated,  # Y_true (Grund Truth)\n",
        "          [Y_pred],\n",
        "          pred_labels=[coupling_name],\n",
        "          full=False\n",
        "\n",
        "      )\n",
        "\n",
        "      metrics_dict = metrics_df_pred[coupling_name].to_dict()\n",
        "      metrics_dict[\"method\"] = coupling_name\n",
        "      metrics_dict[\"time\"] = time_for_train\n",
        "      results_pred.append(metrics_dict)\n",
        "\n",
        "\n",
        "      print(f\"[{i+1}/{len(couplings)}] {coupling_name}: \"\n",
        "            f\"MSE={metrics_dict.get('MSE', np.nan):.4f}, \"\n",
        "            f\"Pearson={metrics_dict.get('Pearson_corr', np.nan):.4f}\")\n",
        "\n",
        "\n",
        "  results_df = pd.DataFrame(results_pred)\n",
        "  print(f\"\\nSummary Table of Prediction Metrics for fold{k}\")\n",
        "  print(results_df)\n",
        "  folds_results.append(results_df)\n",
        "print(folds_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "vRa1wiCcVCjW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "vRa1wiCcVCjW",
        "outputId": "af3df61e-449e-44d5-e020-dbc2b22e6a65"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"summary_df\",\n  \"rows\": 7,\n  \"fields\": [\n    {\n      \"column\": \"method\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"COOT All (cot)\",\n          \"COOT Labeled (cotl)\",\n          \"EGWOT Per Label (egwper)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pearson_corr\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"0.446 \\u00b1 0.026\",\n          \"0.734 \\u00b1 0.011\",\n          \"0.725 \\u00b1 0.004\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Spearman_corr\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"0.475 \\u00b1 0.019\",\n          \"0.713 \\u00b1 0.011\",\n          \"0.705 \\u00b1 0.003\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pearson_samples\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"-0.067 \\u00b1 0.038\",\n          \"0.386 \\u00b1 0.022\",\n          \"0.278 \\u00b1 0.014\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Spearman_samples\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"-0.067 \\u00b1 0.036\",\n          \"0.375 \\u00b1 0.02\",\n          \"0.267 \\u00b1 0.016\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MSE\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"3.08 \\u00b1 0.156\",\n          \"1.363 \\u00b1 0.066\",\n          \"1.382 \\u00b1 0.025\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "summary_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-b05699f8-549f-4c4f-8246-5e785c917ebf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pearson_corr</th>\n",
              "      <th>Spearman_corr</th>\n",
              "      <th>Pearson_samples</th>\n",
              "      <th>Spearman_samples</th>\n",
              "      <th>MSE</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>method</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>COOT All (cot)</th>\n",
              "      <td>0.446 ± 0.026</td>\n",
              "      <td>0.475 ± 0.019</td>\n",
              "      <td>-0.067 ± 0.038</td>\n",
              "      <td>-0.067 ± 0.036</td>\n",
              "      <td>3.08 ± 0.156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>COOT Labeled (cotl)</th>\n",
              "      <td>0.734 ± 0.011</td>\n",
              "      <td>0.713 ± 0.011</td>\n",
              "      <td>0.386 ± 0.022</td>\n",
              "      <td>0.375 ± 0.02</td>\n",
              "      <td>1.363 ± 0.066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ECOOT All (ecot)</th>\n",
              "      <td>0.686 ± 0.005</td>\n",
              "      <td>0.675 ± 0.005</td>\n",
              "      <td>0.017 ± 0.011</td>\n",
              "      <td>0.016 ± 0.01</td>\n",
              "      <td>1.534 ± 0.035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>EGWOT All (egw)</th>\n",
              "      <td>0.686 ± 0.004</td>\n",
              "      <td>0.673 ± 0.004</td>\n",
              "      <td>0.01 ± 0.019</td>\n",
              "      <td>0.01 ± 0.019</td>\n",
              "      <td>1.536 ± 0.035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>EGWOT Labeled (legw)</th>\n",
              "      <td>0.784 ± 0.006</td>\n",
              "      <td>0.761 ± 0.006</td>\n",
              "      <td>0.456 ± 0.016</td>\n",
              "      <td>0.433 ± 0.019</td>\n",
              "      <td>1.087 ± 0.029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>EGWOT Per Label (egwper)</th>\n",
              "      <td>0.725 ± 0.004</td>\n",
              "      <td>0.705 ± 0.003</td>\n",
              "      <td>0.278 ± 0.014</td>\n",
              "      <td>0.267 ± 0.016</td>\n",
              "      <td>1.382 ± 0.025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Fused GW (fgw)</th>\n",
              "      <td>0.765 ± 0.007</td>\n",
              "      <td>0.742 ± 0.006</td>\n",
              "      <td>0.422 ± 0.014</td>\n",
              "      <td>0.402 ± 0.016</td>\n",
              "      <td>1.185 ± 0.029</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b05699f8-549f-4c4f-8246-5e785c917ebf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b05699f8-549f-4c4f-8246-5e785c917ebf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b05699f8-549f-4c4f-8246-5e785c917ebf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-81d3146d-de98-42e7-a666-1ba4e98a385e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-81d3146d-de98-42e7-a666-1ba4e98a385e')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-81d3146d-de98-42e7-a666-1ba4e98a385e button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_42b003cc-a166-4527-8066-e1bc704d960e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('summary_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_42b003cc-a166-4527-8066-e1bc704d960e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('summary_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                           Pearson_corr  Spearman_corr Pearson_samples  \\\n",
              "method                                                                   \n",
              "COOT All (cot)            0.446 ± 0.026  0.475 ± 0.019  -0.067 ± 0.038   \n",
              "COOT Labeled (cotl)       0.734 ± 0.011  0.713 ± 0.011   0.386 ± 0.022   \n",
              "ECOOT All (ecot)          0.686 ± 0.005  0.675 ± 0.005   0.017 ± 0.011   \n",
              "EGWOT All (egw)           0.686 ± 0.004  0.673 ± 0.004    0.01 ± 0.019   \n",
              "EGWOT Labeled (legw)      0.784 ± 0.006  0.761 ± 0.006   0.456 ± 0.016   \n",
              "EGWOT Per Label (egwper)  0.725 ± 0.004  0.705 ± 0.003   0.278 ± 0.014   \n",
              "Fused GW (fgw)            0.765 ± 0.007  0.742 ± 0.006   0.422 ± 0.014   \n",
              "\n",
              "                         Spearman_samples            MSE  \n",
              "method                                                    \n",
              "COOT All (cot)             -0.067 ± 0.036   3.08 ± 0.156  \n",
              "COOT Labeled (cotl)          0.375 ± 0.02  1.363 ± 0.066  \n",
              "ECOOT All (ecot)             0.016 ± 0.01  1.534 ± 0.035  \n",
              "EGWOT All (egw)              0.01 ± 0.019  1.536 ± 0.035  \n",
              "EGWOT Labeled (legw)        0.433 ± 0.019  1.087 ± 0.029  \n",
              "EGWOT Per Label (egwper)    0.267 ± 0.016  1.382 ± 0.025  \n",
              "Fused GW (fgw)              0.402 ± 0.016  1.185 ± 0.029  "
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics = ['Pearson_corr', 'Spearman_corr', 'Pearson_samples', 'Spearman_samples', 'MSE']\n",
        "all_data = pd.concat(folds_results, keys=range(len(folds_results)), names=['fold', 'index'])\n",
        "mean_df = all_data.groupby('method')[metrics].mean()\n",
        "std_df = all_data.groupby('method')[metrics].std(ddof=1)\n",
        "n_folds = len(folds_results)\n",
        "sem_df = std_df / np.sqrt(n_folds)\n",
        "summary_df = mean_df.copy()\n",
        "for col in metrics:\n",
        "    summary_df[col] = mean_df[col].round(3).astype(str) + ' ± ' + sem_df[col].round(3).astype(str)\n",
        "\n",
        "summary_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "ca188e8c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca188e8c",
        "outputId": "5d36d0c1-6712-487c-ccbc-1dd1ba4d8d4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation of coupling metrics on the training data\n",
            "\n",
            "Summary Table of Coupling Metrics (Training Set)\n",
            "                 method  Bary_FOSCTTM  Mean_Bary_FOSCTTM\n",
            "0  EGWOT Labeled (legw)      0.244480           0.244480\n",
            "1   COOT Labeled (cotl)      0.306980           0.306980\n",
            "2    EGWOT Global (egw)      0.488722           0.489098\n",
            "3     COOT Global (cot)      0.609023           0.577556\n",
            "4   ECOOT Global (ecot)      0.500000           0.500069\n",
            "5        Fused GW (fgw)      0.154135           0.247143\n",
            "Evaluation of coupling metrics on the training data\n",
            "\n",
            "Summary Table of Coupling Metrics (Training Set)\n",
            "                 method  Bary_FOSCTTM  Mean_Bary_FOSCTTM\n",
            "0  EGWOT Labeled (legw)      0.238108           0.238108\n",
            "1   COOT Labeled (cotl)      0.280833           0.280833\n",
            "2    EGWOT Global (egw)      0.496241           0.497632\n",
            "3     COOT Global (cot)      0.508772           0.519524\n",
            "4   ECOOT Global (ecot)      0.500000           0.500031\n",
            "5        Fused GW (fgw)      0.166667           0.243170\n",
            "Evaluation of coupling metrics on the training data\n",
            "\n",
            "Summary Table of Coupling Metrics (Training Set)\n",
            "                 method  Bary_FOSCTTM  Mean_Bary_FOSCTTM\n",
            "0  EGWOT Labeled (legw)      0.246748           0.246748\n",
            "1   COOT Labeled (cotl)      0.297945           0.297945\n",
            "2    EGWOT Global (egw)      0.496241           0.501028\n",
            "3     COOT Global (cot)      0.593985           0.555395\n",
            "4   ECOOT Global (ecot)      0.500000           0.500125\n",
            "5        Fused GW (fgw)      0.171679           0.248553\n",
            "Evaluation of coupling metrics on the training data\n",
            "\n",
            "Summary Table of Coupling Metrics (Training Set)\n",
            "                 method  Bary_FOSCTTM  Mean_Bary_FOSCTTM\n",
            "0  EGWOT Labeled (legw)      0.244330           0.244330\n",
            "1   COOT Labeled (cotl)      0.286642           0.286642\n",
            "2    EGWOT Global (egw)      0.507519           0.501422\n",
            "3     COOT Global (cot)      0.394737           0.417500\n",
            "4   ECOOT Global (ecot)      0.500000           0.500188\n",
            "5        Fused GW (fgw)      0.161654           0.236541\n",
            "Evaluation of coupling metrics on the training data\n",
            "\n",
            "Summary Table of Coupling Metrics (Training Set)\n",
            "                 method  Bary_FOSCTTM  Mean_Bary_FOSCTTM\n",
            "0  EGWOT Labeled (legw)      0.268321           0.268321\n",
            "1   COOT Labeled (cotl)      0.299436           0.299436\n",
            "2    EGWOT Global (egw)      0.500000           0.498346\n",
            "3     COOT Global (cot)      0.571429           0.552299\n",
            "4   ECOOT Global (ecot)      0.500000           0.500075\n",
            "5        Fused GW (fgw)      0.184211           0.267920\n",
            "[                 method  Bary_FOSCTTM  Mean_Bary_FOSCTTM\n",
            "0  EGWOT Labeled (legw)      0.244480           0.244480\n",
            "1   COOT Labeled (cotl)      0.306980           0.306980\n",
            "2    EGWOT Global (egw)      0.488722           0.489098\n",
            "3     COOT Global (cot)      0.609023           0.577556\n",
            "4   ECOOT Global (ecot)      0.500000           0.500069\n",
            "5        Fused GW (fgw)      0.154135           0.247143,                  method  Bary_FOSCTTM  Mean_Bary_FOSCTTM\n",
            "0  EGWOT Labeled (legw)      0.238108           0.238108\n",
            "1   COOT Labeled (cotl)      0.280833           0.280833\n",
            "2    EGWOT Global (egw)      0.496241           0.497632\n",
            "3     COOT Global (cot)      0.508772           0.519524\n",
            "4   ECOOT Global (ecot)      0.500000           0.500031\n",
            "5        Fused GW (fgw)      0.166667           0.243170,                  method  Bary_FOSCTTM  Mean_Bary_FOSCTTM\n",
            "0  EGWOT Labeled (legw)      0.246748           0.246748\n",
            "1   COOT Labeled (cotl)      0.297945           0.297945\n",
            "2    EGWOT Global (egw)      0.496241           0.501028\n",
            "3     COOT Global (cot)      0.593985           0.555395\n",
            "4   ECOOT Global (ecot)      0.500000           0.500125\n",
            "5        Fused GW (fgw)      0.171679           0.248553,                  method  Bary_FOSCTTM  Mean_Bary_FOSCTTM\n",
            "0  EGWOT Labeled (legw)      0.244330           0.244330\n",
            "1   COOT Labeled (cotl)      0.286642           0.286642\n",
            "2    EGWOT Global (egw)      0.507519           0.501422\n",
            "3     COOT Global (cot)      0.394737           0.417500\n",
            "4   ECOOT Global (ecot)      0.500000           0.500188\n",
            "5        Fused GW (fgw)      0.161654           0.236541,                  method  Bary_FOSCTTM  Mean_Bary_FOSCTTM\n",
            "0  EGWOT Labeled (legw)      0.268321           0.268321\n",
            "1   COOT Labeled (cotl)      0.299436           0.299436\n",
            "2    EGWOT Global (egw)      0.500000           0.498346\n",
            "3     COOT Global (cot)      0.571429           0.552299\n",
            "4   ECOOT Global (ecot)      0.500000           0.500075\n",
            "5        Fused GW (fgw)      0.184211           0.267920]\n"
          ]
        }
      ],
      "source": [
        "folds_results_match=[]\n",
        "for k in range(len(folds)):\n",
        "\n",
        "    print(\"Evaluation of coupling metrics on the training data\")\n",
        "    #Metrics evaluation on the training set\n",
        "    data_train = folds[k][\"data_train\"]\n",
        "    Xs_train_dict = data_train[0]\n",
        "    Xt_train_dict = data_train[1]\n",
        "\n",
        "    labeled_couplings = [legw[k], cotl[k]]\n",
        "    labeled_coupling_names = [\n",
        "        \"EGWOT Labeled (legw)\",\n",
        "        \"COOT Labeled (cotl)\"\n",
        "    ]\n",
        "    global_couplings = [egw[k], cot[k], ecot[k], fgw[k]]\n",
        "    global_coupling_names = [\n",
        "        \"EGWOT Global (egw)\",\n",
        "        \"COOT Global (cot)\",\n",
        "        \"ECOOT Global (ecot)\",\n",
        "        \"Fused GW (fgw)\"\n",
        "    ]\n",
        "\n",
        "    results_match = []\n",
        "    #Labeled couplings\n",
        "    for i, coupling in enumerate(labeled_couplings):\n",
        "        coupling_name = labeled_coupling_names[i]\n",
        "        T_dict = coupling[0]\n",
        "\n",
        "        #Bary FSTTM (Barycentre FOSCTTM: use_barycenter=True)\n",
        "        foscttm_bary_list, median_foscttm_bary = get_FOSCTTM(\n",
        "            T_dict=T_dict, Xs_dict=Xs_train_dict, Xt_dict=Xt_train_dict, use_barycenter=True\n",
        "        )\n",
        "\n",
        "        all_foscttm_bary = foscttm_bary_list\n",
        "        mean_foscttm_bary = np.nanmean(all_foscttm_bary)\n",
        "\n",
        "        results_match.append({\n",
        "            \"method\": coupling_name,\n",
        "            \"Bary_FOSCTTM\": median_foscttm_bary,\n",
        "            \"Mean_Bary_FOSCTTM\": mean_foscttm_bary,\n",
        "        })\n",
        "\n",
        "    #Global couplings\n",
        "    for i, coupling in enumerate(global_couplings):\n",
        "        coupling_name = global_coupling_names[i]\n",
        "        T = coupling[0]\n",
        "\n",
        "        foscttm_bary_list, median_foscttm_bary = get_FOSCTTM_single(\n",
        "            T=T, Xs_dict=Xs_train_dict, Xt_dict=Xt_train_dict, use_barycenter=True\n",
        "        )\n",
        "\n",
        "        all_foscttm_bary = foscttm_bary_list\n",
        "        mean_foscttm_bary = np.nanmean(all_foscttm_bary)\n",
        "\n",
        "        results_match.append({\n",
        "            \"method\": coupling_name,\n",
        "            \"Bary_FOSCTTM\": median_foscttm_bary,\n",
        "\n",
        "            \"Mean_Bary_FOSCTTM\": mean_foscttm_bary,\n",
        "\n",
        "        })\n",
        "\n",
        "    results_match_df = pd.DataFrame(results_match)\n",
        "    print(\"\\nSummary Table of Coupling Metrics (Training Set)\")\n",
        "    print(results_match_df)\n",
        "    folds_results_match.append(results_match_df)\n",
        "print(folds_results_match)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "OXuz0PEb985V",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "OXuz0PEb985V",
        "outputId": "a5c7ad1d-0826-47ad-8d5a-7ba091b1a49f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"summary_df\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"method\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"COOT Global (cot)\",\n          \"COOT Labeled (cotl)\",\n          \"Fused GW (fgw)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Bary_FOSCTTM\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"0.536 \\u00b1 0.039\",\n          \"0.294 \\u00b1 0.005\",\n          \"0.168 \\u00b1 0.005\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Mean_Bary_FOSCTTM\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"0.524 \\u00b1 0.028\",\n          \"0.294 \\u00b1 0.005\",\n          \"0.249 \\u00b1 0.005\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "summary_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-91b46e13-5727-46b9-b8e0-ad7795ddfc86\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Bary_FOSCTTM</th>\n",
              "      <th>Mean_Bary_FOSCTTM</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>method</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>COOT Global (cot)</th>\n",
              "      <td>0.536 ± 0.039</td>\n",
              "      <td>0.524 ± 0.028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>COOT Labeled (cotl)</th>\n",
              "      <td>0.294 ± 0.005</td>\n",
              "      <td>0.294 ± 0.005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ECOOT Global (ecot)</th>\n",
              "      <td>0.5 ± 0.0</td>\n",
              "      <td>0.5 ± 0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>EGWOT Global (egw)</th>\n",
              "      <td>0.498 ± 0.003</td>\n",
              "      <td>0.498 ± 0.002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>EGWOT Labeled (legw)</th>\n",
              "      <td>0.248 ± 0.005</td>\n",
              "      <td>0.248 ± 0.005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Fused GW (fgw)</th>\n",
              "      <td>0.168 ± 0.005</td>\n",
              "      <td>0.249 ± 0.005</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-91b46e13-5727-46b9-b8e0-ad7795ddfc86')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-91b46e13-5727-46b9-b8e0-ad7795ddfc86 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-91b46e13-5727-46b9-b8e0-ad7795ddfc86');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e63da29e-66d9-455f-81ba-774629b9e9d6\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e63da29e-66d9-455f-81ba-774629b9e9d6')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e63da29e-66d9-455f-81ba-774629b9e9d6 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_ca3df0b9-e8f0-4a3d-91b7-df20d709a76b\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('summary_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_ca3df0b9-e8f0-4a3d-91b7-df20d709a76b button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('summary_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                       Bary_FOSCTTM Mean_Bary_FOSCTTM\n",
              "method                                               \n",
              "COOT Global (cot)     0.536 ± 0.039     0.524 ± 0.028\n",
              "COOT Labeled (cotl)   0.294 ± 0.005     0.294 ± 0.005\n",
              "ECOOT Global (ecot)       0.5 ± 0.0         0.5 ± 0.0\n",
              "EGWOT Global (egw)    0.498 ± 0.003     0.498 ± 0.002\n",
              "EGWOT Labeled (legw)  0.248 ± 0.005     0.248 ± 0.005\n",
              "Fused GW (fgw)        0.168 ± 0.005     0.249 ± 0.005"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics = ['Bary_FOSCTTM', 'Mean_Bary_FOSCTTM' ]\n",
        "all_data = pd.concat(folds_results_match, keys=range(len(folds_results_match)), names=['fold', 'index'])\n",
        "mean_df = all_data.groupby('method')[metrics].mean()\n",
        "std_df = all_data.groupby('method')[metrics].std(ddof=1)\n",
        "n_folds = len(folds_results_match)\n",
        "sem_df = std_df / np.sqrt(n_folds)\n",
        "summary_df = mean_df.copy()\n",
        "for col in metrics:\n",
        "    summary_df[col] = mean_df[col].round(3).astype(str) + ' ± ' + sem_df[col].round(3).astype(str)\n",
        "\n",
        "summary_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6177edcd",
      "metadata": {
        "id": "6177edcd"
      },
      "source": [
        "## **Computation times**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "9cc74564",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cc74564",
        "outputId": "23fb6b02-9fe7-4bbf-bf48-076e484da195"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ligne 0 - Clés : ['n_iters_outer', 'converged_inner', 'converged_outer', 'GW cost', 'time', 'cost_time']\n",
            "Ligne 1 - Clés : ['n_iters_outer', 'converged_inner', 'converged_outer', 'GW cost', 'time', 'cost_time']\n",
            "Ligne 2 - Clés : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Ligne 3 - Clés : ['cost', 'time']\n",
            "Ligne 4 - Clés : ['cost', 'time']\n",
            "Ligne 5 - Clés : ['cost', 'time']\n",
            "Ligne 6 - Clés : ['time']\n",
            "                     method  extracted_time\n",
            "0      EGWOT Labeled (legw)       11.299049\n",
            "1           EGWOT All (egw)        2.930561\n",
            "2  EGWOT Per Label (egwper)       26.871022\n",
            "3       COOT Labeled (cotl)        0.069252\n",
            "4            COOT All (cot)        1.505712\n",
            "5          ECOOT All (ecot)       18.682953\n",
            "6            Fused GW (fgw)       23.932863\n",
            "Ligne 0 - Clés : ['n_iters_outer', 'converged_inner', 'converged_outer', 'GW cost', 'time', 'cost_time']\n",
            "Ligne 1 - Clés : ['n_iters_outer', 'converged_inner', 'converged_outer', 'GW cost', 'time', 'cost_time']\n",
            "Ligne 2 - Clés : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Ligne 3 - Clés : ['cost', 'time']\n",
            "Ligne 4 - Clés : ['cost', 'time']\n",
            "Ligne 5 - Clés : ['cost', 'time']\n",
            "Ligne 6 - Clés : ['time']\n",
            "                     method  extracted_time\n",
            "0      EGWOT Labeled (legw)        3.371064\n",
            "1           EGWOT All (egw)        2.718793\n",
            "2  EGWOT Per Label (egwper)       25.583922\n",
            "3       COOT Labeled (cotl)        0.100843\n",
            "4            COOT All (cot)        0.560155\n",
            "5          ECOOT All (ecot)       14.219697\n",
            "6            Fused GW (fgw)       36.745894\n",
            "Ligne 0 - Clés : ['n_iters_outer', 'converged_inner', 'converged_outer', 'GW cost', 'time', 'cost_time']\n",
            "Ligne 1 - Clés : ['n_iters_outer', 'converged_inner', 'converged_outer', 'GW cost', 'time', 'cost_time']\n",
            "Ligne 2 - Clés : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Ligne 3 - Clés : ['cost', 'time']\n",
            "Ligne 4 - Clés : ['cost', 'time']\n",
            "Ligne 5 - Clés : ['cost', 'time']\n",
            "Ligne 6 - Clés : ['time']\n",
            "                     method  extracted_time\n",
            "0      EGWOT Labeled (legw)        3.621446\n",
            "1           EGWOT All (egw)        2.372936\n",
            "2  EGWOT Per Label (egwper)       23.012394\n",
            "3       COOT Labeled (cotl)        0.119515\n",
            "4            COOT All (cot)        0.857648\n",
            "5          ECOOT All (ecot)        9.993825\n",
            "6            Fused GW (fgw)       12.771082\n",
            "Ligne 0 - Clés : ['n_iters_outer', 'converged_inner', 'converged_outer', 'GW cost', 'time', 'cost_time']\n",
            "Ligne 1 - Clés : ['n_iters_outer', 'converged_inner', 'converged_outer', 'GW cost', 'time', 'cost_time']\n",
            "Ligne 2 - Clés : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Ligne 3 - Clés : ['cost', 'time']\n",
            "Ligne 4 - Clés : ['cost', 'time']\n",
            "Ligne 5 - Clés : ['cost', 'time']\n",
            "Ligne 6 - Clés : ['time']\n",
            "                     method  extracted_time\n",
            "0      EGWOT Labeled (legw)        3.594464\n",
            "1           EGWOT All (egw)        2.109577\n",
            "2  EGWOT Per Label (egwper)       21.656293\n",
            "3       COOT Labeled (cotl)        0.069019\n",
            "4            COOT All (cot)        0.380308\n",
            "5          ECOOT All (ecot)       14.306770\n",
            "6            Fused GW (fgw)       16.515815\n",
            "Ligne 0 - Clés : ['n_iters_outer', 'converged_inner', 'converged_outer', 'GW cost', 'time', 'cost_time']\n",
            "Ligne 1 - Clés : ['n_iters_outer', 'converged_inner', 'converged_outer', 'GW cost', 'time', 'cost_time']\n",
            "Ligne 2 - Clés : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Ligne 3 - Clés : ['cost', 'time']\n",
            "Ligne 4 - Clés : ['cost', 'time']\n",
            "Ligne 5 - Clés : ['cost', 'time']\n",
            "Ligne 6 - Clés : ['time']\n",
            "                     method  extracted_time\n",
            "0      EGWOT Labeled (legw)        3.383763\n",
            "1           EGWOT All (egw)        2.136042\n",
            "2  EGWOT Per Label (egwper)       22.916007\n",
            "3       COOT Labeled (cotl)        0.074637\n",
            "4            COOT All (cot)        1.481652\n",
            "5          ECOOT All (ecot)       13.973184\n",
            "6            Fused GW (fgw)       67.119715\n"
          ]
        }
      ],
      "source": [
        "# Time information for the couplings\n",
        "for k in range(len(folds)):\n",
        "    results_df=folds_results[k]\n",
        "\n",
        "    for index, log_dict in results_df['time'].items():\n",
        "        if isinstance(log_dict, dict):\n",
        "            print(f\"Ligne {index} - Clés : {list(log_dict.keys())}\")\n",
        "        else:\n",
        "            print(f\"Ligne {index} - Ce n'est pas un dictionnaire : {type(log_dict)}\")\n",
        "\n",
        "    def extract_total_time(log_dict):\n",
        "        if 'time' in log_dict:\n",
        "            return log_dict['time']\n",
        "        total_time = 0\n",
        "        found_time = False\n",
        "        for key, val in log_dict.items():\n",
        "            if isinstance(val, dict) and 'time' in val:\n",
        "                total_time += val['time']\n",
        "                found_time = True\n",
        "\n",
        "        if found_time:\n",
        "            return total_time\n",
        "\n",
        "        return None\n",
        "\n",
        "    results_df['extracted_time'] = results_df['time'].apply(extract_total_time)\n",
        "\n",
        "    print(results_df[['method', 'extracted_time']])\n",
        "    folds_results[k]=results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "X4nlhkda8ftQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "X4nlhkda8ftQ",
        "outputId": "934cc98c-240e-4f8f-b249-91915765dbc4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"summary_df\",\n  \"rows\": 7,\n  \"fields\": [\n    {\n      \"column\": \"method\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"COOT All (cot)\",\n          \"COOT Labeled (cotl)\",\n          \"EGWOT Per Label (egwper)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pearson_corr\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"0.446 \\u00b1 0.026\",\n          \"0.734 \\u00b1 0.011\",\n          \"0.725 \\u00b1 0.004\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Spearman_corr\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"0.475 \\u00b1 0.019\",\n          \"0.713 \\u00b1 0.011\",\n          \"0.705 \\u00b1 0.003\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pearson_samples\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"-0.067 \\u00b1 0.038\",\n          \"0.386 \\u00b1 0.022\",\n          \"0.278 \\u00b1 0.014\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Spearman_samples\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"-0.067 \\u00b1 0.036\",\n          \"0.375 \\u00b1 0.02\",\n          \"0.267 \\u00b1 0.016\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MSE\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"3.08 \\u00b1 0.156\",\n          \"1.363 \\u00b1 0.066\",\n          \"1.382 \\u00b1 0.025\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"extracted_time\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"0.957 \\u00b1 0.232\",\n          \"0.087 \\u00b1 0.01\",\n          \"24.008 \\u00b1 0.959\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "summary_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-f248143f-545e-4081-8e66-6bb910b60129\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pearson_corr</th>\n",
              "      <th>Spearman_corr</th>\n",
              "      <th>Pearson_samples</th>\n",
              "      <th>Spearman_samples</th>\n",
              "      <th>MSE</th>\n",
              "      <th>extracted_time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>method</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>COOT All (cot)</th>\n",
              "      <td>0.446 ± 0.026</td>\n",
              "      <td>0.475 ± 0.019</td>\n",
              "      <td>-0.067 ± 0.038</td>\n",
              "      <td>-0.067 ± 0.036</td>\n",
              "      <td>3.08 ± 0.156</td>\n",
              "      <td>0.957 ± 0.232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>COOT Labeled (cotl)</th>\n",
              "      <td>0.734 ± 0.011</td>\n",
              "      <td>0.713 ± 0.011</td>\n",
              "      <td>0.386 ± 0.022</td>\n",
              "      <td>0.375 ± 0.02</td>\n",
              "      <td>1.363 ± 0.066</td>\n",
              "      <td>0.087 ± 0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ECOOT All (ecot)</th>\n",
              "      <td>0.686 ± 0.005</td>\n",
              "      <td>0.675 ± 0.005</td>\n",
              "      <td>0.017 ± 0.011</td>\n",
              "      <td>0.016 ± 0.01</td>\n",
              "      <td>1.534 ± 0.035</td>\n",
              "      <td>14.235 ± 1.376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>EGWOT All (egw)</th>\n",
              "      <td>0.686 ± 0.004</td>\n",
              "      <td>0.673 ± 0.004</td>\n",
              "      <td>0.01 ± 0.019</td>\n",
              "      <td>0.01 ± 0.019</td>\n",
              "      <td>1.536 ± 0.035</td>\n",
              "      <td>2.454 ± 0.162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>EGWOT Labeled (legw)</th>\n",
              "      <td>0.784 ± 0.006</td>\n",
              "      <td>0.761 ± 0.006</td>\n",
              "      <td>0.456 ± 0.016</td>\n",
              "      <td>0.433 ± 0.019</td>\n",
              "      <td>1.087 ± 0.029</td>\n",
              "      <td>5.054 ± 1.562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>EGWOT Per Label (egwper)</th>\n",
              "      <td>0.725 ± 0.004</td>\n",
              "      <td>0.705 ± 0.003</td>\n",
              "      <td>0.278 ± 0.014</td>\n",
              "      <td>0.267 ± 0.016</td>\n",
              "      <td>1.382 ± 0.025</td>\n",
              "      <td>24.008 ± 0.959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Fused GW (fgw)</th>\n",
              "      <td>0.765 ± 0.007</td>\n",
              "      <td>0.742 ± 0.006</td>\n",
              "      <td>0.422 ± 0.014</td>\n",
              "      <td>0.402 ± 0.016</td>\n",
              "      <td>1.185 ± 0.029</td>\n",
              "      <td>31.417 ± 9.82</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f248143f-545e-4081-8e66-6bb910b60129')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f248143f-545e-4081-8e66-6bb910b60129 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f248143f-545e-4081-8e66-6bb910b60129');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f616f5c6-9a1d-4848-b302-fb619b8b534d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f616f5c6-9a1d-4848-b302-fb619b8b534d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f616f5c6-9a1d-4848-b302-fb619b8b534d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_ce391ffe-e623-47c0-88d8-148f8d0836bb\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('summary_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_ce391ffe-e623-47c0-88d8-148f8d0836bb button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('summary_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                           Pearson_corr  Spearman_corr Pearson_samples  \\\n",
              "method                                                                   \n",
              "COOT All (cot)            0.446 ± 0.026  0.475 ± 0.019  -0.067 ± 0.038   \n",
              "COOT Labeled (cotl)       0.734 ± 0.011  0.713 ± 0.011   0.386 ± 0.022   \n",
              "ECOOT All (ecot)          0.686 ± 0.005  0.675 ± 0.005   0.017 ± 0.011   \n",
              "EGWOT All (egw)           0.686 ± 0.004  0.673 ± 0.004    0.01 ± 0.019   \n",
              "EGWOT Labeled (legw)      0.784 ± 0.006  0.761 ± 0.006   0.456 ± 0.016   \n",
              "EGWOT Per Label (egwper)  0.725 ± 0.004  0.705 ± 0.003   0.278 ± 0.014   \n",
              "Fused GW (fgw)            0.765 ± 0.007  0.742 ± 0.006   0.422 ± 0.014   \n",
              "\n",
              "                         Spearman_samples            MSE  extracted_time  \n",
              "method                                                                    \n",
              "COOT All (cot)             -0.067 ± 0.036   3.08 ± 0.156   0.957 ± 0.232  \n",
              "COOT Labeled (cotl)          0.375 ± 0.02  1.363 ± 0.066    0.087 ± 0.01  \n",
              "ECOOT All (ecot)             0.016 ± 0.01  1.534 ± 0.035  14.235 ± 1.376  \n",
              "EGWOT All (egw)              0.01 ± 0.019  1.536 ± 0.035   2.454 ± 0.162  \n",
              "EGWOT Labeled (legw)        0.433 ± 0.019  1.087 ± 0.029   5.054 ± 1.562  \n",
              "EGWOT Per Label (egwper)    0.267 ± 0.016  1.382 ± 0.025  24.008 ± 0.959  \n",
              "Fused GW (fgw)              0.402 ± 0.016  1.185 ± 0.029   31.417 ± 9.82  "
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics = ['Pearson_corr', 'Spearman_corr', 'Pearson_samples', 'Spearman_samples', 'MSE', 'extracted_time']\n",
        "all_data = pd.concat(folds_results, keys=range(len(folds_results)), names=['fold', 'index'])\n",
        "mean_df = all_data.groupby('method')[metrics].mean()\n",
        "std_df = all_data.groupby('method')[metrics].std(ddof=1)\n",
        "n_folds = len(folds_results)\n",
        "sem_df = std_df / np.sqrt(n_folds)\n",
        "summary_df = mean_df.copy()\n",
        "for col in metrics:\n",
        "    summary_df[col] = mean_df[col].round(3).astype(str) + ' ± ' + sem_df[col].round(3).astype(str)\n",
        "\n",
        "summary_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a0c38ff",
      "metadata": {},
      "source": [
        "## **Coupling matrices visualization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71d9e726",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _safe_name(name: str) -> str:\n",
        "    \"\"\"Make a filesystem-friendly name from a method string.\"\"\"\n",
        "    return (\n",
        "        name.lower()\n",
        "            .replace(\" \", \"_\")\n",
        "            .replace(\"(\", \"\")\n",
        "            .replace(\")\", \"\")\n",
        "            .replace(\"/\", \"_\")\n",
        "    )\n",
        "\n",
        "def plot_labeled_coupling_all_folds(coupling_list, method_name, label_to_plot, max_display=500, save_dir=\".\"):\n",
        "    \"\"\"\n",
        "    For a labeled method (coupling_list[fold] = (T_dict, info)):\n",
        "    create one figure with subplots for all folds for a given label.\n",
        "    \"\"\"\n",
        "    n_folds = len(coupling_list)\n",
        "\n",
        "    fig, axes = plt.subplots(\n",
        "        1, n_folds,\n",
        "        figsize=(4 * n_folds, 4),\n",
        "        squeeze=False\n",
        "    )\n",
        "    axes = axes[0]  # row of axes\n",
        "\n",
        "    for fold in range(n_folds):\n",
        "        ax = axes[fold]\n",
        "        T_dict = coupling_list[fold][0]\n",
        "\n",
        "        if label_to_plot not in T_dict:\n",
        "            ax.set_visible(False)\n",
        "            continue\n",
        "\n",
        "        P = T_dict[label_to_plot]\n",
        "        if hasattr(P, \"cpu\"):\n",
        "            P = P.cpu().numpy()\n",
        "\n",
        "        # optionally crop for display if huge\n",
        "        ds = min(P.shape[0], max_display)\n",
        "        dt = min(P.shape[1], max_display)\n",
        "        P_disp = P[:ds, :dt]\n",
        "\n",
        "        sns.heatmap(\n",
        "            P_disp,\n",
        "            cmap=\"viridis\",\n",
        "            cbar=(fold == n_folds - 1),  # only last subplot has colorbar\n",
        "            xticklabels=False,\n",
        "            yticklabels=False,\n",
        "            square=True,\n",
        "            ax=ax\n",
        "        )\n",
        "        ax.set_title(f\"Fold {fold}\\n{ds}×{dt}\", fontsize=10)\n",
        "\n",
        "    fig.suptitle(f\"{method_name} - Label {label_to_plot}\", fontsize=14)\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    fname = f\"{_safe_name(method_name)}_label_{label_to_plot}.png\"\n",
        "    path = os.path.join(save_dir, fname)\n",
        "    plt.savefig(path, dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Saved: {path}\")\n",
        "\n",
        "def plot_global_coupling_all_folds(coupling_list, method_name, max_display=500, save_dir=\".\"):\n",
        "    \"\"\"\n",
        "    For a global/unlabeled method (coupling_list[fold] = (T, info)):\n",
        "    create one figure with subplots for all folds.\n",
        "    \"\"\"\n",
        "    n_folds = len(coupling_list)\n",
        "\n",
        "    fig, axes = plt.subplots(\n",
        "        1, n_folds,\n",
        "        figsize=(4 * n_folds, 4),\n",
        "        squeeze=False\n",
        "    )\n",
        "    axes = axes[0]\n",
        "\n",
        "    for fold in range(n_folds):\n",
        "        ax = axes[fold]\n",
        "        T = coupling_list[fold][0]\n",
        "        if hasattr(T, \"cpu\"):\n",
        "            T = T.cpu().numpy()\n",
        "\n",
        "        ds = min(T.shape[0], max_display)\n",
        "        dt = min(T.shape[1], max_display)\n",
        "        P_disp = T[:ds, :dt]\n",
        "\n",
        "        sns.heatmap(\n",
        "            P_disp,\n",
        "            cmap=\"viridis\",\n",
        "            cbar=(fold == n_folds - 1),\n",
        "            xticklabels=False,\n",
        "            yticklabels=False,\n",
        "            square=True,\n",
        "            ax=ax\n",
        "        )\n",
        "        ax.set_title(f\"Fold {fold}\\n{ds}×{dt}\", fontsize=10)\n",
        "\n",
        "    fig.suptitle(f\"{method_name} - All Folds\", fontsize=14)\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    fname = f\"{_safe_name(method_name)}.png\"\n",
        "    path = os.path.join(save_dir, fname)\n",
        "    plt.savefig(path, dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Saved: {path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e925feff",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n=== VISUALIZATION OF COUPLING MATRICES - ALL FOLDS IN SUBPLOTS ===\\n\")\n",
        "\n",
        "save_dir = \"./synthetic_coupling_plots\"\n",
        "\n",
        "# ---- Labeled methods ----\n",
        "labeled_coupling_lists = [legw, cotl, ecotl]\n",
        "labeled_coupling_names = [\n",
        "    \"EGWOT Labeled (legw)\",\n",
        "    \"COOT Labeled (cotl)\",\n",
        "    \"ECOOT Labeled (ecotl)\",\n",
        "]\n",
        "\n",
        "for c_list, name in zip(labeled_coupling_lists, labeled_coupling_names):\n",
        "    for label in np.unique(labels):\n",
        "        print(f\"\\n{name} — Label {label}: plotting all folds...\")\n",
        "        plot_labeled_coupling_all_folds(\n",
        "            coupling_list=c_list,\n",
        "            method_name=name,\n",
        "            label_to_plot=label,\n",
        "            max_display=500,\n",
        "            save_dir=save_dir,\n",
        "        )\n",
        "\n",
        "# ---- Global / full-matrix methods ----\n",
        "global_coupling_lists = [egw, egwper, cot, ecot, fgw]\n",
        "global_coupling_names = [\n",
        "    \"EGWOT Global (egw)\",\n",
        "    \"EGWOT Per Label (egwper)\",\n",
        "    \"COOT Global (cot)\",\n",
        "    \"ECOOT Global (ecot)\",\n",
        "    \"Fused GW (fgw)\",\n",
        "]\n",
        "\n",
        "for c_list, name in zip(global_coupling_lists, global_coupling_names):\n",
        "    print(f\"\\n{name}: plotting all folds...\")\n",
        "    plot_global_coupling_all_folds(\n",
        "        coupling_list=c_list,\n",
        "        method_name=name,\n",
        "        max_display=500,\n",
        "        save_dir=save_dir,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d50262a6",
      "metadata": {},
      "source": [
        "## **Grid search on regularization coefficient**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe7288d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # GRID SEARCH SUR EPSILON POUR LES MÉTHODES ENTROPIC\n",
        "# epsilon_values = [1e-3, 5e-3, 1e-2, 5e-2, 1e-1, 0.5, 1.0]\n",
        "\n",
        "# entropic_methods = [\n",
        "#     (\"EGWOT Labeled (legw)\", perturbot.match.get_coupling_egw_labels_ott),\n",
        "#     (\"EGWOT All (egw)\",      perturbot.match.get_coupling_egw_all_ott),\n",
        "#     (\"EGWOT Per Label (egwper)\", perturbot.match.get_coupling_egw_ott),\n",
        "#     (\"ECOOT Labeled (ecotl)\", perturbot.match.get_coupling_cotl_sinkhorn),\n",
        "#     (\"ECOOT All (ecot)\",     perturbot.match.get_coupling_cot_sinkhorn),\n",
        "#     (\"Fused GW (fgw)\",       get_coupling_fused_gw)\n",
        "# ]\n",
        "\n",
        "# results_grid = []\n",
        "\n",
        "\n",
        "# def extract_time_robust(log_dict):\n",
        "#     if not isinstance(log_dict, dict): return np.nan\n",
        "#     if 'time' in log_dict: return log_dict['time']\n",
        "#     first_key = list(log_dict.keys())[0]\n",
        "#     if isinstance(log_dict[first_key], dict) and 'time' in log_dict[first_key]:\n",
        "#         total_time = sum([v['time'] for k, v in log_dict.items() if isinstance(v, dict) and 'time' in v])\n",
        "#         return total_time\n",
        "#     return np.nan\n",
        "\n",
        "# print(f\"--- Begining of Grid Search on {len(epsilon_values)} epsilon values ---\\n\")\n",
        "\n",
        "# for eps in epsilon_values:\n",
        "#     print(f\"\\n>>> Testing Epsilon = {eps}\")\n",
        "    \n",
        "\n",
        "#     for method_name, method_func in entropic_methods:\n",
        "        \n",
        "#         print(f\"   Running {method_name}...\", end=\" \")\n",
        "        \n",
        "#         try:\n",
        "#             coupling_res = method_func(data_train, eps=eps)\n",
        "            \n",
        "#             coupling_mat = coupling_res[0]\n",
        "#             log_dict = coupling_res[1]     \n",
        "            \n",
        "\n",
        "#             model, pred_log = perturbot.predict.train_mlp(data_train, coupling_mat)\n",
        "            \n",
        "#             model.eval()\n",
        "#             with torch.no_grad():\n",
        "#                 Y_pred = model(test_adata_PROT.X).cpu().numpy().astype(np.float64)\n",
        "            \n",
        "\n",
        "#             Y_test_concatenated = Y_test_concatenated.astype(np.float64)\n",
        "#             metrics_df_pred = get_evals_preds(\n",
        "#                 Y_test_concatenated, \n",
        "#                 [Y_pred],              \n",
        "#                 pred_labels=[method_name], \n",
        "#                 full=False\n",
        "#             )\n",
        "            \n",
        "#             metrics_dict = metrics_df_pred[method_name].to_dict()\n",
        "#             metrics_dict[\"method\"] = method_name\n",
        "#             metrics_dict[\"epsilon\"] = eps\n",
        "#             metrics_dict[\"time\"] = extract_time_robust(log_dict)\n",
        "            \n",
        "#             results_grid.append(metrics_dict)\n",
        "            \n",
        "#             print(f\"Done. (MSE={metrics_dict.get('MSE', np.nan):.4f}, Time={metrics_dict.get('time', np.nan):.2f}s)\")\n",
        "            \n",
        "#         except Exception as e:\n",
        "#             print(f\"FAILED. Error: {e}\")\n",
        "\n",
        "#             results_grid.append({\n",
        "#                 \"method\": method_name,\n",
        "#                 \"epsilon\": eps,\n",
        "#                 \"MSE\": np.nan,\n",
        "#                 \"error\": str(e)\n",
        "#             })\n",
        "\n",
        "\n",
        "# df_grid_results = pd.DataFrame(results_grid)\n",
        "\n",
        "\n",
        "# df_grid_results = df_grid_results.sort_values(by=[\"method\", \"epsilon\"])\n",
        "\n",
        "# print(\"\\n--- Summary Table of the Grid Search ---\")\n",
        "\n",
        "# cols_to_show = ['method', 'epsilon', 'MSE', 'Pearson_corr', 'time']\n",
        "# available_cols = [c for c in cols_to_show if c in df_grid_results.columns]\n",
        "# print(df_grid_results[available_cols])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e083f1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # On trie d'abord par 'method' (pour regrouper), puis par 'MSE' croissant (le plus petit en premier)\n",
        "# df_sorted = df_grid_results.sort_values(by=['method', 'MSE'], ascending=[True, True])\n",
        "\n",
        "# print(\"\\n--- Résultats triés par Modèle (Meilleur MSE en premier) ---\")\n",
        "# print(df_sorted[available_cols])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
